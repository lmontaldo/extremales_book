
```{r include=FALSE}
library(dplyr)
library(tidyverse)
```


# Un primer enfoque de datos no $iid$ {#noiid}

 

## Datos no $iid$

Vamos a ver que en la realidad frecuentemente los datos no son $iid$, y
que hay muy diversos tipos de datos no $iid$.

Recordemos que si los datos no son $iid$ es porque los datos:

-   no son independientes,
-   su distribución no es la misma,
-   no son independientes y su distribución no es la misma.

__Nota:__ Los tests que permiten contrastar la hipótesis $iid$ son:
Spearman, runs up and down, etc. En `R` se encuentran como randtests.

```{=html}
<!--
Aclaración 1: Dado que abordamos un curso de
posgrado, y que en los cursos básicos se abordan
los tests que permiten contrastar la hipótesis iid
(Spearman, runs up and down, etc.) supondremos
estas técnicas conocidas, pero obviamente podemos
analizarlas aparte con quien lo necesite. En R, por
ejemplo, se encuentran como randtests. Un escalón
más arriba, las Series de Tiempo, su análisis
espectral, etc., es tema de otros curso de segundo
nivel de Estadística que ofrecemos, por lo cual no
nos detendremos mayormente, pero nuevamente,
estamos a disposición de quien lo requiera y en este
curso al menos explicaremos brevemente los
aspectos medulares que usemos.
-->
```

## Procesos estacionar

Una secuencia de variables aleatorias independientes es, en general, estacionaria. Sin embargo, la estacionariedad admite dependecia en las variables de una serie pero implica que sus propiedades estocásticas sean homogéneas en el tiempo. Si $X_1,X_2, \dots$ es una serie estacionaria, entonces $X_1$ tiene la misma distribución que $X_{60}$, la distribución conjunta de $(X_1, X_2)$ es la misma que $X_{101}$ y $X_{102}$ pero no es necesario que $X_1$ sea independiente de $X_2$ o de $X_{102}$. La dependencia en las series estacionarias puede tomar múltiples formas por lo que se hace dificil realizar generalizaciones. Para el caso extremal, es necesario imponer restricciones: se puede suponer que los valores extremos son independientes cuando están lo suficientemente apartados en el tiempo. Muchas series estacionarias satisfacen esto y además es una condición posible para muchos procesos físicos. Eliminar la dependencia de largo plazo para valores extremos, nos permite poner el foco en el efecto de la dependencia de corto plazo.  Cuando los datos extremales sean estacionarios podremos modelarlos a través del método de máximos por bloques con distribuciones GEV como vimos anteriormente y como veremos luego, por métodos de umbrales [@coles2001introduction].



### Concepto de Estacionariedad

Consideremos los datos relativos a los niveles máximos anuales del nivel del mar registrados en Port Pirie, una localidad de Australia del Sur, durante el período 1923-1987.

```{r echo=TRUE, warning=FALSE}
library(ismev)
data(portpirie)
```

<!--
Los primeros 5 datos son:

```{r}
head(portpirie, 5)
```

-->

```{r nice-fig2, echo=FALSE, fig.cap='Máximos anuales del nivel del mar en Port Pirie (1923-1987)', ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
plot(portpirie,
     xlab = "Año",
     ylab = "Nivel del mar (metros)",
     pch = 21,        # círculo con borde y relleno
     bg = "red",      # color de relleno
     col = "black")   # color del borde
```

__Problema:__ A partir de estos datos, puede ser necesario estimar el nivel máximo del mar que
probablemente ocurra en la región en los próximos 100 o 1000 años. 

Lo anterior plantea un desafío importante: ¿cómo podemos estimar los niveles para los próximos 1000 años sin conocer, por ejemplo, los cambios climáticos que podrían suceder?

No hay evidencia contundente en la Fig. \@ref(fig:nice-fig2) que sugiera que el patrón de
variación en los niveles del mar haya cambiado durante el período de
observación, pero dicha estabilidad podría no persistir en el futuro.

La observación anterior es crucial: aunque la teoría de valores extremos ha
adoptado una terminología como el "nivel de retorno a 1000 años", este concepto solo tiene sentido bajo el supuesto de estabilidad (o estacionariedad) en el proceso subyacente (@coles2001introduction).




Entonces, cuando los datos:

(1) No necesariamente son independientes pero la estructura de
    distribuciones es siempre la misma, se habla de datos estacionarios
    o *procesos estacionarios*.

(2) No son independientes pero la dependencia se va atenuando a medida
    que se consideran datos más lejanos (para fijar ideas, imaginemos
    que los datos corresponde a medidas en el tiempo, y que datos muy
    viejos no influirían significativamente sobre el presente, por
    ejemplo), se habla de datos débilmente dependientes.


#### Caso estacionario: datos $m-$dependientes

El caso más sencillo de la situación (2) es la de datos $m$-dependientes,
siendo $m$ un número; si los datos que están a una distancia mayor que
$m$ entonces son _independientes_. 

Un ejemplo de datos $m$-dependientes son los procesos llamados
Medias Móviles^[Moving Average en inglés.] de orden $q$ [@enders]. El proceso $MA(q)$ con $m=q$ puede expresarse como

\[
X_t = \mu + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q},
\]

donde

- \( X_t \) es el valor observado en el tiempo \( t \),
-  \( \mu \) es la media del proceso,
-  \( \varepsilon_t \sim \text{iid}(0, \sigma^2) \) es un proceso de ruido blanco,
- \( \theta_1, \dots, \theta_q \) son los coeficientes del modelo,
- \( q \) es el orden del modelo.

En forma compacta, puede escribirse como

\[
X_t = \mu + \sum_{j=0}^{q} \theta_j \varepsilon_{t-j}, \quad \text{con } \theta_0 = 1.
\]

 

Algunos datos bioquímicos del agua de determinadas playas uruguayas que
se miden diariamente suelen ser 7-dependientes, por ejemplo. Esto
significa que datos que fueron tomados con más de una semana de
separación, son independientes. 

En cambio, algunos datos de poblaciones o de composición de suelos que se van registrando a lo largo de los
años, son débilmente dependientes, pero no son $m$-dependientes para
ningún $m$. Es el caso de los llamados procesos Autorregresivos de orden
$p$, $AR(p)$, donde cada dato es una combinación lineal de los $p$ anteriores más un término aleatorio que es un ruido blanco [@enders]. 

El proceso \( \text{AR}(p) \), se define como


\[
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \cdots + \phi_p X_{t-p} + \varepsilon_t,
\]

donde:
\begin{itemize}
  \item \( X_t \) es el valor de la serie en el tiempo \( t \),
  \item \( \phi_1, \dots, \phi_p \) son los coeficientes autorregresivos,
  \item \( \varepsilon_t \sim \text{iid}(0, \sigma^2) \) representa el ruido blanco,
  \item \( p \) es el orden del modelo (número de retardos).
\end{itemize}

En forma compacta, se puede escribir como

\[
X_t = \sum_{j=1}^{p} \phi_j X_{t-j} + \varepsilon_t.
\]


Estos procesos son Markovianos, es decir, de memoria infinita: todo el pasado
es tan informativo como los últimos $p$ datos^[El carácter Markoviano es mal presentado en algunos textos y cursos, hablando confusamente de memoria corta.]. En este sentido, la
genética aporta excelentes ejemplos para distinguir los procesos
markovianos. Esto es así porque algunas pocas generaciones atrás
alcanzan para calcular probabilidades de muchas características de la
descendencia, mientras que la memoria no es corta sino infinita: cada
característica resulta de toda la fiogenética, así nos remontemos al
origen de las especies.



<!--- EJEMPLO DEL LIBRO A REVEER -->

```{=html}
<!--- Para simplificar al extremo y usando la
imagen bíblica (con todo respeto a todos los
pensares), si Adán y Eva hubieran tenido otros
genes, la especie podría ser diferente!!-->
```

En la teoría clásica de series de tiempo, una estructura $AR(p)$ a
partir de un proceso $MA(q)$, da lugar a lo que llamamos proceso Autorregresivo de Medias Móviles de orden \( (p,q) \). Este modelo combina un modelo Autorregresivo \( \text{AR}(p) \) con un modelo de Medias Móviles \( \text{MA}(q) \). La expresión general del modelo $ARMA(p,q)$ es

\[
X_t = \phi_1 X_{t-1} + \phi_2 X_{t-2} + \cdots + \phi_p X_{t-p} + \varepsilon_t + \theta_1 \varepsilon_{t-1} + \theta_2 \varepsilon_{t-2} + \cdots + \theta_q \varepsilon_{t-q},
\]

donde
\begin{itemize}
  \item \( \{X_t\} \) es la serie temporal observada,
  \item \( \varepsilon_t \sim \text{iid}(0, \sigma^2) \) es un ruido blanco,
  \item \( \phi_1, \dots, \phi_p \) son los coeficientes autorregresivos,
  \item \( \theta_1, \dots, \theta_q \) son los coeficientes de medias móviles,
  \item \( p \) es el orden autorregresivo, y \( q \) el orden de medias móviles.
\end{itemize}



\medskip

También puede escribirse en forma compacta como

\[
X_t = \sum_{i=1}^{p} \phi_i X_{t-i} + \varepsilon_t + \sum_{j=1}^{q} \theta_j \varepsilon_{t-j}.
\]





Los procesos \( \text{ARMA}(p,q) \), en general, son débilmente dependientes y aproximan bien a cualquier proceso estacionario débilmente dependiente\footnote{Lo cual es una de las razones de su popularidad.}.

Un proceso estocástico \( \{X_t\} \) se dice \emph{débilmente dependiente}^[Se dice también que es un proceso \emph{débilmente correlacionado} o de \emph{dependencia de corto alcance}.] si la dependencia entre sus observaciones disminuye a medida que crece la distancia temporal entre ellas [@doukhan2007].

En términos de la función de autocorrelación, esto significa que

\[
\lim_{h \to \infty} \rho(h) = 0,
\]

donde \( \rho(h) \) es la función de autocorrelación de orden \( h \), definida como

\[
\rho(h) = \frac{\text{Cov}(X_t, X_{t+h})}{\sqrt{\text{Var}(X_t)\, \text{Var}(X_{t+h})}} = \text{Corr}(X_t, X_{t+h}).
\]

Esta mide la correlación lineal entre dos observaciones de la serie temporal separadas por \( h \) unidades de tiempo.

Una condición más fuerte que suele usarse es que la suma de las autocorrelaciones sea absolutamente convergente

\[
\sum_{h=-\infty}^{\infty} |\rho(h)| < \infty.
\]

Esta propiedad garantiza que la influencia del pasado sobre el presente se desvanece rápidamente, lo cual caracteriza a los procesos débilmente dependientes.







\vspace{1em}

Procesos como los modelos \( \text{ARMA}(p,q) \) son ejemplos típicos de procesos débilmente dependientes. En contraste, procesos con \emph{dependencia fuerte} (o de \emph{largo alcance}) mantienen correlación significativa incluso para retardos grandes.Es decir, lo opuesto a dependencia débil es dependencia fuerte^[En inglés, long range dependence.]. En algunos datos de telecomunicaciones que se registran en escalas temporales muy finas, se han encontrado ejemplos de dependencia fuerte.
 

\newpage 
## Descomposición de series temporales

Las series temporales no estacionarias contienen características que cambian sistemáticamente con el tiempo. Por ejemplo, los procesos asociados al medio ambiente están sujetos a los efectos estacionales, a tendencias y también a efectos del cambio climático de más largo plazo [@coles2001introduction]. Lo anterior implica se puede descomponer a una serie temporal en distintos componentes: tendencia-ciclo, estacional, irregular. Se emplean métodos estadísticos para extraer estos componentes con el objetivo de entender a los patrones subyacentes de la serie estudiada y obtener predicciones confiables [@enders].

Siguiendo @hyndman, los distintos componentes se pueden describir de la siguiente manera:

- Tendencia-ciclo: Se vincula a los movimientos de largo plazo. Los ciclos en particular, son movimientos irregulares. Es decir, podemos pensar a los ciclos como fluctuaciones de largo plazo, no necesariamente regulares, asociadas a fenómenos económicos, sociales o naturales (como recesiones o expansiones). A diferencia de la estacionalidad, los ciclos no tienen una duración fija.

- Estacionalidad: Son fluctuaciones periódicas que se repiten a intervalos regulares, como días, meses o años. Por ejemplo, ventas más altas en diciembre o temperaturas más bajas en invierno.

- Irregularidad: Todo lo que no es lo anterior. Es decir, son variaciones impredecibles, aleatorias o residuales, causadas por factores no sistemáticos. Es un componente residual.


Existen dos modelos generales para descomponer una serie temporal: el modelo aditivo y el modelo multiplicativo.

__Modelo aditivo :__ Se utiliza cuando la variación estacional es aproximadamente constante a lo largo del tiempo.

\begin{equation}
Y_t = T_t + S_t + R_t
 (\#eq:aditivo)
\end{equation}

donde \(Y_t\) es la serie original, \(T_t\) la tendencia, \(S_t\) la estacionalidad y \(R_t\) el residuo.

__Modelo multiplicativo :__ Se aplica cuando la variación estacional cambia con el nivel de la serie (heterocedasticidad).

$$
Y_t = T_t \times S_t \times R_t
$$
<!--
A modo ilustrativo en la Fig.\@ref(fig:adit) , tomamos el ejemplo de @hyndman donde se muestran los distintos componentes de una serie temporal en un modelo aditivo.



```{r adit, fig.cap="Número total de personas empleadas en el comercio minorista de EE.UU. y sus tres componentes" ,echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
#install.packages("fpp3")
library(fpp3)
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))
components(dcmp) |> autoplot()
```


-->

<!---


Si esta función es monótona, se habla de que los datos presentan
tendencias (trends). Si en cambio la función es periódica, se dice que
los datos presentan ciclos (seasonal effects).

  [Text to display](https://otexts.com/fpp3/decomposition.html) 

Se puede demostrar que si los datos se vuelven iid al restar una función
determinística cualquiera (lo cual no siempre es el caso!), entonces se
pueden modelar aproximadamente como TENDENCIAS+CICLOS+ DATOS IID.

Al descomponer una serie temporal, a veces es útil primero transformar o
ajustar la serie para que la descomposición, y su análisis posterior,
sea lo más sencilla posible (@hyndman).

-->

Considerando lo anterior, cuando los datos no son $iid$ pero se vuelven $iid$ al restarle una
función determinística, son bastante simples de manejar.


Existes varios métodos para estimar y descomponer series temporales [@brockwell2016], entre ellos:

- El método X-11, desarrollado originalmente por la Oficina del Censo de EE.UU. y ampliado por Statistics Canada, se basa en la descomposición clásica pero incorpora ajustes adicionales para mejorar sus limitaciones. Permite estimar la tendencia-ciclo en todos los puntos, admite variaciones estacionales suaves y corrige efectos de calendario, feriados y predictores conocidos.

- STL es un método robusto y flexible para descomponer series temporales en tendencia y estacionalidad, utilizando suavizado local (loess) para capturar relaciones no lineales.


Una vez estimados los componentes de una serie temporal, se realiza el ajuste de la serie, restándole los componentes a los datos originales (que pueden estan previamente transformados en logaritmos).





::: {.example #datosnoiid name="Datos no estacionarios"}
Vamos a presentar a continuación como “lucen” los distintos tipos de
datos y qué tan reconocibles son. Los datos se han simulado
computacionalmente, por lo tanto sabemos a cuál es su real estructura.

1- Empecemos por lo básico”:¿Cómo lucen datos $iid$? A continuación una
muestra de tamaño mil de datos $iid$ con distribución Gumbel (Fig.\@ref(fig:datosiid)).

2- Comparación de la distribución anterior con una distribución normal (Fig.\@ref(fig:compnorm)).

3- Simulación de datos con distribución Gumbel $iid$ con una tendencia parabólica y ciclos sinusoidales.

4- Restar los componentes de tendencia y ciclo de la serie simulada en 3-.
:::

1- Simulación de una distribución Gumbel con 1000 daots $iid$.

```{r datosiid, fig.cap="Mil datos iid simulados con una distribución Gumbel" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
set.seed(123)
n <- 1000
library(evd)
datos_iid <- rgumbel(n)
plot(datos_iid, type = "l", col = "steelblue", lwd = 1.2,
     xlab = "Tiempo", ylab = "Valor")
```

Lo más sobresaliente en la visualización es la presencia de
variaciones abruptas en los datos, que dificultan su representación
gráfica e interpretación inmediata. Estas oscilaciones no son
necesariamente equilibradas en torno a un eje horizontal, lo cual se
debe a la asimetría de la distribución Gumbel, que presenta una cola
más larga hacia la izquierda.


```{r echo=TRUE}
# Calcular media y mediana
media_gumbel <- mean(datos_iid)
mediana_gumbel <- median(datos_iid)
```

```{r echo=FALSE}
cat("Media:", round(media_gumbel, 4), "\n")
cat("Mediana:", round(mediana_gumbel, 4), "\n")
```



Aunque su valor esperado es aproximadamente 0.54, la mediana se
encuentra en torno a -0.31, reflejando dicha asimetría. 

2- Si simulamos datos a partir de una distribución simétrica, como la Normal,
podemos observar un mayor equilibrio visual entre los picos y valles
de la serie. Sin embargo, eso podría desviar la atención del aspecto
esencial que son las oscilaciones intensas. Además, el conjunto de datos no muestra una tendencia clara ni evidencia de ciclos persistentes.



```{r compnorm, fig.cap="Mil datos iid simulados con una distribución Gumbel y una Normal" ,echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
set.seed(123)
n <- 1000
library(evd)
library(ggplot2)
library(dplyr)

# Simular datos
datos <- tibble(
  tiempo = 1:n,
  Gumbel = rgumbel(n),
  Normal = rnorm(n)
) %>%
  tidyr::pivot_longer(cols = c(Gumbel, Normal), names_to = "Distribución", values_to = "Valor")

# Graficar
ggplot(datos, aes(x = tiempo, y = Valor, color = Distribución)) +
  geom_line() +
  scale_color_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  facet_wrap(~Distribución, ncol = 1, scales = "free_y") +
  labs(x = "Tiempo", y = "Valor") +
  theme_minimal() +
  theme(legend.position = "none")

```

De las Fig. \@ref(fig:compnorm) y \@ref(fig:distribmar) desprende que :

-   Gumbel muestra más valores extremos hacia la izquierda, presenta asimetría
    negativa (Fig.\@ref(fig:distribmar))



-   Normal muestra oscilaciones más simétricas alrededor de cero (Fig. \@ref(fig:compnorm)).

```{r distribmar, fig.cap="Comparación de distribuciones marginales: Gumbel vs. Normal" ,echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
ggplot(datos, aes(x = Valor, fill = Distribución, color = Distribución)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, alpha = 0.4, position = "identity") +
  geom_density(linewidth  = 1.2, alpha = 0.2)+
  scale_fill_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  scale_color_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  facet_wrap(~Distribución, ncol = 1, scales = "free") +
  labs(x = "Valor", y = "Densidad") +
  theme_minimal()
```

Este comportamiento es típico de secuencias $iid$: aunque presentan
variabilidad local intensa, su estructura global carece de patrones de
dependencia temporal.





3-Simulamos 500 datos con una distribución Gumbel más un componente de tendencia y ciclo. El modelo simulado puede expresarse como

\begin{equation} 
Y_t = X_t + \underbrace{a t^2 + b t + c}_{\text{tendencia cuadrática}} + \underbrace{A \sin(\omega t + \phi)}_{\text{componente cíclica}}, \quad t = 1, 2, \dots, n,
  (\#eq:simu)
\end{equation} 




donde:

- \( X_t \overset{\text{iid}}{\sim} \text{Gumbel}(\mu, \sigma) \) son los datos $iid$ con distribución Gumbel,
- \( a, b, c \in \mathbb{R} \) son coeficientes de la tendencia polinómica de segundo orden,
- \( A \) es la amplitud del ciclo sinusoidal,
- \( \omega \) es la frecuencia angular,
- \( \phi \) es el desfase,
- \( n \) es el número total de observaciones.





```{r echo=FALSE, warning=FALSE}
# Parámetros y simulación
set.seed(123)
n <- 500
t <- 1:n

# Ruido iid Gumbel
gumbel_noise <- rgumbel(n, loc = 0, scale = 1.2)

# Tendencia cuadrática: a * t^2 + b * t + c
a <- 0.0002
b <- 0.02
c <- 0
trend <- a * t^2 + b * t + c

# Componente cíclica: A * sin(omega * t + phi)
A <- 3
omega <- 2 * pi / 50
phi <- 0
cycle <- A * sin(omega * t + phi)

# Serie observada
Y <- gumbel_noise + trend + cycle
```


```{r gumbelt, fig.cap="Serie simulada: Gumbel + tendencia cuadrática + ciclo sinusoidal" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Armar data frame
df <- data.frame(t = t, Y = Y, trend = trend, cycle = cycle)

# Gráfico
ggplot(df, aes(x = t, y = Y)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(x = "Tiempo", y = "Valor observado") +
  theme_minimal()
```



De la Fig. \@ref(fig:gumbelt) se desprende que si bien hay oscilaciones aleatorias, es perceptible a simple vista una tendencia creciente y la presencia de una estructura cíclica.

Sin conocer el modelo de simulación de la Eq. \@ref(eq:simu), sólo con observar la Fig. \@ref(fig:gumbelt) y trás un poco de análisis exploratorio unx puede intuir que la tendencia debe ser lineal o cuadrática. Si se estima un polinomio de orden 2 más un término sinusoidal (con parámetros de amplitud, frecuencia y fase), mediante mínimos cuadrados se puede ajustar bien (Fig. \@ref(fig:gumbelmco)).


```{r include=FALSE}
# Usar el mismo omega que en la simulación
omega <- 2 * pi / 50

# Crear variables para el modelo
df$tt <- df$t
df$tt2 <- df$t^2
df$sin_term <- sin(omega * df$t)
df$cos_term <- cos(omega * df$t)

# Ajuste por mínimos cuadrados
modelo_mco <- lm(Y ~ tt2 + tt + sin_term + cos_term, data = df)

# Resumen del modelo
summary(modelo_mco)
```





```{r gumbelmco, fig.cap="Ajuste por mínimos cuadrados: tendencia cuadrática + componente sinusoidal" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Gráfico: datos y ajuste
df$ajuste <- predict(modelo_mco)

library(ggplot2)
ggplot(df, aes(x = t)) +
  geom_line(aes(y = Y), color = "steelblue", size = 0.8, alpha = 0.6) +
  geom_line(aes(y = ajuste), color = "darkred", size = 0.9) +
  labs(x = "Tiempo", y = "Valor") +
  theme_minimal()
```




4 - Si se le restan a los datos originales los valores de las componentes determinísticas —la tendencia y el ciclo ajustados por mínimos cuadrados—, se obtienen los **residuos**:

$$
\hat{\varepsilon}_t = Y_t - \hat{T}_t - \hat{C}_t,
$$

donde \( \hat{T}_t \) es la tendencia estimada y \( \hat{C}_t \) la componente cíclica estimada.

```{r echo=TRUE}
# Calcular residuos
df$residuos <- df$Y - df$ajuste
```

Estos residuos (Fig. \@ref(fig:residgumbelmco)) deberían comportarse como ruido aleatorio, y en particular, deberían superar razonablemente los tests para datos independientes e idénticamente distribuidos ($iid$). A continuación se presentan algunas herramientas para tomar este tipo de conclusiones sobre los residuos.


```{r residgumbelmco, fig.cap="Residuos del modelo ajustado" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Graficar los residuos
library(ggplot2)
ggplot(df, aes(x = t, y = residuos)) +
  geom_line(color = "gray30", size = 0.7) +
  labs(
       x = "Tiempo", y = "Residuos") +
  theme_minimal()
```


- Herramientas de diagnóticos visual: Sirven para diagnosticar la dependencia temporal de una serie: si la serie es ruido blanco ($iid$), tanto la ACF como la PACF mostrarán valores cercanos a cero para todos los retardos (excepto en el lag 0).


  - La función de autocorrelación (ACF) mide la correlación entre los valores actuales de una serie temporal \( Y_t \) y sus valores pasados \( Y_{t-k} \), para diferentes retardos \( k \). Es decir, nos dice cuánto del valor actual está relacionado con sus propios valores pasados, incluyendo cualquier efecto intermedio. Nos indica qué tan correlacionado está \( Y_t \) con \( Y_{t-k} \), sin tener en cuenta lo que ocurre entre medio.

  - La función de autocorrelación parcial (PACF) mide la correlación directa entre \( Y_t \) y \( Y_{t-k} \), eliminando el efecto de los retardos intermedios \( Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1} \).
 






```{r acfpacfcombined, fig.cap = "ACF y PACF de los residuos", echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
# Establecer diseño: 2 filas, 1 columna
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))

# ACF
acf(df$residuos, main = "")

# PACF
pacf(df$residuos, main = "")
```



\newpage

- El resultado del test de Box-Ljung, que se usa para evaluar si los residuos presentan autocorrelación (es decir, si son dependientes entre sí). Se pone a prueba $H_0:$ los residuos son independientes (no autocorrelacionados).


```{r echo=FALSE}
# Test de Ljung-Box para autocorrelación (independencia)
Box.test(df$residuos, lag = 20, type = "Ljung-Box")
```


Como el valor-\(p = 0.5551\) es mayor que 0.05, no se rechaza la hipótesis nula.

Entonces, no hay evidencia de autocorrelación en los residuos.

Esto es positivo, ya que indica que el modelo ajustado capturó correctamente la estructura determinística (tendencia + ciclo), y lo que queda se comporta como \textit{ruido aleatorio} ($iid$).

<!--

```{r residuoshist, fig.cap = "Histograma de residuos",echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
ggplot(df, aes(x = residuos)) +
  geom_histogram(color = "black", fill = "steelblue", bins = 30) +
  labs(x = "Residuos", y = "Frecuencia") +
  theme_minimal()
```



```{r hist, fig.cap="QQ-plot de residuos" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# QQ-plot para evaluar distribución
qqnorm(df$residuos)
qqline(df$residuos, col = "red")
```

-->




::: {.example #mdepen name="Datos estacionarios"}
Consideremos un conjunto de observaciones de un proceso 2-dependiente, estacionario, donde los
datos tienen distribución Gumbel.

1- Comparación de una distribución Gumbel $iid$ con la de Gumbel 2-dependiente.

2- Gráfico de una distribución Gumbel 2-dependiente con componente cíclica.

3- Veamos finalmente como luce la dependencia fuerte. Nuevamente el proceso es estacionario y los
datos son Gumbel.
:::

1- Veamos qué pasa ahora si yuxtaponemos la gráfica de Gumbel $iid$ con la de Gumbel 2-dependiente.

```{r echo=FALSE, warning=FALSE}
# Simular datos Gumbel iid
set.seed(123)
n <- 500
gumbel_iid <- rgumbel(n, loc = 0, scale = 1)

# Proceso 2-dependiente
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 1)
a1 <- 0.5
a2 <- 0.3

X_dep <- numeric(n)
for (t in 3:(n+2)) {
  X_dep[t-2] <- a1 * gumbel_full[t-1] + a2 * gumbel_full[t-2] + gumbel_full[t]
}

# Crear data frame en formato largo manualmente
df_long <- data.frame(
  time = rep(1:n, 2),
  Valor = c(gumbel_iid, X_dep),
  Proceso = factor(rep(c("IID", "Dependiente"), each = n))
)

```





```{r iidvs2m, fig.cap = "Comparación: Gumbel iid vs. Gumbel 2-dependiente",echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
ggplot(df_long, aes(x = time, y = Valor, color = Proceso)) +
  geom_line(alpha = 0.8) +
  scale_color_manual(values = c("IID" = "steelblue", "Dependiente" = "red")) +
  labs(x = "Tiempo", y = "Valor", color = "Proceso") +
  theme_minimal()+
  theme(legend.position = "bottom")
```



Aún para el ojo más adiestrado es muy difícil distinguir las dos situaciones. Puede quizás
percibirse mayor “inercia” en el caso de los datos en linea roja, correspondiente a la 2-dependencia, pero es realmente muy difícil lograrlo.

Esto demuestra la importancia de realizar tests de hipótesis sobre la estructura subyacente y no
utilizar el “ojímetro”.


2- Veamos como luce ahora una serie de datos Gumbel 2-dependientes con componente cíclica.


```{r echo=FALSE, warning=FALSE}
set.seed(123)
n <- 500
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 0.7)

a1 <- 0.5
a2 <- 0.3

frequency <- 2 * pi / 100
amplitude <- 0.2

X_dep_cycle <- numeric(n)
for (t in 3:(n+2)) {
  X_dep_cycle[t-2] <- a1 * gumbel_full[t-1] + a2 * gumbel_full[t-2] +
                      gumbel_full[t] + amplitude * sin(frequency * (t-2))
}

time <- 1:n
df <- data.frame(time = time, X = X_dep_cycle)

```



```{r ciclica2m, fig.cap = "Gumbel 2-dependiente con componente cíclica",echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
ggplot(df, aes(x = time, y = X)) +
  geom_line(color = 'steelblue', size = 0.8) +
  labs(x = "Tiempo", y = "Observación") +
  theme_minimal()
```


Se insinúa una estructura de ciclos, pero nuevamente es altamente recomendable chequear la
estructura mediante tests. 

3- Veamos finalmente como luce la dependencia fuerte. Nuevamente el proceso es estacionario y los
datos son Gumbel.




```{r gumbelpoda, fig.cap = "Proceso Gumbel 2-dependiente con poda inferior evidente", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Simulación
set.seed(123)
n <- 500
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 0.7)

a1 <- 0.9
a2 <- -0.7

X_dep <- numeric(n)
for (t in 3:(n + 2)) {
  X_dep[t - 2] <- a1 * gumbel_full[t - 1] + a2 * gumbel_full[t - 2] + gumbel_full[t]
}

time <- 3:(n + 2)
df <- data.frame(time = time, X = X_dep)

# Poda visual: reemplazar todo valor menor a -1 por -1 (sin poner NA)
df$Xp <- pmax(df$X, -1)

# Graficar
ggplot(df, aes(x = time, y = Xp)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(x = "Tiempo", y = "Valores") +
  theme_minimal()
```





Las oscilaciones son parecidas a los casos anteriores pero la muy parejita “poda” inferior es la que hace sospechar algo “raro”. De todos modos, las posibles estructuras de dependencia fuerte son muchas, complejas y
verificarlas no es sencillo.



Como resumen final, en los más próximos capítulos tomaremos direcciones no $iid$, en las que veremos:

1- Si el proceso es estacionario y débilmente dependiente, la técnica clásica de DEA puede
aplicarse esencialmente igual.

<!-- (Leadbetter, Lindgren, Rootzén) -->

2- Si el proceso no es estacionario o no es débilmente dependiente, cumple algunas
propiedades condicionales a otro proceso que le pauta la “fase”, la técnica clásica de DEApuede aplicarse, pero con modificaciones no menores.

Además:

3- Puede cambiarse el enfoque y mirar el número de eventos extremos en diversos lapsos, esto
lleva en el caso $iid$ a un proceso de Poisson, para estacionarios y débilmente dependientes a
un Poisson Compuesto y en el marco más general del punto 2, a mezclas de Procesos de
Poisson Compuestos. El conteo de eventos extremos será también la base o el “pie”, para introducir una técnica muy usada, POT (Picos Sobre Umbrales) y sus variaciones más recientes que veremos al final del curso.

<!-- (Lise Bellanger-GP). -->


```{example, label="co2", name="Concentración atmosférica de $CO_2$ en Mauna Loa"}
Vamos a analizar a la serie temporal mensual relativa a la concentración atmosférica de $CO_2$ en Mauna Loa, Hawai. Las concentraciones atmosféricas de $(CO_2)$ se expresan en partes por millón (ppm) y se reportan según la escala preliminar de fracción molar manométrica del SIO de 1997. Es una serie temporal, de frecuencia mensual, con 468 observaciones, desde 1959 a 1997.
```


```{r echo=TRUE}
library("datasets")
```



```{r echo=TRUE}
class(co2)
```

```{r echo=TRUE}
head(co2)
```

Los datos se pueden visualizar en la siguiente Figura \@ref(fig:co2g).

```{r co2g, fig.cap = "Concentración atmosférica de CO2", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# plot(co2, ylab = expression("Concentración atmosférica de CO"[2]),las = 1)
# Extraer tiempo y convertirlo en fechas reales
co2_df <- data.frame(
  fecha = seq(as.Date("1959-01-01"), by = "month", length.out = length(co2)),
  co2 = as.numeric(co2)
)

# Crear gráfico
ggplot(co2_df, aes(x = fecha, y = co2)) +
  geom_line(color = "steelblue") +
  labs(
    x = "Fecha",
    y = expression("Concentración atmosférica de CO"[2])
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0))

```


Del análisis visual del gráfico podemos concluir los siguiente:

- Tendencia creciente: la concentración media de $CO_2$ crece sostenidamente entre 1959 y 1997. Esto podría reflejar un fenómeno de largo plazo, relacionado con las emisiones globales de $CO_2$.

- Estacionalidad marcada: hay un patrón oscilante de subidas y bajadas regulares cada año. Es decir, se evidencia un ciclo con máximos y mínimos anuales. Entonces, se puede evidenciar  un patrón repetitivo que ocurre cada año: los valores suben y bajan de manera bastante regular dentro de cada ciclo anual.

- Transición entre máximos y mínimos: es evidente el pasaje sistemático desde un máximo a un mínimo dentro de cada año. 




Si  asumimos un modelo aditivo (Eq. \@ref(eq:aditivo)) para descomponer a la serie temporal, la estacionalidad de la serie resulta evidente y clara. Vamos a realizar una descomposición estacional clásica por médias móviles.

```{r}
# descomposicion por MM
co2_decomp <- decompose(co2, type="additive")
```




```{r co2d, fig.cap = "Descompoisción de la serie", echo=FALSE, fig.height=6, fig.width=6, message=FALSE, warning=FALSE}
plot(co2_decomp)
```



Ahora, analizamos el resultado del test de Box-Ljung para evaluar si los residuos presentan autocorrelación. Se pone a prueba $H_0)$ los residuos son independientes (no autocorrelacionados).




```{r echo=TRUE}
# help(decompose) --> podemos ver en la ayuda los resultados
# random: The remainder part
residuos<-co2_decomp$random
# Test de Ljung-Box para autocorrelación (independencia)
Box.test(residuos, lag = 20, type = "Ljung-Box")
```
Como el p-valor es muchísimo menor que cualquier nivel típico de significación (0.05, 0.01, etc.), rechazamos $H_0)$. En un buen modelo de series temporales (o en un buen ajuste estacional con `decompose`), los residuos deberían ser ruido blanco (independientes, media 0, varianza constante).

De la Fig.\@ref(fig:acfres), se desprende que hay varias barras (sobre todos las iniciales) que claramente sobrepasan las bandas azules (intervalos de confianza al 95%). Esto indica que existe autocorrelación significativa en los primeros retardos y aunque la correlación parecería decaer a medida que aumentan los lags, no desaparece del todo. 



```{r acfres, fig.cap="ACF de los residuos para 20 rezagos", fig.height=3, fig.width=6, echo=FALSE, message=FALSE, warning=FALSE}
# ACF
acf(residuos, na.action = na.omit, lag.max =20, main="")
```

Por todo lo anterior podemos concluir que con la descomposición empleada los residuos no se comportan como ruidos blancos. Por esto, es necesario elegir otra descomposición o buscar métodos alternativos que nos brinden los resultados esperados. En este sentido, una alternativa consiste en emplear la función `auto.arima` del paquete `forecast` que nos permite ajustar (detectar) el mejor modelo ARIMA de una serie temporal univariada. 



```{r echo=TRUE}
# Si no la tenemos instalada corremos: install.packages("forecast")
library(forecast)
# Ajusta el mejor modelo ARIMA a una serie temporal univariada
fit <- auto.arima(co2, stationary = FALSE, seasonal = TRUE) 
```

Una vez que ajustamos el modelo, podemos analizar los residuos con `checkresiduals`. 

```{r echo=TRUE}
# Corre Ljung–Box test
checkresiduals(fit, plot=FALSE)  
```
La prueba de Ljung-Box nos arroja un $p-value = 0.4424 > \alpha=0.05$ por lo que habría evidencia de no autocorrelación de los residuos para 24 rezagos. Podemos concluir que los residuos se comportan como un ruido blanco con una confianza del $95\%$. 

En la Figura \@ref(fig:forecastres) hay tres paneles que resumen el diagnóstico de residuos para el modelo $ARIMA(1,1,1)(1,1,2)[12]$.
En el gráfico superior se visualizan los residuos hallados trás el ajuste del modelo ARIMA. Los residuos parecerían oscilar alrededor de cero sin mostrar tendencias persistentes ni patrones estacionales visibles. No se observan _rachas_ largas de valores positivos o negativos, lo cual es un buen signo de independencia. Además, la amplitud parece relativamente constante por lo que no parece haber heterocedasticidad fuerte. 
En el gráfico inferior izquierdo, tenemos el ACF de los residuos. Observamos que la mayoría de las barras están dentro de las bandas del IC($95\%$). Esto refuerza el resultado del test de Ljung–Box. A su vez, en el gráfico inferior derecho, del histograma se desprende un comportamiento aproximadamente normal de los residuos. La distribución de los residuos es bastante simétrica y centrada en cero. La curva roja (densidad normal ajustada) se superpone bien al histograma. Habría ligeras desviaciones en las colas, pero nada dramático.

```{r forecastres, fig.cap="Análisis de los residuos del modelo ARIMA", fig.height=6, fig.width=6, echo=FALSE, message=FALSE, warning=FALSE}
checkresiduals(fit, plot=TRUE, test=FALSE)  
```


De todo lo anterior podemos concluir que:

- El modelo captura bien la estructura temporal y estacional de la serie.

- Residuos parecerían comportarsr como un ruido blanco, sin autocorrelación significativa y con distribución aproximadamente normal.

- El diagnóstico visual es coherente con el p-valor hallado en el test de Ljung–Box, entonces el modelo es estadísticamente adecuado.

\newpage
Avanzando en el análisis, es interesante analizar en qué meses se dan tanto los máximos y los mínimos en cada año para entender el componente estacional.




```{r echo=TRUE}
# slice de la de serie para ver como estan estructurados los datos
co2_s <- window(co2, start = c(1959, 1), end = c(1970, 12))
co2_s
```


```{r include=FALSE}
library(dplyr)
```



```{r echo=TRUE}
# de ts a df
co2_df <- data.frame(
  year = floor(time(co2)),
  month = cycle(co2),
  value = as.numeric(co2)
) %>%
  mutate(date = as.Date(paste(year, month, "01", sep = "-")))
```


```{r echo=TRUE}
head(co2_df)
```




```{r echo=TRUE}
# hallamos los maximos anuales
co2_max_per_year <- co2_df %>%
  group_by(year) %>%
  filter(value == max(value)) %>%
  ungroup()
co2_max_per_year
```


```{r echo=TRUE}
# cuando se dan los maximos anuales: los meses
unique(co2_max_per_year$month)
```



```{r echo=TRUE}
# minimos anuales
co2_min_per_year <- co2_df %>%
  group_by(year) %>%
  filter(value == min(value)) %>%
  ungroup()
co2_min_per_year
```

```{r echo=TRUE}
unique(co2_min_per_year$month)
```


En los meses de Abril y Mayo se detectan los picos en las emisiones de $CO_2$ mientras que en los meses de Setiembre y Octubre se registran los valores mínimos. Podríamos pensar que estos meses marcan el punto de inflexión estacional en el ciclo del carbono. La alternancia entre máximos (Abril-Mayo) y mínimos (Setiembre-Octubre) podría ser un reflejo el pulso respiratorio asociado a esa zona del planeta. Con esto, sería interesante analizar la transición entre ciclos. 




\newpage

## Pruebas de raíz unitaria y estacionariedad

Para analizar si una serie temporal es estacionaria, se suelen utilizar dos pruebas complementarias: Pruebas ADF y KPSS.

### Prueba ADF (Augmented Dickey-Fuller)

La prueba ADF se utiliza para detectar raíces unitarias en una serie temporal, es decir, para evaluar si la serie es no estacionaria (@dickey1981).

Se plantea

\[
\begin{cases}
H_0: \gamma = 0 & \text{(la serie no es estacionaria)} \\
H_1: \gamma < 0 & \text{(la serie es estacionaria)}
\end{cases}
\]

Para ello, se estiman tres modelos

\begin{align}
\text{(c)}\quad \Delta z_t &= a_0 + \gamma z_{t-1} + a_2 t + \sum_{i=2}^{p} \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
\text{(b)}\quad \Delta z_t &= a_0 + \gamma z_{t-1} + \sum_{i=2}^{p} \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
\text{(a)}\quad \Delta z_t &= \gamma z_{t-1} + \sum_{i=2}^{p} \beta_i \Delta z_{t-i+1} + \varepsilon_t
\end{align}

Los modelos se aplican de lo más general (modelo c: con constante y tendencia) a lo más simple (modelo a: sin términos determinísticos).

En cada caso se evalúa si \( \gamma = 0 \) mediante estadísticos tipo \( \tau \), y también se pueden hacer pruebas conjuntas con estadísticos \( \phi \) (pruebas \( F \)).

\bigskip

Recomendación práctica: Al desconocer la verdadera forma del modelo, se comienza con el más general. Si se rechaza \( H_0 \) en ese modelo, no es necesario probar los más simples. Si no se rechaza, se procede a simplificar y volver a testear.



### Prueba KPSS (Kwiatkowski–Phillips–Schmidt–Shin)



La prueba de KPSS se utiliza para verificar si una serie temporal es estacionaria [@kpss]. Parte de un modelo que descompone la serie como la suma de varios componentes

\[
x_t = \mu + \alpha t + u_t + \varepsilon_t,
\]

donde
\begin{itemize}
  \item \( \mu \) es una constante (drift),
  \item \( \alpha t \) es una tendencia determinística,
  \item \( u_t \) es un paseo aleatorio,
  \item \( \varepsilon_t \) es un error estacionario.
\end{itemize}

El paseo aleatorio \( u_t \) se define como $u_t = u_{t-1} + a_t, \quad a_t \overset{iid}{\sim} \mathcal{N}(0, \sigma_a^2).$


Se testea 

\begin{align*}
H_0 &: \sigma_a^2 = 0 \quad \text{(la serie es estacionaria)} \\
H_1 &: \sigma_a^2 > 0 \quad \text{(la serie no es estacionaria)}
\end{align*}



La prueba KPSS puede aplicarse sobre tres formas distintas del modelo, según los términos determinísticos considerados:


- Sin constante ni tendencia:  $x_t = u_t + \varepsilon_t$.


- Con constante, sin tendencia: $x_t = \mu + u_t + \varepsilon_t$.


- Con constante y tendencia: $x_t = \mu + \alpha t + u_t + \varepsilon_t$




El estadístico de KPSS mide la magnitud acumulada de los residuos de la regresión. Se compara con valores críticos preestablecidos. 

La regla de decisión es

- Si el valor-p es menor que 0.05, se rechaza \( H_0 \): la serie no es estacionaria.

- Si el valor-p es mayor que 0.05, no se rechaza \( H_0 \): la serie es estacionaria.


\vspace{1cm}




Una vez que se ha ajustado la serie, es posible aplicar estas pruebas tanto sobre la serie limpia de componentes completa o sobre bloques de valores extremos, dependiendo del objetivo del análisis.




:::{.example #DJ  name="Pruebas de RU y estacionariedad: Indice Dow Jones"}
Las técnicas de valores extremos han ganado popularidad en aplicaciones
financieras. Esto no es sorprendente: la solvencia financiera de una
inversión probablemente esté determinada por cambios extremos en las
condiciones del mercado, en lugar de cambios típicos. Sin embargo, la
compleja estructura estocástica de los mercados financieros implica que
una aplicación "ingenua" de técnicas de valores extremos puede ser engañosa (@coles2001introduction).
:::

Consideremos los siguientes datos con los valores de cierre diario del índice Dow Jones en el período de 1996 a 2000.

```{r echo=TRUE}
data(dowjones) # del paquete ismev
head(dowjones)
```




```{r dji, fig.cap = "Valores de cierre diario del índice Dow Jones (1996-2000)", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
plot(x=dowjones$Date, y=dowjones$Index, type="l", col="blue", xlab="Fecha", ylab="índice Dow Jones")
```

La Figura \@ref(fig:dji) muestra cómo fue cambiando el valor del índice Dow Jones día a día durante cinco años. Se nota que el nivel general del índice varió mucho en ese tiempo, por lo que resulta difícil ver con claridad los valores extremos del día a día: quedan "tapados" por los grandes cambios a largo plazo.




Para este tipo de series, muchos estudios muestran que una buena forma de hacerla más estable (estacionaria) es calcular los logaritmos de los cocientes entre valores de días consecutivos. A eso se le llama retorno logarítmico diario. 

El retorno logarítmico diario se puede definir como

\[
r_t = \log\left( \frac{I_t}{I_{t-1}} \right) = \log(I_t) - \log(I_{t-1}),
\]

donde \( I_t \) representa el precio de cierre del activo (el índice Dow Jones) en el día \( t \).

Esta transformación estabiliza la varianza y facilita el análisis estadístico de la serie.

```{r echo=TRUE}
# Calcular retornos logarítmicos diarios
dowjones$LogReturn <- c(NA, diff(log(dowjones$Index)))
# El primer valor será NA porque no hay dato anterior para el día 1.


# Ver resultado
head(dowjones)
```


La Fig. \@ref(fig:retornodji) sugiere una transformación razonablemente exitosa
hacia la estacionariedad. El análisis de las propiedades de valores
extremos de dichas series transformadas puede proporcionar a los
analistas financieros información clave sobre el mercado
(@coles2001introduction).



```{r retornodji, fig.cap = "Retorno logarítmico diario del índice Dow Jones (1996-2000)", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
plot(x=dowjones$Date, y=dowjones$LogReturn, type="l", col="darkgrey", xlab="Fecha", ylab="retornos logarítmicos diarios")
```

Antes de correr las pruebas sobre la serie transformada es necesario eliminar (y reemplazar) datos faltantes y valores infinitos dados por el logaritmo.
\vspace{0.3cm}
```{r}
any(is.na(dowjones$LogReturn))      # TRUE si hay NA
any(!is.finite(dowjones$LogReturn)) # TRUE si hay Inf o -Inf
```

```{r}
# Reemplazar todo Inf y -Inf por 0
dowjones$LogReturn[is.infinite(dowjones$LogReturn)] <- 0

# Eliminar NA (puede ser el primero)
dowjones <- dowjones[!is.na(dowjones$LogReturn), ]
```

\vspace{0.5cm}

Aplicamos las pruebas de RU y KPSS:

```{r warning=FALSE}
#install.packages("aTSA")
library(aTSA)

# Test ADF
aTSA::adf.test(dowjones$LogReturn, output=TRUE)
```

En los tres modelos evaluados (sin constante ni tendencia, con constante, y con constante y tendencia), todos los valores del estadístico ADF son muy negativos (por ejemplo, −35.6, −26.2, etc.), y todos los $valores-p$ son menores  a $\alpha=0.05$.

Conclusión ADF: Se rechaza la hipótesis nula en todos los casos. Por lo tanto, la serie de retornos logarítmicos es estacionaria según esta prueba.

\newpage

```{r}
# Test KPSS
aTSA::kpss.test(dowjones$LogReturn,  lag.short = TRUE, output = TRUE)
```


- Modelo sin constante ni tendencia: valor-p = 0.0153 $\Rightarrow$ se rechaza $H_0$.

- Modelos con constante y con constante + tendencia: valores-p = 0.10 $\Rightarrow$ no se rechaza $H_0$.



Conclusión KPSS:

- En el modelo más simple (sin constante ni tendencia), se rechaza la estacionariedad.

- En los modelos con constante y/o tendencia, no se rechaza la estacionariedad.


Ambas pruebas coinciden en los modelos más completos (con constante y tendencia): la serie es estacionaria.
Esto es coherente con el comportamiento esperado de una serie de retornos financieros, que usualmente son estacionarios.

\vspace{1cm}
Los datos del índice Dow Jones ofrecen un ejemplo para explorar la utilidad del enfoque POT.




<!--

\bigskip

\textit{Limitaciones:}  
Cuando existe una raíz unitaria, los coeficientes determinísticos pueden ser estimados con baja precisión. Esto puede dificultar el rechazo de \( H_0 \) incluso cuando la serie es estacionaria en realidad.

-->
<!--

### Concepto de series de tiempo estacionarias
Siguiendo a @enders, se dice que un proceso estocástico $z_{t}$ con media y varianza finitas es estacionario en covarianza para todo $t$ y $t-s$ cuando

\begin{align}
	\label{eq:2_7}
	E(z_{t})&=E(z_{t-s})=\mu\\
	\label{eq:2_8}
	var(z_t)&=var(z_{t-s})=\sigma^2_z\\
	\label{eq:2_9}
	cov(z_t,z_{t-s})&=cov(z_{t-j},z_{t-j-s})=\gamma_s
\end{align}
donde $\mu, \sigma^2_z, \gamma_s$ con constantes.

En \eqref{eq:2_8}, cuando $s=0$ se va a tener que $\gamma_0$ es la varianza de $z_t$. Que una serie temporal sea estacionaria en covarianza implica que tanto la media como todas sus autocovarianzas\footnote{También se habla de proceso debilmente estacionario o estacionario de segundo orden.} no están afectadas por cambios en los orígenes temporales\footnote{En modelos multivariados, el término autocovarianza refiere a la covarianza entre $z_t$ y sus propios rezagos mientras que covarianza cruzada refiere a la covarianza entre series temporales}.

Dado que se está en un marco de ecuaciones en diferencias, se tienen que cumplir condiciones de estabilidad en el sistema y esto implica que las raíces características asociadas al sistema de interés caígan dentro del círculo unitario. Cuando una serie temporal no sea estacionaria, es posible que presente una evidente tendencia, medias y varianzas que no son constantes en el tiempo.
\end{comment}



### Pruebas ADF

Se testea para cada serie de datos

\begin{align*}
	&H_0)\; \gamma = 0\\
	&H_1)\; \gamma \neq 0   
\end{align*}

En este sentido, se consideran tres ecuaciones de regresión distintas para poner a prueba $H_0$,

\begin{align}
	\label{eq: modelo_c}
	\Delta z_t=&\gamma z_{t-1} + \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
	\label{eq: modelo_b}
	\Delta z_t=&a_0+\gamma z_{t-1} + \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
	\label{eq: modelo_a}
	\Delta z_t=&a_0+\gamma z_{t-1} + a_2 t+ \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t
\end{align}

Existen tres estadísticos $\tau,\;\tau_{\mu},\;\tau_{\tau}$ para probar la hipótesis nula $H_0)\:\gamma=0$ en cada caso y además, se tienen otros tres $F-$estadísticos $\:\phi_1,\:\phi_2,\:\phi_3$ para hacer pruebas conjuntas sobre los coeficientes.  

Los estadísticos $\:\phi_1,\:\phi_2,\:\phi_3$ se construyen como pruebas $F$:

\begin{equation*}
	\phi_{i}=\frac{\left[SSR_{restringido}-SSR_{no\:restringido}\right]/r}{SSR_{no\:restringido}/(t-k)}    
\end{equation*}

donde SSR es la suma de los cuadrados de los residuos en los modelos restringidos y no restringidos, $r$ es la cantidad de restricciones, $T$ es la cantidad de observaciones,  $k$ es la cantidad de parámetros estimados en el modelo irrestricto, $i=1,2,3$. A su vez, $T-k$ van a ser los grados de libertad del modelos sin restricciones. Los valores de los coeficientes estimados se van a comparar con los valores críticos de tablas reportados por @dickey1981.
La hipótesis nula indica que el proceso de generación de los datos es la del modelo restringido contra la hipótesis alternativa de que los datos son generados por el modelo sin restringir. Cuando los valores de $\:\phi_1,\:\phi_2,\:\phi_3$ sean mayores a los valores críticos reportados por @dickey1981 se rechaza la hipótesis nula, cuando sean menores a los valores críticos entonces no se rechaza la hipótesis nula.



En el siguiente cuadro \@ref(tab:modelos_adf) se consideran los tres modelos y cada una de las hipótesis a testear con sus respectivos estadísticos.



\begin{table}[h!]
	\centering
	\small % Smaller font size
	\setlength{\tabcolsep}{3pt} % Adjust column spacing
	\caption{Modelos del test ADF}
	\label{tab: modelos_adf}
	\begin{tabular}{|c|c|c|c|}
		\hline 
		& Modelo & $H_0)$ & Estadistíco de prueba \\
		\hline 
		\hline 
		c & $\Delta z_{t}=a_{0}+\gamma z_{t-1}+a_{2}t+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\begin{array}{c}
			\gamma=0\\
			\gamma=a_{2}=0\\
			\gamma=a_{2}=a_{0}=0
		\end{array}$ & $\begin{array}{c}
			\tau_{\tau}\\
			\phi_{3}\\
			\phi_{2}
		\end{array}$ \\
		\hline 
		b & $\Delta z_{t}=a_{0}+\gamma z_{t-1}+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\begin{array}{c}
			\gamma=0\\
			\gamma=a_{0}=0
		\end{array}$ & $\begin{array}{c}
			\tau_{\mu}\\
			\phi_{1}
		\end{array}$ \\
		\hline 
		a & $\Delta z_{t}=\gamma z_{t-1}+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\gamma=0$ & $\tau$ \\
		\hline 
	\end{tabular}
\end{table}


```{r tab:modelos_adf, echo=FALSE}
library(kableExtra)

# Create the table as a data frame
adf_table <- data.frame(
  Modelo = c("c", "b", "a"),
  Ecuación = c(
    "$\\Delta z_t = a_0 + \\gamma z_{t-1} + a_2 t + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$",
    "$\\Delta z_t = a_0 + \\gamma z_{t-1} + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$",
    "$\\Delta z_t = \\gamma z_{t-1} + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$"
  ),
  `H_0` = c(
    "$\\begin{array}{c} \\gamma = 0 \\\\ \\gamma = a_2 = 0 \\\\ \\gamma = a_2 = a_0 = 0 \\end{array}$",
    "$\\begin{array}{c} \\gamma = 0 \\\\ \\gamma = a_0 = 0 \\end{array}$",
    "$\\gamma = 0$"
  ),
  `Estadístico` = c(
    "$\\begin{array}{c} \\tau_{\\tau} \\\\ \\phi_3 \\\\ \\phi_2 \\end{array}$",
    "$\\begin{array}{c} \\tau_{\\mu} \\\\ \\phi_1 \\end{array}$",
    "$\\tau$"
  )
)

# Render the table
kbl(adf_table, booktabs = TRUE, escape = FALSE,
    caption = "Modelos del test ADF",
    col.names = c("", "Modelo", "$H_0$", "Estadístico de prueba")) %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 9)
```



Cuando no se conozca el proceso de generación de los datos, se sugiere realizar las pruebas de Dickey-Fuller Aumentado partiendo del modelo
menos restrictivo para cada serie temporal a uno más particular. Si bien las pruebas de ADF son útiles para detectar la presencia de raíces unitarias, los mismos tienen sus limitaciones. Partiendo del modelo general al particular, cada prueba está condicionada a que las pruebas anteriores sean correctas.  Cuando se empieza por el primer paso, es decir, con el modelo (c) con constante y con tendencia, se hace más difícil rechazar $H_0)$, por lo tanto, cuando se rechace la hipótesis nula en un modelo (c) se tiende a rechazar también la hipótesis nula cuando no se incluyan los términos deterministas.  A su vez, establece que el problema principal de las pruebas de Dickey-Fuller es que tanto el intercepto como la pendiente de la tendencia son, con frecuencia, estimados de manera \textit{pobre}  bajo la presencia de raíces unitarias. En general, se tiende a no rechazar la hipótesis nula de raíz unitaria incluso cuando el verdadero valor de $\gamma$ no es cero. Además, la prueba presenta limitaciones también frente a cambios de régimen 


(@enders).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

### Pruebas KPSS

En @kpss, se parte de una representación cada serie temporal como la suma de un componente de tendencia determinística, un paseo aleatorio y un error estacionario. En este contexto, se pone a prueba


\begin{align*}
	H_0)&\text{ la serie es estacionaria alrededor de una tendencia}    \\
	H_1)&\text{ la serie es no estacionaria}
\end{align*}

que se corresponde con la hipótesis de que la varianza del paseo aleatorio (_random walk_) es igual a cero. 

Se emplea un estadístico de Multiplicadores de Lagrange (ML) para testear la hipótesis nula de estacionariedad. De esta manera, siendo $z_t$ con $t=1,2,...,T$ las series a las que se les quiere aplicar el test, se asume que se puede descomponer a la serie en la suma de un componente de tendencia determinística, un paseo aleatorio y un error estacionario se tiene que,

\begin{equation}
	z_t=\xi t + r_t + \varepsilon_t
(\#eq:kpss2)
\end{equation}

Donde $r_t$ es un paseo aleatorio:

\begin{equation}
	r_t = r_{t-1} + u_t,
		(\#eq:kpss3)
\end{equation}

donde $u_t$ es $iid(0,\sigma_u^2)$. El valor inicial $r_0$  es fijo y sirve se intercepto. La hipótesis de estacionariedad es $\sigma_u^2=0$ y como se asume que $\varepsilon_t$ es estacionario, bajo la hipótesis nula $z_t$ es estacionaria alrededor de una tendencia.

En el caso particular de que en el modelo \@ref(eq: kpss2) se tenga $\xi=0$, bajo la hipótesis nula $z_t$ va a ser estacionaria alrededor de una constante ($r_0$).

Sean $e_t$ con $t=1,2,...,T$, los residuos de la regresión $z$ con un intercepto y tendencia. A su vez, sea $\hat{\sigma_\varepsilon^2}$ la estimación del error de la varianza de la regresión (suma de los residuos al cuadrado divida $T$). Con lo anterior, se define el proceso de suma parcial de los residuos como

\begin{equation}
	S_t=\sum_{i=1}^t r_i, \quad t=1,....,T   
		(\#eq:kpss5)
\end{equation}

Entonces el estadístico ML es 

\begin{equation}
	\label{eq: kpss6}
	ML=\sum_{t=1}^T S^2_t/\hat{\sigma^2_\varepsilon}   
\end{equation}

En el caso de que se quiera poner a prueba la hipótesis nula de estacionariedad alrededor de una constante se define $e_t$ como los residuos de la regresión $z$ sobre un intercepto ($e_t=z_t-\bar{z})$. Cabe resaltar que es una prueba de cola superior y se reportan los valores críticos. Además, para este caso se asume que los errores $\varepsilon_t \overset{\text{iid}}{\sim} \mathcal{N}(0,\sigma_{\varepsilon}^2)$
Sin embargo, se puede extender la prueba con supuestos más débiles sobre la distribución de los errores dado que el supuesto anterior puede ser poco realista.

-->

