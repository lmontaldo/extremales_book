
```{r include=FALSE}
library(dplyr)
library(tidyverse)
```


# Un primer enfoque de datos no $iid$ {#noiid}

 

## Datos no $iid$

Vamos a ver que en la realidad frecuentemente los datos no son $iid$, y
que hay muy diversos tipos de datos no $iid$.

Recordemos que si los datos no son $iid$ es porque los datos:

-   no son independientes,
-   su distribución no es la misma,
-   no son independientes y su distribución no es la misma.

Nota: Los tests que permiten contrastar la hipótesis $iid$ son:
Spearman, runs up and down, etc. En R se encuentran como randtests.

```{=html}
<!--
Aclaración 1: Dado que abordamos un curso de
posgrado, y que en los cursos básicos se abordan
los tests que permiten contrastar la hipótesis iid
(Spearman, runs up and down, etc.) supondremos
estas técnicas conocidas, pero obviamente podemos
analizarlas aparte con quien lo necesite. En R, por
ejemplo, se encuentran como randtests. Un escalón
más arriba, las Series de Tiempo, su análisis
espectral, etc., es tema de otros curso de segundo
nivel de Estadística que ofrecemos, por lo cual no
nos detendremos mayormente, pero nuevamente,
estamos a disposición de quien lo requiera y en este
curso al menos explicaremos brevemente los
aspectos medulares que usemos.
-->
```

## Procesos estacionarios

Una secuencia de variables aleatorias independientes es, en general, estacionaria. Sin embargo, la estacionariedad admite dependecia en las variables de una serie pero implica que sus propiedades estocásticas sean homogéneas en el tiempo. Si $X_1,X_2, \dots$ es una serie estacionaria, entonces $X_1$ tiene la misma distribución que $X_{60}$, la distribución conjunta de $(X_1, X_2)$ es la misma que $X_{101}$ y $X_{102}$ pero no es necesario que $X_1$ sea independiente de $X_2$ o de $X_102$. La dependencia en las series estacionarias puede tomar múltiples formas por lo que se hace dificil realizar generalizaciones. Para el caso extremal, es necesario imponer restricciones: se puede suponer que los valores extremos son independientes cuando están lo suficientemente apartados en el tiempo. Muchas series estacionarias satisfacen esto y además es una condición posible para muchos procesos físicos. Eliminar la dependencia de largo plazo para valores extremos, nos permite poner el foco en el efecto de la dependencia de corto plazo.  Cuando los datos extremales sean estacionarios podremos modelarlos a través del método de máximos por bloques con distribuciones GEV como vimos anteriormente y como veremos luego, por métodos de umbrales [@coles2001introduction].




Entonces, cuando los datos:

-   No necesariamente son independientes pero la estructura de
    distribuciones es siempre la misma, se habla de datos estacionarios
    o *procesos estacionarios*.

-   No son independientes pero la dependencia se va atenuando a medida
    que se consideran datos más lejanos (para fijar ideas, imaginemos
    que los datos corresponde a medidas en el tiempo, y que datos muy
    viejos no influirían significativamente sobre el presente, por
    ejemplo), se habla de datos débilmente dependientes.

El caso más sencillo de esta situación es la de datos $m$-dependientes,
siendo $m$ un número; si los datos que están a una distancia mayor que
$m$ entonces son independientes. Un ejemplo son los procesos llamados
promedios móviles de orden $q$, $MA(q)$[^1] con $m=q$.

[^1]: Moving Averages en inglés.

Algunos datos bioquímicos del agua de determinadas playas uruguayas que
se miden diariamente suelen ser 7-dependientes, por ejemplo. Esto
significa que datos que fueron tomados con más de una semana de
separación, son independientes. En cambio, algunos datos de poblaciones
o de composición de suelos que se van registrando a lo largo de los
años, son débilmente dependientes, pero no son $m$-dependientes para
ningún $m$. Es el caso de los llamados procesos autorregresivos de orden
$p$, $AR(p)$, donde cada dato es una combinación lineal de los $p$
anteriores más un término aleatorio que es un ruido blanco. Esto
procesos son Markovianos, es decir, de memoria infinita: todo el pasado
es tan informativo como los últimos $p$ datos[^2]. En este sentido, la
genética aporta excelentes ejemplos para distinguir los procesos
markovianos. Esto es así porque algunas pocas generaciones atrás
alcanzan para calcular probabilidades de muchas características de la
descendencia, mientras que la memoria no es corta sino infinita: cada
característica resulta de toda la fiogenética, así nos remontemos al
origen de las especies.

[^2]: El carácter Markoviano es mal presentado en algunos textos y
    cursos, hablando confusamente de *memoria corta*

<!--- EJEMPLO DEL LIBRO A REVEER -->

```{=html}
<!--- Para simplificar al extremo y usando la
imagen bíblica (con todo respeto a todos los
pensares), si Adán y Eva hubieran tenido otros
genes, la especie podría ser diferente!!-->
```
En la teoría clásica de series de tiempo, una estructura $AR(p)$ a
partir de un proceso $MA(q)$, da lugar a los procesos $ARMA(p,q)$, que
en general son débilmente dependientes y aproximan bien a cualquier
proceso estacionario débilmente dependiente[^3]. Lo opuesto a
dependencia débil es dependencia fuerte[^4]. En algunos datos de
telecomunicaciones que se registran en escalas temporales muy finas, se
han encontrado ejemplos de dependencia fuerte.

[^3]: Lo cual es una de las razones de su popularidad.

[^4]: En inglés, long range dependence.

\newpage 
## Descomposición de series temporales

Las series temporales no estacionarias contienen características que cambian sistemáticamente con el tiempo. Por ejemplo, los procesos asociados al medio ambiente están sujetos a los efectos estacionales, a tendencias y también a efectos del cambio climático de más largo plazo [@coles2001introduction]. Lo anterior implica se puede descomponer a una serie temporal en distintos componentes: tendencia-ciclo, estacional, irregular. Se emplean métodos estadísticos para extraer estos componentes con el objetivo de entender a los patrones subyacentes de la serie estudiada y obtener predicciones confiables [@enders].

Siguiendo @hyndman, los distintos componentes se pueden describir de la siguiente manera:

- Tendencia-ciclo: Se vincula a los movimientos de largo plazo. Los ciclos en particular, son movimientos irregulares. Es decir, podemos pensar a los ciclos como fluctuaciones de largo plazo, no necesariamente regulares, asociadas a fenómenos económicos, sociales o naturales (como recesiones o expansiones). A diferencia de la estacionalidad, los ciclos no tienen una duración fija.

- Estacionalidad: Son fluctuaciones periódicas que se repiten a intervalos regulares, como días, meses o años. Por ejemplo, ventas más altas en diciembre o temperaturas más bajas en invierno.

- Irregularidad: Todo lo que no es lo anterior. Es decir, son variaciones impredecibles, aleatorias o residuales, causadas por factores no sistemáticos. Es un componente residual.


Existen dos modelos generales para descomponer una serie temporal: el modelo aditivo y el modelo multiplicativo.

__Modelo aditivo :__ Se utiliza cuando la variación estacional es aproximadamente constante a lo largo del tiempo.

$$
Y_t = T_t + S_t + R_t
$$

donde \(Y_t\) es la serie original, \(T_t\) la tendencia, \(S_t\) la estacionalidad y \(R_t\) el residuo.

__Modelo multiplicativo :__ Se aplica cuando la variación estacional cambia con el nivel de la serie (heterocedasticidad).

$$
Y_t = T_t \times S_t \times R_t
$$
<!--
A modo ilustrativo en la Fig.\@ref(fig:adit) , tomamos el ejemplo de @hyndman donde se muestran los distintos componentes de una serie temporal en un modelo aditivo.



```{r adit, fig.cap="Número total de personas empleadas en el comercio minorista de EE.UU. y sus tres componentes" ,echo=FALSE, fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
#install.packages("fpp3")
library(fpp3)
us_retail_employment <- us_employment |>
  filter(year(Month) >= 1990, Title == "Retail Trade") |>
  select(-Series_ID)
dcmp <- us_retail_employment |>
  model(stl = STL(Employed))
components(dcmp) |> autoplot()
```


-->

<!---


Si esta función es monótona, se habla de que los datos presentan
tendencias (trends). Si en cambio la función es periódica, se dice que
los datos presentan ciclos (seasonal effects).

  [Text to display](https://otexts.com/fpp3/decomposition.html) 

Se puede demostrar que si los datos se vuelven iid al restar una función
determinística cualquiera (lo cual no siempre es el caso!), entonces se
pueden modelar aproximadamente como TENDENCIAS+CICLOS+ DATOS IID.

Al descomponer una serie temporal, a veces es útil primero transformar o
ajustar la serie para que la descomposición, y su análisis posterior,
sea lo más sencilla posible (@hyndman).

-->

Considerando lo anterior, cuando los datos no son $iid$ pero se vuelven $iid$ al restarle una
función determinística, son bastante simples de manejar.

::: {.example #datosnoiid name="Datos no estacionarios"}
Vamos a presentar a continuación como “lucen” los distintos tipos de
datos y qué tan reconocibles son. Los datos se han simulado
computacionalmente, por lo tanto sabemos a cuál es su real estructura.

1- Empecemos por lo básico”:¿Cómo lucen datos $iid$? A continuación una
muestra de tamaño mil de datos $iid$ con distribución Gumbel (Fig.\@ref(fig:datosiid)).

2- Comparación de la distribución anterior con una distribución normal (Fig.\@ref(fig:compnorm)).

3- Simulación de datos con distribución Gumbel $iid$ con una tendencia parabólica y ciclos sinusoidales.

4- Restar los componentes de tendencia y ciclo de la serie simulada en 3-.
:::

1- Simulación de una distribución Gumbel con 1000 daots $iid$.

```{r datosiid, fig.cap="Mil datos iid simulados con una distribución Gumbel" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
set.seed(123)
n <- 1000
library(evd)
datos_iid <- rgumbel(n)
plot(datos_iid, type = "l", col = "steelblue", lwd = 1.2,
     xlab = "Tiempo", ylab = "Valor")
```

Lo más sobresaliente en la visualización es la presencia de
variaciones abruptas en los datos, que dificultan su representación
gráfica e interpretación inmediata. Estas oscilaciones no son
necesariamente equilibradas en torno a un eje horizontal, lo cual se
debe a la asimetría de la distribución Gumbel, que presenta una cola
más larga hacia la izquierda.


```{r echo=TRUE}
# Calcular media y mediana
media_gumbel <- mean(datos_iid)
mediana_gumbel <- median(datos_iid)
```

```{r echo=FALSE}
cat("Media:", round(media_gumbel, 4), "\n")
cat("Mediana:", round(mediana_gumbel, 4), "\n")
```



Aunque su valor esperado es aproximadamente 0.54, la mediana se
encuentra en torno a -0.31, reflejando dicha asimetría. 

2- Si simulamos datos a partir de una distribución simétrica, como la Normal,
podemos observar un mayor equilibrio visual entre los picos y valles
de la serie. Sin embargo, eso podría desviar la atención del aspecto
esencial que son las oscilaciones intensas. Además, el conjunto de datos no muestra una tendencia clara ni evidencia de ciclos persistentes.



```{r compnorm, fig.cap="Mil datos iid simulados con una distribución Gumbel y una Normal" ,echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
set.seed(123)
n <- 1000
library(evd)
library(ggplot2)
library(dplyr)

# Simular datos
datos <- tibble(
  tiempo = 1:n,
  Gumbel = rgumbel(n),
  Normal = rnorm(n)
) %>%
  tidyr::pivot_longer(cols = c(Gumbel, Normal), names_to = "Distribución", values_to = "Valor")

# Graficar
ggplot(datos, aes(x = tiempo, y = Valor, color = Distribución)) +
  geom_line() +
  scale_color_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  facet_wrap(~Distribución, ncol = 1, scales = "free_y") +
  labs(x = "Tiempo", y = "Valor") +
  theme_minimal() +
  theme(legend.position = "none")

```

De las Fig. \@ref(fig:compnorm) y \@ref(fig:distribmar) desprende que :

-   Gumbel muestra más valores extremos hacia la izquierda, presenta asimetría
    negativa (Fig.\@ref(fig:distribmar))



-   Normal muestra oscilaciones más simétricas alrededor de cero (Fig. \@ref(fig:compnorm)).

```{r distribmar, fig.cap="Comparación de distribuciones marginales: Gumbel vs. Normal" ,echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
ggplot(datos, aes(x = Valor, fill = Distribución, color = Distribución)) +
  geom_histogram(aes(y = after_stat(density)), bins = 40, alpha = 0.4, position = "identity") +
  geom_density(linewidth  = 1.2, alpha = 0.2)+
  scale_fill_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  scale_color_manual(values = c("Gumbel" = "steelblue", "Normal" = "purple")) +
  facet_wrap(~Distribución, ncol = 1, scales = "free") +
  labs(x = "Valor", y = "Densidad") +
  theme_minimal()
```

Este comportamiento es típico de secuencias $iid$: aunque presentan
variabilidad local intensa, su estructura global carece de patrones de
dependencia temporal.





3-Simulamos 500 datos con una distribución Gumbel más un componente de tendencia y ciclo. El modelo simulado puede expresarse como

\begin{equation} 
Y_t = X_t + \underbrace{a t^2 + b t + c}_{\text{tendencia cuadrática}} + \underbrace{A \sin(\omega t + \phi)}_{\text{componente cíclica}}, \quad t = 1, 2, \dots, n,
  (\#eq:simu)
\end{equation} 




donde:

- \( X_t \overset{\text{iid}}{\sim} \text{Gumbel}(\mu, \sigma) \) son los datos $iid$ con distribución Gumbel,
- \( a, b, c \in \mathbb{R} \) son coeficientes de la tendencia polinómica de segundo orden,
- \( A \) es la amplitud del ciclo sinusoidal,
- \( \omega \) es la frecuencia angular,
- \( \phi \) es el desfase,
- \( n \) es el número total de observaciones.





```{r echo=FALSE, warning=FALSE}
# Parámetros y simulación
set.seed(123)
n <- 500
t <- 1:n

# Ruido iid Gumbel
gumbel_noise <- rgumbel(n, loc = 0, scale = 1.2)

# Tendencia cuadrática: a * t^2 + b * t + c
a <- 0.0002
b <- 0.02
c <- 0
trend <- a * t^2 + b * t + c

# Componente cíclica: A * sin(omega * t + phi)
A <- 3
omega <- 2 * pi / 50
phi <- 0
cycle <- A * sin(omega * t + phi)

# Serie observada
Y <- gumbel_noise + trend + cycle
```


```{r gumbelt, fig.cap="Serie simulada: Gumbel + tendencia cuadrática + ciclo sinusoidal" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Armar data frame
df <- data.frame(t = t, Y = Y, trend = trend, cycle = cycle)

# Gráfico
ggplot(df, aes(x = t, y = Y)) +
  geom_line(color = "steelblue", size = 0.8) +
  labs(x = "Tiempo", y = "Valor observado") +
  theme_minimal()
```



De la Fig. \@ref(fig:gumbelt) se desprende que si bien hay oscilaciones aleatorias, es perceptible a simple vista una tendencia creciente y la presencia de una estructura cíclica.

Sin conocer el modelo de simulación de la Eq. \@ref(eq:simu), sólo con observar la Fig. \@ref(fig:gumbelt) y trás un poco de análisis exploratorio unx puede intuir que la tendencia debe ser lineal o cuadrática. Si se estima un polinomio de orden 2 más un término sinusoidal (con parámetros de amplitud, frecuencia y fase), mediante mínimos cuadrados se puede ajustar bien (Fig. \@ref(fig:gumbelmco)).


```{r include=FALSE}
# Usar el mismo omega que en la simulación
omega <- 2 * pi / 50

# Crear variables para el modelo
df$tt <- df$t
df$tt2 <- df$t^2
df$sin_term <- sin(omega * df$t)
df$cos_term <- cos(omega * df$t)

# Ajuste por mínimos cuadrados
modelo_mco <- lm(Y ~ tt2 + tt + sin_term + cos_term, data = df)

# Resumen del modelo
summary(modelo_mco)
```





```{r gumbelmco, fig.cap="Ajuste por mínimos cuadrados: tendencia cuadrática + componente sinusoidal" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Gráfico: datos y ajuste
df$ajuste <- predict(modelo_mco)

library(ggplot2)
ggplot(df, aes(x = t)) +
  geom_line(aes(y = Y), color = "steelblue", size = 0.8, alpha = 0.6) +
  geom_line(aes(y = ajuste), color = "darkred", size = 0.9) +
  labs(x = "Tiempo", y = "Valor") +
  theme_minimal()
```




4 - Si se le restan a los datos originales los valores de las componentes determinísticas —la tendencia y el ciclo ajustados por mínimos cuadrados—, se obtienen los **residuos**:

$$
\hat{\varepsilon}_t = Y_t - \hat{T}_t - \hat{C}_t,
$$

donde \( \hat{T}_t \) es la tendencia estimada y \( \hat{C}_t \) la componente cíclica estimada.

```{r echo=TRUE}
# Calcular residuos
df$residuos <- df$Y - df$ajuste
```

Estos residuos (Fig. \@ref(fig:residgumbelmco)) deberían comportarse como ruido aleatorio, y en particular, deberían superar razonablemente los tests para datos independientes e idénticamente distribuidos ($iid$). A continuación se presentan algunas herramientas para tomar este tipo de conclusiones sobre los residuos.


```{r residgumbelmco, fig.cap="Residuos del modelo ajustado" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Graficar los residuos
library(ggplot2)
ggplot(df, aes(x = t, y = residuos)) +
  geom_line(color = "gray30", size = 0.7) +
  labs(
       x = "Tiempo", y = "Residuos") +
  theme_minimal()
```


- Herramientas de diagnóticos visual: Sirven para diagnosticar la dependencia temporal de una serie: si la serie es ruido blanco ($iid$), tanto la ACF como la PACF mostrarán valores cercanos a cero para todos los retardos (excepto en el lag 0).


  - La función de autocorrelación (ACF) mide la correlación entre los valores actuales de una serie temporal \( Y_t \) y sus valores pasados \( Y_{t-k} \), para diferentes retardos \( k \). Es decir, nos dice cuánto del valor actual está relacionado con sus propios valores pasados, incluyendo cualquier efecto intermedio. Nos indica qué tan correlacionado está \( Y_t \) con \( Y_{t-k} \), sin tener en cuenta lo que ocurre entre medio.

  - La función de autocorrelación parcial (PACF) mide la correlación directa entre \( Y_t \) y \( Y_{t-k} \), eliminando el efecto de los retardos intermedios \( Y_{t-1}, Y_{t-2}, \dots, Y_{t-k+1} \).
 






```{r acfpacfcombined, fig.cap = "ACF y PACF de los residuos", echo=FALSE, fig.height=4, fig.width=6, message=FALSE, warning=FALSE}
# Establecer diseño: 2 filas, 1 columna
par(mfrow = c(2, 1), mar = c(4, 4, 2, 1))

# ACF
acf(df$residuos, main = "")

# PACF
pacf(df$residuos, main = "")
```



\newpage

- El resultado del test de Box-Ljung, que se usa para evaluar si los residuos presentan autocorrelación (es decir, si son dependientes entre sí). Se pone a prueba $H_0:$ los residuos son independientes (no autocorrelacionados).


```{r echo=FALSE}
# Test de Ljung-Box para autocorrelación (independencia)
Box.test(df$residuos, lag = 20, type = "Ljung-Box")
```


Como el valor-\(p = 0.5551\) es mayor que 0.05, no se rechaza la hipótesis nula.

Entonces, no hay evidencia de autocorrelación en los residuos.

Esto es positivo, ya que indica que el modelo ajustado capturó correctamente la estructura determinística (tendencia + ciclo), y lo que queda se comporta como \textit{ruido aleatorio} (\textit{iid}).

<!--

```{r residuoshist, fig.cap = "Histograma de residuos",echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
ggplot(df, aes(x = residuos)) +
  geom_histogram(color = "black", fill = "steelblue", bins = 30) +
  labs(x = "Residuos", y = "Frecuencia") +
  theme_minimal()
```



```{r hist, fig.cap="QQ-plot de residuos" ,echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# QQ-plot para evaluar distribución
qqnorm(df$residuos)
qqline(df$residuos, col = "red")
```

-->

```{r echo=FALSE, warning=FALSE}
# Simular datos Gumbel iid
set.seed(123)
n <- 500
gumbel_iid <- rgumbel(n, loc = 0, scale = 1)

# Proceso 2-dependiente
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 1)
a1 <- 0.5
a2 <- 0.3

X_dep <- numeric(n)
for (t in 3:(n+2)) {
  X_dep[t-2] <- a1 * gumbel_full[t-1] + a2 * gumbel_full[t-2] + gumbel_full[t]
}

# Crear data frame en formato largo manualmente
df_long <- data.frame(
  time = rep(1:n, 2),
  Valor = c(gumbel_iid, X_dep),
  Proceso = factor(rep(c("IID", "Dependiente"), each = n))
)

# Graficar comparación
ggplot(df_long, aes(x = time, y = Valor, color = Proceso)) +
  geom_line(alpha = 0.8) +
  labs(title = "Comparación: Gumbel iid vs. Gumbel 2-dependiente",
       x = "Tiempo", y = "Valor") +
  theme_minimal()
```

Aún para el ojo más adiestrado es muy difícil
distinguir las dos situaciones. Puede quizás
percibirse mayor “inercia” en el caso de los datos en linea roja, correspondiente a la 2-dependencia, pero es realmente muy difícil lograrlo (y nada seguro jugarse).

Esto demuestra la importancia de realizar tests de hipótesis sobre la estructura subyacente y no
utilizar el “ojímetro”.


Veamos como luce ahora una serie de datos Gumbel 2-dependientes con componente cíclica.


```{r echo=FALSE, warning=FALSE}
set.seed(123)
n <- 500
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 0.7)

a1 <- 0.5
a2 <- 0.3

frequency <- 2 * pi / 100
amplitude <- 0.2

X_dep_cycle <- numeric(n)
for (t in 3:(n+2)) {
  X_dep_cycle[t-2] <- a1 * gumbel_full[t-1] + a2 * gumbel_full[t-2] +
                      gumbel_full[t] + amplitude * sin(frequency * (t-2))
}

time <- 1:n
df <- data.frame(time = time, X = X_dep_cycle)

ggplot(df, aes(x = time, y = X)) +
  geom_line(color = 'steelblue', size = 0.8) +
  labs(x = "Tiempo", y = "Observación") +
  theme_minimal()
```


Se insinúa una estructura de ciclos, pero
nuevamente es altanebte recomendable chequear la
estructura mediante tests.
La adición de una tendencia creciente fuerte,
podría ser una técnica exploratoria que ayudara a
visualizar los ciclos, como apuntó Juan Piccini en
la primera edición de este curso.

Veamos finalmente como luce la dependencia
fuerte. Nuevamente el proceso es estacionario y los
datos son Gumbel.

```{r echo=FALSE, warning=FALSE}
set.seed(123)
n <- 500
gumbel_full <- rgumbel(n + 2, loc = 0, scale = 0.7)

a1 <- 0.9
a2 <- -0.7  # más negativo para intensificar la poda

X_dep_poda <- numeric(n)
for (t in 3:(n+2)) {
  X_dep_poda[t-2] <- a1 * gumbel_full[t-1] + a2 * gumbel_full[t-2] + gumbel_full[t]
}

time <- 1:n
df <- data.frame(time = time, X = X_dep_poda)

ggplot(df, aes(x = time, y = X)) +
  geom_line(color = 'steelblue', size = 0.8) +
  labs(title = "Proceso Gumbel 2-dependiente con poda inferior evidente",
       x = "Tiempo", y = "Observación") +
  theme_minimal()
```

Las oscilaciones son parecidas a los casos anteriores pero la muy parejita “poda” inferior es la que hace sospechar algo “raro”. De todos modos, las posibles estructuras de dependencia fuerte son muchas, complejas y
verificarlas no es sencillo.
Como resumen final, en los más próximos capítulos
tomaremos direcciones no iid, en las que veremos:

1- Si el proceso es estacionario y débilmente
dependiente, la técnica clásica de DEA puede
aplicarse esencialmente igual (Leadbetter,
Lindgren, Rootzén).

2- Si el proceso no es estacionario o no es
débilmente dependiente, cumple algunas
propiedades condicionales a otro proceso que le
pauta la “fase”, la técnica clásica de DEA
puede aplicarse, pero con modificaciones no
menores.

Además:

3- Puede cambiarse el enfoque y mirar el número
de eventos extremos en diversos lapsos, esto
lleva en el caso iid a un proceso de Poisson,
para estacionarios y débilmente dependientes a
un Poisson Compuesto y en el marco más
general del punto 2, a mezclas de Procesos de
Poisson Compuestos (Lise Bellanger-GP).

El conteo de eventos extremos será también la base
o el “pie”, para introducir una técnica muy usada,
POT (Picos Sobre Umbrales) y sus variaciones más
recientes que veremos al final.

## Análisis de series temporales



## Pruebas de raíz unitaria y tendencia

```{r include=FALSE, warning=FALSE}
library(ismev)
data(portpirie)
```

### Concepto de Estacionariedad

```{r}
head(portpirie)
```

```{r nice-fig2, echo=FALSE, fig.cap='Niveles máximos anuales del nivel del mar registrados en Port Pirie.', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Niveles máximos anuales del nivel del mar registrados en Port Pirie, una localidad justo al norte de Adelaida, Australia del Sur, durante el período 1923-1987'}

plot(portpirie,  xlab = "Year", ylab = "Sea Level (meters)", col="brown")
```

La figura \@ref(fig:nice-fig2) muestra los niveles máximos anuales del
nivel del mar registrados en Port Pirie, una localidad justo al norte de
Adelaida, Australia del Sur, durante el período 1923-1987. A partir de
estos datos, puede ser necesario estimar el nivel máximo del mar que
probablemente ocurra en la región en los próximos 100 o 1000 años. Esto
plantea una cuestión importante: ¿cómo podemos estimar los niveles que
pueden ocurrir en los próximos 1000 años sin conocer, por ejemplo, los
cambios climáticos que podrían suceder?

No hay evidencia contundente en la figura que sugiera que el patrón de
variación en los niveles del mar haya cambiado durante el período de
observación, pero dicha estabilidad podría no persistir en el futuro.
Esta advertencia es crucial: aunque la teoría de valores extremos ha
adoptado terminología como el "nivel de retorno a 1000 años", que
corresponde al nivel que se espera que sea excedido exactamente una vez
en los próximos 1000 años, esto solo tiene sentido bajo el supuesto de
estabilidad (o estacionariedad) en el proceso subyacente. Es más
realista hablar en términos de niveles que, bajo las condiciones
actuales, ocurrirán en un año determinado con una baja probabilidad
(@coles2001introduction).

#### Ejemplo en Finanzas

Las técnicas de valores extremos han ganado popularidad en aplicaciones
financieras. Esto no es sorprendente: la solvencia financiera de una
inversión probablemente esté determinada por cambios extremos en las
condiciones del mercado en lugar de cambios típicos. Sin embargo, la
compleja estructura estocástica de los mercados financieros implica que
la aplicación ingenua de técnicas de valores extremos puede ser engañosa
(@coles2001introduction).

```{r echo=TRUE}
data(dowjones)
dowjones$LogDiff <- c(NA, diff(log(dowjones$Index)))
```

```{r echo=TRUE}
head(dowjones)
```

```{r nice-fig4,echo=FALSE, fig.cap='Panel a) Precios de cierre diario del índice Dow Jones. Panel b) Rendimientos diarios del índice Dow Jones (en logarítmos).', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Panel izquierdo: precios de cierre diario del índice Dow Jones.Panel derecho: rendimientos logarítmicos diarios del índice Dow Jones.'}
par(mfrow = c(1,2))
plot(x=dowjones$Date, y=dowjones$Index, type="l", col="blue", xlab="Date", ylab="Index", main="a)")
plot(x=dowjones$Date, y=dowjones$LogDiff, type="l", col="purple", xlab="Date", ylab="diff log Index", main="b)")
```

La Fig.\@ref(fig:nice-fig4) muestra los precios de cierre diario del
índice Dow Jones durante un período de 5 años. Evidentemente, el nivel
del proceso ha cambiado drásticamente a lo largo del período observado,
y las cuestiones sobre los valores extremos del comportamiento diario
quedan eclipsadas por la variación temporal a largo plazo en la serie.
Varios estudios empíricos sobre series de este tipo han indicado que se
puede obtener una aproximación a la estacionariedad tomando los
logaritmos de los cocientes de observaciones sucesivas, lo que se conoce
como los retornos logarítmicos diarios. El panel b) de la Fig.
\@ref(fig:nice-fig4) sugiere una transformación razonablemente exitosa
hacia la estacionariedad. El análisis de las propiedades de valores
extremos de dichas series transformadas puede proporcionar a los
analistas financieros información clave sobre el mercado
(@coles2001introduction).


### Concepto de series de tiempo estacionarias
Siguiendo a @enders, se dice que un proceso estocástico $z_{t}$ con media y varianza finitas es estacionario en covarianza para todo $t$ y $t-s$ cuando

\begin{align}
	\label{eq:2_7}
	E(z_{t})&=E(z_{t-s})=\mu\\
	\label{eq:2_8}
	var(z_t)&=var(z_{t-s})=\sigma^2_z\\
	\label{eq:2_9}
	cov(z_t,z_{t-s})&=cov(z_{t-j},z_{t-j-s})=\gamma_s
\end{align}
donde $\mu, \sigma^2_z, \gamma_s$ con constantes.

En \eqref{eq:2_8}, cuando $s=0$ se va a tener que $\gamma_0$ es la varianza de $z_t$. Que una serie temporal sea estacionaria en covarianza implica que tanto la media como todas sus autocovarianzas\footnote{También se habla de proceso debilmente estacionario o estacionario de segundo orden.} no están afectadas por cambios en los orígenes temporales\footnote{En modelos multivariados, el término autocovarianza refiere a la covarianza entre $z_t$ y sus propios rezagos mientras que covarianza cruzada refiere a la covarianza entre series temporales}.

Dado que se está en un marco de ecuaciones en diferencias, se tienen que cumplir condiciones de estabilidad en el sistema y esto implica que las raíces características asociadas al sistema de interés caígan dentro del círculo unitario. Cuando una serie temporal no sea estacionaria, es posible que presente una evidente tendencia, medias y varianzas que no son constantes en el tiempo.
\end{comment}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
<!--
### Pruebas ADF

Se testea para cada serie de datos

\begin{align*}
	&H_0)\; \gamma = 0\\
	&H_1)\; \gamma \neq 0   
\end{align*}

En este sentido, se consideran tres ecuaciones de regresión distintas para poner a prueba $H_0$,

\begin{align}
	\label{eq: modelo_c}
	\Delta z_t=&\gamma z_{t-1} + \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
	\label{eq: modelo_b}
	\Delta z_t=&a_0+\gamma z_{t-1} + \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t \\
	\label{eq: modelo_a}
	\Delta z_t=&a_0+\gamma z_{t-1} + a_2 t+ \sum_{i=2}^p \beta_i \Delta z_{t-i+1} + \varepsilon_t
\end{align}

Existen tres estadísticos $\tau,\;\tau_{\mu},\;\tau_{\tau}$ para probar la hipótesis nula $H_0)\:\gamma=0$ en cada caso y además, se tienen otros tres $F-$estadísticos $\:\phi_1,\:\phi_2,\:\phi_3$ para hacer pruebas conjuntas sobre los coeficientes.  

Los estadísticos $\:\phi_1,\:\phi_2,\:\phi_3$ se construyen como pruebas $F$:

\begin{equation*}
	\phi_{i}=\frac{\left[SSR_{restringido}-SSR_{no\:restringido}\right]/r}{SSR_{no\:restringido}/(t-k)}    
\end{equation*}

donde SSR es la suma de los cuadrados de los residuos en los modelos restringidos y no restringidos, $r$ es la cantidad de restricciones, $T$ es la cantidad de observaciones,  $k$ es la cantidad de parámetros estimados en el modelo irrestricto, $i=1,2,3$. A su vez, $T-k$ van a ser los grados de libertad del modelos sin restricciones. Los valores de los coeficientes estimados se van a comparar con los valores críticos de tablas reportados por @dickey1981.
La hipótesis nula indica que el proceso de generación de los datos es la del modelo restringido contra la hipótesis alternativa de que los datos son generados por el modelo sin restringir. Cuando los valores de $\:\phi_1,\:\phi_2,\:\phi_3$ sean mayores a los valores críticos reportados por @dickey1981 se rechaza la hipótesis nula, cuando sean menores a los valores críticos entonces no se rechaza la hipótesis nula.



En el siguiente cuadro \@ref(tab:modelos_adf) se consideran los tres modelos y cada una de las hipótesis a testear con sus respectivos estadísticos.



\begin{table}[h!]
	\centering
	\small % Smaller font size
	\setlength{\tabcolsep}{3pt} % Adjust column spacing
	\caption{Modelos del test ADF}
	\label{tab: modelos_adf}
	\begin{tabular}{|c|c|c|c|}
		\hline 
		& Modelo & $H_0)$ & Estadistíco de prueba \\
		\hline 
		\hline 
		c & $\Delta z_{t}=a_{0}+\gamma z_{t-1}+a_{2}t+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\begin{array}{c}
			\gamma=0\\
			\gamma=a_{2}=0\\
			\gamma=a_{2}=a_{0}=0
		\end{array}$ & $\begin{array}{c}
			\tau_{\tau}\\
			\phi_{3}\\
			\phi_{2}
		\end{array}$ \\
		\hline 
		b & $\Delta z_{t}=a_{0}+\gamma z_{t-1}+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\begin{array}{c}
			\gamma=0\\
			\gamma=a_{0}=0
		\end{array}$ & $\begin{array}{c}
			\tau_{\mu}\\
			\phi_{1}
		\end{array}$ \\
		\hline 
		a & $\Delta z_{t}=\gamma z_{t-1}+\sum_{i=2}^{p}\beta_{i}\Delta z_{t-i+1}+\varepsilon_{t}$ & $\gamma=0$ & $\tau$ \\
		\hline 
	\end{tabular}
\end{table}


```{r tab:modelos_adf, echo=FALSE}
library(kableExtra)

# Create the table as a data frame
adf_table <- data.frame(
  Modelo = c("c", "b", "a"),
  Ecuación = c(
    "$\\Delta z_t = a_0 + \\gamma z_{t-1} + a_2 t + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$",
    "$\\Delta z_t = a_0 + \\gamma z_{t-1} + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$",
    "$\\Delta z_t = \\gamma z_{t-1} + \\sum_{i=2}^{p} \\beta_i \\Delta z_{t-i+1} + \\varepsilon_t$"
  ),
  `H_0` = c(
    "$\\begin{array}{c} \\gamma = 0 \\\\ \\gamma = a_2 = 0 \\\\ \\gamma = a_2 = a_0 = 0 \\end{array}$",
    "$\\begin{array}{c} \\gamma = 0 \\\\ \\gamma = a_0 = 0 \\end{array}$",
    "$\\gamma = 0$"
  ),
  `Estadístico` = c(
    "$\\begin{array}{c} \\tau_{\\tau} \\\\ \\phi_3 \\\\ \\phi_2 \\end{array}$",
    "$\\begin{array}{c} \\tau_{\\mu} \\\\ \\phi_1 \\end{array}$",
    "$\\tau$"
  )
)

# Render the table
kbl(adf_table, booktabs = TRUE, escape = FALSE,
    caption = "Modelos del test ADF",
    col.names = c("", "Modelo", "$H_0$", "Estadístico de prueba")) %>%
  kable_styling(latex_options = c("striped", "hold_position"), font_size = 9)
```



Cuando no se conozca el proceso de generación de los datos, se sugiere realizar las pruebas de Dickey-Fuller Aumentado partiendo del modelo
menos restrictivo para cada serie temporal a uno más particular. Si bien las pruebas de ADF son útiles para detectar la presencia de raíces unitarias, los mismos tienen sus limitaciones. Partiendo del modelo general al particular, cada prueba está condicionada a que las pruebas anteriores sean correctas.  Cuando se empieza por el primer paso, es decir, con el modelo (c) con constante y con tendencia, se hace más difícil rechazar $H_0)$, por lo tanto, cuando se rechace la hipótesis nula en un modelo (c) se tiende a rechazar también la hipótesis nula cuando no se incluyan los términos deterministas.  A su vez, establece que el problema principal de las pruebas de Dickey-Fuller es que tanto el intercepto como la pendiente de la tendencia son, con frecuencia, estimados de manera \textit{pobre}  bajo la presencia de raíces unitarias. En general, se tiende a no rechazar la hipótesis nula de raíz unitaria incluso cuando el verdadero valor de $\gamma$ no es cero. Además, la prueba presenta limitaciones también frente a cambios de régimen (@enders).




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

### Pruebas KPSS

En @kpss, se parte de una representación cada serie temporal como la suma de un componente de tendencia determinística, un paseo aleatorio y un error estacionario. En este contexto, se pone a prueba


\begin{align*}
	H_0)&\text{ la serie es estacionaria alrededor de una tendencia}    \\
	H_1)&\text{ la serie es no estacionaria}
\end{align*}

que se corresponde con la hipótesis de que la varianza del paseo aleatorio (_random walk_) es igual a cero. 

Se emplea un estadístico de Multiplicadores de Lagrange (ML) para testear la hipótesis nula de estacionariedad. De esta manera, siendo $z_t$ con $t=1,2,...,T$ las series a las que se les quiere aplicar el test, se asume que se puede descomponer a la serie en la suma de un componente de tendencia determinística, un paseo aleatorio y un error estacionario se tiene que,

\begin{equation}
	z_t=\xi t + r_t + \varepsilon_t
(\#eq:kpss2)
\end{equation}

Donde $r_t$ es un paseo aleatorio:

\begin{equation}
	r_t = r_{t-1} + u_t,
		(\#eq:kpss3)
\end{equation}

donde $u_t$ es $iid(0,\sigma_u^2)$. El valor inicial $r_0$  es fijo y sirve se intercepto. La hipótesis de estacionariedad es $\sigma_u^2=0$ y como se asume que $\varepsilon_t$ es estacionario, bajo la hipótesis nula $z_t$ es estacionaria alrededor de una tendencia.

En el caso particular de que en el modelo \@ref(eq: kpss2) se tenga $\xi=0$, bajo la hipótesis nula $z_t$ va a ser estacionaria alrededor de una constante ($r_0$).

Sean $e_t$ con $t=1,2,...,T$, los residuos de la regresión $z$ con un intercepto y tendencia. A su vez, sea $\hat{\sigma_\varepsilon^2}$ la estimación del error de la varianza de la regresión (suma de los residuos al cuadrado divida $T$). Con lo anterior, se define el proceso de suma parcial de los residuos como

\begin{equation}
	S_t=\sum_{i=1}^t r_i, \quad t=1,....,T   
		(\#eq:kpss5)
\end{equation}

Entonces el estadístico ML es 

\begin{equation}
	\label{eq: kpss6}
	ML=\sum_{t=1}^T S^2_t/\hat{\sigma^2_\varepsilon}   
\end{equation}

En el caso de que se quiera poner a prueba la hipótesis nula de estacionariedad alrededor de una constante se define $e_t$ como los residuos de la regresión $z$ sobre un intercepto ($e_t=z_t-\bar{z})$. Cabe resaltar que es una prueba de cola superior y se reportan los valores críticos. Además, para este caso se asume que los errores $\varepsilon_t \overset{\text{iid}}{\sim} \mathcal{N}(0,\sigma_{\varepsilon}^2)$
Sin embargo, se puede extender la prueba con supuestos más débiles sobre la distribución de los errores dado que el supuesto anterior puede ser poco realista.

-->

