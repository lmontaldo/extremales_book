# El enfoque de conteo de eventos y los modelos de base Poissoniana

## Teoría de los procesos puntuales

Siguiendo a @coles2001introduction, se puede caracterizar un comportamiento de valores extremos de varias maneras, una de ellas es la __Teoría de los procesos puntuales__. Aunque los resultados de la inferencia obtenida bajo este esquema pueden ser equivalentes a los obtenidos con un modelo apropiado como los vistos antes, existen buenas razones para considerar este enfoque:

1- Su interpretación permite unificar a los modelos vistos anteriormente.

2- Se vincula a una verosimilitud que brinda una formulación más natural para los excesos sobre un umbral no estacionarios.

Un __proceso puntual__ es un modelo estocástico que se utiliza para describir la ocurrencia de eventos en el tiempo^[O en el espacio.]. Por ejemplo, si se desea estudiar cuándo ocurren eventos como tormentas eléctricas, terremotos o valores extremos en una serie temporal, un proceso puntual permite representar en qué momento  suceden esos eventos, sin modelar directamente su magnitud.

En este contexto, el conjunto $\mathcal{J}$ representa el dominio donde pueden ocurrir los eventos, es decir, un __intervalo de tiempo__, y el proceso puntual asigna de manera aleatoria posiciones a los eventos dentro de dicho intervalo. A partir del modelo, es posible calcular la probabilidad de que ocurra una cantidad determinada de eventos (como tormentas eléctricas o terremotos) dentro de un período de tiempo específico. Además, dado que ha ocurrido un evento, se puede estimar el tiempo esperado hasta el siguiente evento.

Una forma de caracterizar las propiedades estadísticas de un proceso puntual es definir un conjunto de variables aleatorias de valor entero no negativo, \( N(J) \), para cada subconjunto \( J \subset \mathcal{J} \), tal que \( N(J) \) representa la cantidad de puntos en el conjunto \( J \). Especificar de manera consistente la distribución de probabilidad de cada \( N(J) \) determina las medidas del proceso puntual, al cual denotamos como \( N \). 

También se pueden definir las medidas de resumen de un proceso puntual. En particular, definimos la __medida de intensidad del proceso__ como

\begin{equation}
\Lambda\left( J \right)=E\left\{ N\left( J \right) \right\}
\end{equation}

que da la cantidad esperada de puntos en cualquier subconjunto \( J \subset \mathcal{J} \).

Suponiendo que \( J = [j_1, x_1) \times \dots \times [j_k, x_k) \subset \mathbb{R}^k \), y siempre que exista, la función derivada

\begin{equation}
\lambda(x) = \frac{\partial \Lambda(J)}{\partial x_1 \cdots \partial x_k}
\end{equation}

se denomina __función de intensidad (densidad)__ del proceso.


### Proceso de Poisson homogéneo unidimensional

El proceso de __Poisson homogéneo unidimensional__ es el modelo más simple para describir eventos que ocurren aleatoriamente a lo largo del tiempo, como terremotos, llamadas o llegadas de clientes. Se caracteriza por los siguientes supuestos:

\begin{itemize}
  \item Los eventos ocurren de manera independiente.
  \item La probabilidad de que ocurra un evento es constante en el tiempo.
\end{itemize}

Sea \( N(J) \) la cantidad de eventos en el intervalo \( J= [t_1, t_2] \). Esta cantidad sigue una distribución de Poisson, tal que 

\begin{equation}
N([t_1, t_2]) \sim \text{Poisson}(\lambda (t_2 - t_1))
\end{equation}

donde \( \lambda > 0 \) es la __tasa (intensidad) de ocurrencia de eventos por unidad de tiempo__.

Además, si los intervalos \( J \) y \(M  \) no se superponen, entonces $N(J) \perp\!\!\!\perp N(M)$. Es decir, el número de eventos en __intervalos disjuntos__ son variables aleatorias independientes. La medida de intensidad del proceso es $\Lambda([t_1, t_2]) = \lambda (t_2 - t_1)$, y la función de intensidad (densidad de intensidad) es simplemente constante $\lambda(t) = \lambda.$ 


Este modelo es apropiado cuando los eventos ocurren de manera aleatoria y uniforme en el tiempo.


### Proceso de Poisson no homogéneo unidimensional

El proceso de Poisson homogéneo puede generalizarse para modelar eventos que ocurren aleatoriamente en el tiempo, pero con una tasa variable \( \lambda(t) \). Esto da lugar al __proceso de Poisson no homogéneo unidimensional__, el cual mantiene la propiedad de contar eventos de forma independiente en subconjuntos disjuntos (como en el caso homogéneo), pero con una propiedad modificada:

Para todo subconjunto \( J = [t_1, t_2] \subset \mathcal{J} \), se cumple que $N(J) \sim \text{Poisson}(\Lambda(J))$, donde

\[
\Lambda(J) = \int_{t_1}^{t_2} \lambda(t)\, dt.
\]

De forma implícita, \( \Lambda(\cdot) \) es la medida de intensidad del proceso, y \( \lambda(\cdot) \) es su función de densidad de intensidad.

### Proceso de Poisson no homogéneo k-dimensional

El proceso de Poisson no homogéneo puede generalizarse aún más para describir puntos que ocurren aleatoriamente dentro de un subconjunto de un espacio \( k \)-dimensional. Un proceso puntual sobre \( \mathcal{J} \subset \mathbb{R}^k \) se dice que es un __proceso de Poisson no homogéneo \( k \)-dimensional__ con función de densidad de intensidad \( \lambda(x) \) si cumple con:

\begin{itemize}
  \item independencia de conteos sobre subconjuntos disjuntos de \( \mathcal{J} \), y
  \item para todo \( J \subset \mathcal{J} \),
  \[
  N(J) \sim \text{Poisson}(\Lambda(J)), \quad \text{donde} \quad \Lambda(J) = \int_J \lambda(x) \, dx.
  \]
\end{itemize}

La propiedad intrínseca de un proceso de Poisson es que los puntos ocurren independientemente unos de otros. La aparición de un punto en una ubicación \( x \in \mathcal{J} \) ni fomenta ni inhibe la aparición de otros puntos en las cercanías de \( x \) ni en ninguna otra parte del espacio.

Por esta razón, los procesos de Poisson son modelos ideales para fenómenos de __dispersión aleatoria__. Las variaciones en la cantidad de puntos entre diferentes subregiones del espacio \( \mathcal{J} \) pueden explicarse mediante una función de intensidad no constante, lo cual refleja una mayor cantidad esperada de puntos en ciertas regiones respecto de otras.  Sin embargo, esto no implica que la presencia de puntos en una región afecte la ocurrencia en otra. Esto también sugiere que hay fenómenos físicos para los que el proceso de Poisson no es un buen modelo, como aquellos con espaciamiento natural (por ejemplo, la ubicación de árboles en un bosque), o con agrupamiento natural (como los momentos de ocurrencia de tormentas).
 

En aplicaciones estadísticas, los modelos de procesos puntuales requieren estimar el proceso a partir de un conjunto de puntos observados \( x_1, \dots, x_n \) en una región o intervalo \( J \). Esto implica elegir una clase de modelos de procesos puntuales, y estimar los parámetros dentro de dicha clase. Nos centraremos en la estimación dentro de la familia de __procesos de Poisson no homogéneos__.

Supondremos que la función de intensidad \( \lambda(\cdot) \) pertenece a una familia paramétrica \( \lambda(\cdot\, ; \beta) \). Entonces, el problema se reduce a estimar el vector de parámetros desconocido \( \beta \), suponiendo que el modelo es válido. Esta aproximación es similar a la que se utiliza en la estimación de parámetros para distribuciones de probabilidad. Entre los métodos disponibles, el de MV se destaca como una metodología general con buenas propiedades estadísticas.

<!--
La verosimilitud se obtiene considerando la probabilidad de los datos observados como una función del parámetro desconocido \( \theta \)^[Presentamos el desarrollo en el caso más simple, donde el conjunto \( J \) es unidimensional, aunque el razonamiento es análogo para procesos de Poisson en dimensiones superiores.].
-->


## Procesos de conteo y distribuciones Poisson

### Proceso de conteo

Considerando la Teoría de los procesos puntuales presentada anteriormente, introducimos a continuación algunas de las definiciones de los conceptos ya vistos para seguir avanzando en el estudio de los procesos de conteo.

Fijaremos un cierto __umbral__ ($u$), llamaremos __evento__ cuando la variable observada ($X_t$ para $t=1,\dots,T$) supera dicho umbral tal que $X_t>u$. Si además, consideramos un cierto intervalo de tiempo $J$, podemos definir 

$$N(J)= \text{número de eventos en el intervalo}\; J.$$

Por ejemplo, si lo que registramos son velocidades de vientos máximas medidas cada 10
minutos, y fijamos como umbral 80 km/h (aproximadamente $22.22\; m/s$), entonces
$N(Enero)=$ cantidad de períodos de 10 minutos durante enero en la que se registró una velocidad de viento superior a los $80\; km/h$.

Obviamente, dadas las fluctuaciones que tienen los fenómenos que estudiamos, intentamos realizar estadísticas sobre el número de eventos $N$ y para ello primero debemos considerar los __modelos probabilísticos__, de más simples a más complejos, presentados en la sección anterior.

$N$ es lo que se llama un __proceso de conteo__^[_Counting process_ en inglés.] o __proceso
puntual__^[_Point process_ en inglés.], un tipo de modelos empleados en logística, telecomunicaciones, estudios de contaminación atmosférica o costera, entre otros.

### Proceso de Poisson

El proceso de conteo más simple es el llamado __Proceso de Poisson__, que puede caracterizarse de la
siguiente manera.



<!-- definition #d13 name="Proceso de Poisson"
Si $N$ es un proceso de conteo y $\lambda >0$, entonces diremos que $N$
es un Proceso de Poisson de parámetro $\lambda$, $PP(\lambda)$, si se cumple que :
  
a) Para todo intervalo $J$ de los reales positivos, $N(J)$
es una variable aleatoria que tiene distribución de Poisson de parámetro $\lambda$ $longitud(J)$.

b) Si $J, L, M, \dots$ es una cantidad arbitraria de intervalos de reales positivos DISJUNTOS,
entonces $N(J), N(L), N(M),\dots$ son variables aleatorias independientes.

-->



::: {.definition #d13 name="Proceso de Poisson"}
Sea $\{N(t),\, t \geq 0\}$ un proceso de conteo. 
Se dice que es un proceso de Poisson con tasa $\lambda > 0$ ($PP(\lambda)$) si se cumplen las siguientes condiciones:


1- $N(0) = 0$

2- Tiene incrementos independientes: el número de eventos que ocurren en intervalos disjuntos de tiempo son variables aleatorias independientes.

3- El número de eventos en cualquier intervalo de longitud $t$ tiene distribución de Poisson con media $\lambda t$, es decir, para todo $\:s, t \geq 0\:$ y $\:n = 0, 1, 2, \ldots\:$ tal que

$$
\mathbb{P}(N(t + s) - N(s) = n) = \frac{e^{-\lambda t} (\lambda t)^n}{n!}
$$



Como consecuencia del punto (3), el proceso tiene incrementos estacionarios y se cumple que
\[
\mathbb{E}[N(t)] = \lambda t.
\]

Es por este motivo que decimos que $\lambda$ es la tasa del proceso [@ross2014].
:::


::: {.definition #dfinter name="Distribuciones de Tiempos inter-eventos y de Espera"}

Consideremos un proceso de Poisson y definamos el tiempo del primer evento como $T_1$. 

Para $n > 1$, sea $T_n$ el tiempo transcurrido entre el $(n-1)$-ésimo y el $n$-ésimo evento. 
La secuencia $\{T_n,\; n = 1, 2, \dots\}$ se llama la __secuencia de tiempos inter-eventos__. Por ejemplo, si $T_1 = 5$ y $T_2 = 10$, entonces el primer evento del proceso de Poisson ocurre en el instante 5 y el segundo en el instante $5 + 10 = 15$.

\vspace{0.3cm}
Determinemos ahora la distribución de los tiempos $T_n$. Comenzamos observando que el evento $\{T_1 > t\}$ ocurre si y sólo si no ocurre ningún evento del proceso de Poisson en el intervalo $[0, t]$, por lo tanto:

\[
P(T_1 > t) = P(N(t) = 0) = e^{-\lambda t}
\]

Esto implica que $T_1$ tiene una distribución exponencial con media $1/\lambda$.
\vspace{0.3cm}

Ahora bien, $P(T_2 > t) = \mathbb{E}[P(T_2 > t \mid T_1)]$.

\vspace{0.3cm}
Sin embargo,

\begin{align}
P(T_2 > t \mid T_1 = s) &= P(\text{0 eventos en } (s, s + t] \mid T_1 = s) \nonumber \\
&= P(\text{0 eventos en } (s, s + t]) \nonumber \\
&= e^{-\lambda t} 
(\#eq:512)
\end{align}

donde las últimas dos igualdades se deducen de las propiedades de incrementos independientes y estacionarios del proceso de Poisson.
\vspace{0.3cm}
Por lo tanto, a partir de la Ecuación \@ref(eq:512) concluimos que $T_2$ también es una variable aleatoria exponencial con media $1/\lambda$ y, además, que $T_2$ es independiente de $T_1$. Repitiendo el mismo argumento para $T_3, T_4,\dots$, se obtiene la misma conclusión para toda la secuencia (@ross2014).
:::

::: {.proposition #exp} 

$T_n,\; n = 1, 2, \dots,\;$son variables aleatorias exponenciales independientes e idénticamente distribuidas con media $\frac{1}{\lambda}$.

\vspace{0.3cm}
El proceso de Poisson no tiene memoria: desde cualquier punto en el tiempo, su evolución futura es independiente del pasado y sigue la misma distribución. Por eso, los tiempos entre eventos (interarribos) siguen una __distribución exponencial__ (@ross2014).
:::

El siguiente teorema brinda una visualización muy interesante de los Procesos de Poisson, que nos
servirá mucho para introducir otros modelos y que es ideal para poder simular computacionalmente
Procesos de Poissson.

```{theorem, label="d31", name="Otra visión de los Procesos de Poisson"}
Si $T_1,\dots,T_n,\dots$ es $iid$ con distribución Exponencial de
parámetro $\lambda>0$ y definimos que ocurre el primer
evento en el instante $T_1$, el segundo en el instante
$T_1+T_2$, el tercero en el instante $T_1+T_2+T_3$ y asì
sucesivamente, el proceso $N$ de conteo de tales
eventos, es un proceso de Poisson.
```

Dicho de otro modo el Proceso de Poisson representa eventos aislados (“que ocurren de a uno y claramente separados”), con tiempos inter-eventos $iid$ y exponenciales.
Obviamente, esto muchas veces es “too good to be true”, pero variaciones de este modelo tan simple nos brindarán a menudo modelos realistas.



__Observación 1:__ En la práctica, si se toman datos
en los instantes $1,\dots,n$ suele reescalarse el tiempo
dividiendo por $n$ y los instantes quedan en $[0,1]$. Allí se define un PP de manera casi idéntica, obviamente modificando en la Def. \@ref(def:d13), tanto en
a) como en b) que los intervalos deben estar
contenidos en $[0,1]$.




__Observación 2:__ Conviene recordar que si $X$ es
una $V.A.$ Poisson de parámetro $\lambda>0$ y $T$ es una $V.A.$
exponencial de parámetro $\lambda$, entonces $E(X)= \lambda$ y $E(T)=1/\lambda$.
Si $T_1,\dots,T_n,\dots$ $iid$ son los tiempos inter-eventos de un
$PP(\lambda)$ se deduce entonces de la ley de los grandes
números que

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n T_i = \frac{1}{\lambda}
$$

Es decir que el tiempo promedio entre eventos “a
la larga ” es $\frac{1}{\lambda}$. De manera similar, si $J1,\dots,J_n,\dots$ son
intervalos disjuntos de longitud 1, por la definición
1 y la ley de los grandes números se tiene que

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n N(J_i) = \lambda
$$

Más aún, puede probarse que:

$$
\lim_{t \to \infty} \frac{N(0,t)}{t} = \lambda
$$
Esto permite observar una consecuencia del Teorema \@ref(thm:d31), que es una propiedad intuitivamente
muy atractiva.

La tasa promedial de incidencia de los eventos en un $PP(\lambda)$ es inversamente proporcional al tiempo
promedial inter-eventos.



::: {.definition #tiempoar name="Tiempo de espera hasta el n-ésimo evento"}

Otra cantidad de interés es \( S_n \), el tiempo de arribo del \( n \)-ésimo evento, también llamado tiempo de espera hasta el \( n \)-ésimo evento. Se define

\[
S_n = \sum_{i=1}^{n} T_i, \quad n \geq 1,
\]

y por lo tanto, a partir de que los \( T_i \) son variables aleatorias exponenciales independientes e idénticamente distribuidas con parámetro \( \lambda \), se deduce que \( S_n \) sigue una distribución Gamma con parámetros \( n \) y \( \lambda \) [@ross2014]. 

:::
<!--
Es decir, la función de densidad de probabilidad de \( S_n \) está dada por:

\[
f_{S_n}(t) = \lambda e^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1)!}, \quad t \geq 0.
\]

Esta expresión también puede obtenerse observando que el \( n \)-ésimo evento ocurre en o antes del instante \( t \) si, y solo si, han ocurrido al menos \( n \) eventos en ese intervalo.

-->



```{example, label="ex13", name="Proceso de ocupación de las líneas entre dos centrales"}
En las primeras dos décadas del siglo XX, un
creador genial como Agner Erlang modeló mediante Procesos de Poisson las llamadas que
arribaban a una central telefónica, así como (con parámetros muy distintos) el proceso de ocupación
de las líneas entre dos centrales. 

Eso condujo no sólo al desarrollo de las primeras centrales de
telefonía conmutada por circuitos por CTC, la filial danesa de Bell, sino además a que Erlang
desarrollara su “fórmulas de bloqueo”, fino cálculo por el cual, según los parámetros del proceso de
arribo y del proceso de ocupación de líneas, se calcula la probabilidad de “saturación” (no hay
ninguna línea disponible) dado el número de líneas entre centrales, o, dada una probabilidad de
saturación “tolerable” $\varepsilon$ DISEÑAR (determinar el mínimo número de lineas necesarias para que la probabilidad de bloqueo no exceda $\varepsilon$). 

Si el tiempo entre arribos de llamadas a la central es Exponencial de parámetro $\lambda$, y la duración media de una llamada es Exponencial de parámetro $\mu$, entonces el __parámetro crucial__ de la fórmula de Erlang es

$$
\rho=\frac{\lambda}{\mu}=\frac{\text{Duración media de la llamada}}{\text{Tiempo medio entre llamadas}}
$$  
  
y a mayor valor de $\rho$, mayor probabilidad de saturación para una conectividad dada. 


Esta fórmula aún sigue en uso en algunos problemas y
dió pie al desarrollo de fórmulas de bloqueo más sofisticadas para situaciones más complejas. 

La unidad en la que se mide la intensidad de tráfico en redes se llama “erlang” y este ejemplo nos parece una clara muestra de cuán útil ha sido el muy sencillo Proceso de Poisson.

```

\vspace{0.5cm}

Sin embargo, en otros problemas, por ejemplo en modernas redes de datos en las que los eventos relacionados a la demanda de servicio pueden ocurrir simultáneamente en muy grandes cantidades (clustering), aparece un modelo más sofisticado, que puede ser definido a partir del Proceso de Poisson, el __Proceso de Poisson Compuesto__. 

Cuando hay dependencia temporal en las observaciones, los extremos pueden aparecer agrupados (en clusters) por lo que es adecuado emplear este modelo [@poissonComp].


\vspace{0.5cm}

### Generalizaciones del Proceso de Poisson

#### Proceso de Poisson Compuesto

::: {.definition #def13 name="Proceso de Poisson Compuesto"}

Sea \( N(t) \) un proceso de Poisson homogéneo con tasa \( \lambda > 0 \), y sea \( \{S_i\}_{i \in \mathbb{N}} \) una sucesión de variables aleatorias independientes e idénticamente distribuidas (iid) con valores en \( \mathbb{N} = \{1, 2, 3, \dots\} \), y además independientes de \( N(t) \). Entonces, el proceso estocástico \( \{M(t),\ t \geq 0\} \) definido por

\[
M(t) = \sum_{i=1}^{N(t)} S_i, \quad t \geq 0,
\]


se llama **Proceso de Poisson Compuesto** con tasa \( \lambda \) y distribución de saltos \( G \), donde $G$ es la ley de los \( S_i \) [@ross2014]. 

Seguimos la notación \( M \sim PPC(\lambda; G) \).
:::



<!--
::: {.definition #def13 name="Proceso de Poisson Compuesto"}
Si $N$ es un Proceso de Poisson de parámetro $\lambda>0$ , $G$ es una distribución de probabilidad en los naturales $(1,2,3,\dots)$, consideramos $S_1,\dots,S_n,\dots$ iid con distribución $G$ y construímos un nuevo proceso de conteo $M$ de la forma siguiente:
  
- Cuando $N$ tiene su primer evento, $M$ tiene $S_1$ eventos simultáneos; 

- Cuando $N$ tiene su segundo evento, $M$ tiene $S_2$ eventos simultáneos.....y así sucesivamente.

decimos que $M$ es un Proceso de Poisson Compuesto de parámetro $\lambda>0$ y distribución de eventos $G$, abreviaremos $M$ es $PPC(\lambda;G)$.
:::



Matematicamente, un proceso estocástico \( \{X(t),\ t \geq 0\} \) se dice que es un __proceso de Poisson compuesto__ si puede representarse como

\[
X(t) = \sum_{i=1}^{N(t)} Y_i, \quad t \geq 0,
\]

donde \( \{N(t),\ t \geq 0\} \) es un proceso de Poisson, y \( \{Y_i,\ i \geq 1\} \) es una familia de variables aleatorias independientes e idénticamente distribuidas ($iid$) que además son independientes del proceso \( N(t) \). Denominamos \( X(t) \) como la variable aleatoria de Poisson compuesta [@ross2014].

-->








::: {.exercise #ppc name="PPC"}
Demostrar que para un $PPC(\lambda;G)$ el tiempo medio inter-eventos sigue siendo $1/\lambda$, pero que la tasa de incidencia media de eventos ahora es $\lambda E(G)$.
:::






__Observación 3.__ Para aclarar, si $G$ es una distribución degenerada otorga al $1$ probabilidad $1$, el correspondiente $PPC(\lambda;G)$ en realidad es un $PPC(\lambda)$. Ergo, el $PP$ es un caso particular de $PPC$.

<!--
En otras palabras, el proceso de Poisson $PP(\lambda)$ puede verse como un caso particular de un proceso de Poisson Compuesto donde siempre ocurre un único evento en cada arribo del proceso de Poisson base.
-->

__Observación 4.__ Para evitar confusiones frecuentes, distinguiremos explícitamente estos procesos de los llamados _Procesos de Poisson no-homogéneos_. Para ello recordemos, sin entrar en tecnicismos, que una medida en los reales positivos es una función que a los conjuntos asocia números positivos con las mismas propiedades formales,
excepto que no tiene por qué dar a todo el conjunto de los reales positivos (a todo el universo) el valor 1. 

Dicho de otro modo, una probabilidad es una medida particular, que a todo el universo asigna el valor 1. Puede pensarse como ejemplo típico de una medida, la que asigna a un conjunto la integral sobre ese conjunto de una función no negativa (no
necesariamente de integral total 1, puede ser incluso infinita). La longitud es el ejemplo más simple de medida (llamada también _Medida de Lebesgue_) y la longitud de todos los reales positivos es infinito. Puede demostrarse que la longitud multiplicada por una constante no negativa son las únicas medidas invariantes por traslaciones, punto importante para la distinción que queremos hacer.


#### Procesos de Poisson no-homogéneos

Una función \( f(h) \) se dice que es \( o(h) \) si
\[
\lim_{h \to 0} \frac{f(h)}{h} = 0.
\]

::: {.definition #oh name="Orden pequeño respecto de h"}
Una función \( f(h) \) se dice que es \( o(h) \) si
\[
\lim_{h \to 0} \frac{f(h)}{h} = 0.
\] 
:::

Por ejemplo, la función \( f(x) = x^2 \) es \( o(h) \) ya que

\[
  \frac{f(h)}{h} = \frac{h^2}{h} = h \quad \Rightarrow \quad \lim_{h \to 0} \frac{f(h)}{h} = \lim_{h \to 0} h = 0.
 \]




::: {.definition #ppnh name="Proceso de Poisson No Homogéneo"}
Un proceso de conteo \( \{N(t),\ t \geq 0\} \) se denomina \textbf{proceso de Poisson no homogéneo} con función de intensidad \( \lambda(t) \), si cumple las siguientes condiciones:

\begin{enumerate}
  \item \textbf{Condición inicial:} 
  \[
  N(0) = 0.
  \]

  \item \textbf{Incrementos independientes:} Para cualquier elección de intervalos de tiempo no solapados, los incrementos del proceso en esos intervalos son independientes.

  \item \textbf{Probabilidad de múltiples eventos en un intervalo pequeño:}
  \[
  \mathbb{P}(N(t+h) - N(t) \geq 2) = o(h),
  \]
  es decir, la probabilidad de que ocurran dos o más eventos en un intervalo infinitesimal \( h \) es despreciable en comparación con \( h \) cuando \( h \to 0 \).

  \item \textbf{Probabilidad de un solo evento en un intervalo pequeño:}
  \[
  \mathbb{P}(N(t+h) - N(t) = 1) = \lambda(t) h + o(h),
  \]
  lo que significa que la probabilidad de que ocurra un único evento en el pequeño intervalo \( [t, t+h) \) es aproximadamente proporcional a \( h \), con constante de proporcionalidad \( \lambda(t) \).
\end{enumerate}
:::

De manera equivalente, podemos plantear la siguiente Definición.
<!--
::: {.definition #def14 name="Proceso de Poisson No Homogéneo (PPNH)"}
Sea \( m \) una medida definida en los subconjuntos de los reales positivos. Un proceso de conteo \( N \) se llama Proceso de Poisson No Homogéneo de medida \( m \), y se denota \( N \sim \text{PPNH}(m) \), si:

- Para todo intervalo \( J \subset \mathbb{R}_+ \), la variable \( N(J) \sim \text{Poisson}(m(J)) \).

- Si \( J_1, J_2, \dots, J_k \) son intervalos disjuntos, entonces \( N(J_1), N(J_2), \dots, N(J_k) \) son variables aleatorias independientes.
:::

Si la medida \( m \) admite una densidad respecto de la medida de Lebesgue, es decir, si existe una función no negativa \( \lambda(t) \) tal que
\[
m([a,b]) = \int_a^b \lambda(t)\,dt,
\]
entonces \( N \) es un Proceso de Poisson No Homogéneo con función de intensidad \( \lambda(t) \), y cumple

- \( N(0) = 0 \),
- Incrementos independientes,
- Para \( h \to 0 \),
  \[
  \mathbb{P}(N(t+h) - N(t) = 1) = \lambda(t) h + o(h), \quad \mathbb{P}(N(t+h) - N(t) \geq 2) = o(h).
  \]

Este enfoque es útil cuando la función de intensidad \( \lambda(t) \) tiene una expresión conocida, lo que permite modelar y simular el proceso más fácilmente.
-->

```{definition, label="def14", name="Proceso de Poisson No Homogéneo"}
Si $N$ es un proceso de conteo y $m$ es una medida que no puede expresarse como una constante por la longitud, diremos que $N$ es un Proceso de Poisson No Homogéneo de medida $m$ ($N$ es $PPNH(m)$ ) si se cumple:
  
a) Para todo intervalo $J$ de los reales positivos, $N(J)$ es una variable aleatoria que tiene distribución de Poisson de parámetro $m(J)$.


b) Si $J, L, M, \dots$ es una cantidad arbitraria de intervalos positivos DISJUNTOS, entonces $N(J), N(L), N(M),\dots$ son variables aleatorias independientes.
```


La Definición \@ref(def:def14)  usa una medida $m$ para definir la intensidad. Si esa medida fuera simplemente proporcional a la longitud (como pasa en el Poisson homogéneo), entonces el proceso sería homogéneo. Por eso, para evitar confusiones, en la definición de proceso de Poisson no homogéneo se aclara que $m$ no debe ser constante por longitud.

Para dejar en claro la diferencia entre los $PPNH$ y los $PPC$ (o el simple $PP$), recordemos que en los $PPC$, los tiempos inter-eventos son exponenciales de parámetro $\lambda>0$ e $iid$. El siguiente resultado muestra la diferencia de conceptos. 

Para destacar la diferencia entre los procesos de Poisson no homogéneos ($PPNH$) y los procesos de Poisson compuestos ($PPC$) —o incluso el proceso de Poisson simple ($PP$)— recordemos que en los $PPC$, los tiempos entre eventos son variables aleatorias exponenciales independientes e idénticamente distribuidas ($iid$) con parámetro $\lambda>0$.


```{theorem, label="th13", name="PPNH no es PPC"}
Si $N$ es un $PPNH$ y $T_1$ es el tiempo del primer evento, la distribución de $T_1$ no es exponencial. Por lo tanto, un $PPNH$ no es $PPC$.
```


__Demostración:__

\begin{align}
P(T_1 \leq t) &= P(N((0,t)) \geq 1) \\
&= 1 - P(N((0,t)) = 0) \\
&= 1 - e^{-m((0,t))}, \quad \forall\, t > 0.
\end{align}

Si $T_1$ fuera una variable aleatoria exponencial, entonces, para algún $\lambda > 0$ y para todo $t > 0$, se tendría:

\[
P(T_1 \leq t) = 1 - e^{-\lambda t} \quad \Rightarrow \quad m((0,t)) = \lambda t.
\]

Entonces, para cualesquiera $a < b$,

\begin{align}
m((a,b)) &= m((0,b)) - m((0,a)) \\
&= \lambda b - \lambda a = \lambda (b - a) \\
&= \lambda \cdot \text{longitud}((a,b)).
\end{align}

Por lo tanto, $m = \lambda \times \text{longitud}$, lo cual contradice nuestra hipótesis de que $m$ **no** es una medida proporcional a la longitud. \hfill $\diamond$




<!--
__Demostración:__  texto de Gonza

\begin{align}
P(T_1\leq t)&= P(N((0,t))\geq 1)\\
&=1-P(N((0,t))=0)\\
&=1-e^{-m((0,t))},\;\forall\;t>0.
\end{align}

Si $T_1$ fuera exponencial entonces, para algún $\lambda>0$ y para todo $t>0$, sería $m((0,t))=\lambda t$. Por ende, si $a<b$ cualquiera, 

\begin{align}
m((a,b))&= m((0,b))-m((0,a))\\
&=\lambda b - \lambda a=\lambda(a-b)\\
&=\lambda x \;\text{longitud}((a,b)).
\end{align}

Por lo cual se concluye $m=\lambda x \;\text{longitud}$, lo cual es absurdo $\diamond$.

-->


## Técnica de pasajes de altos niveles (HLE)
<!--
__Observación 5.__ Si $n$ es grande y se dispone de una muestra $X_1,\dots,Xn,\dots$ donde se intenta describir eventos extremos de este tipo de datos, la técnica de pasajes de altos niveles, $HLE$ (High Level Exceedances, en inglés), implica hacer un _tunning_ del umbral en función de $n$, respecto al cual se considerará un evento “extremo”, pues es lógico que para mayor $n$, sea posible distinguir con mayor precisión eventos de extremos de mayor o menor intensidad. 
-->

__Observación 5.__ Cuando el tamaño muestral \( n \) es grande y se dispone de una secuencia de datos \( X_1, \dots, X_n \), una estrategia común para el análisis de eventos extremos es la técnica de \emph{pasajes de altos niveles}^[High Level Exceedances, HLE]. Esta consiste en fijar un umbral \( u_n \) dependiente de \( n \), a partir del cual se considera que una observación corresponde a un evento “extremo”. Dado que una muestra mayor permite discriminar mejor entre niveles de rareza, es natural que el umbral se ajuste con \( n \) para capturar eventos cada vez más extremos [@hle].

En este enfoque, se suele asumir que el umbral \( u_n \) se elige de modo tal que:

\begin{equation}
n \, P(X_1 > u_n) \to \lambda, \quad \text{cuando } n \to \infty, \quad \text{con } \lambda > 0.
\end{equation}

__Observación 6.__ Si se cumple la condición anterior, entonces asintóticamente \( P(X_1 > u_n) \approx \lambda/n \). Es decir, el evento de que una observación supere el umbral \( u_n \) se vuelve cada vez más raro a medida que crece \( n \). Por este motivo, en la literatura estadística algunos autores denominan a este tipo de enfoques \emph{Leyes de los pequeños números}^[Laws of Small Numbers, @flaws], ya que se enfocan en contar eventos poco frecuentes.

En lo que sigue, desarrollaremos resultados concretos bajo este paradigma. Para facilitar el tratamiento asintótico, reescalaremos el índice \( j \) de cada observación al intervalo unitario mediante la transformación \( j \mapsto j/n \in [0,1] \).


<!--
Si se cumple a., entonces $P(X_1>u_n)\approx \lambda/n$, por lo cual, si $n$ es grande, el evento de que los datos observados superen el umbral un es un _evento raro_, con probabilidad muy baja, poco frecuente. Esto hace que al conteo de eventos extremos respecto a un tal umbral en la Literatura Estadística, algunos autores le llamen _Laws of Small Numbers_ (@flaws), pues se cuenta algo poco frecuente. Iremos a resultados concretos en tal dirección. Reescalamos el instante $j$ al instante $j/n$ de $[0,1]$. 
-->

```{theorem, label="th14", name="HLE en el caso iid"}
Si $X_1,\dots,X_n,\dots$ iid, se cumple la condición de la Observación $5$ y se
considera el proceso puntual $N_n$ de _pasajes de altos niveles_, definido para todo intervalo $J$ de $[0,1]$ por
$$
N_n(J)= \text{Cantidad de i en}\; nJ\; \text{tales que} X_i>u_n,
$$
entonces cuando $n$ tiende a infinito, $N_n$ tiende a un $PP( \lambda )$.
```


__Observación 7.__ Uno de los elementos más notorios que produce la dependencia entre los
datos, aunque sea débil, es el “clustering” o agrupamiento de los datos muy grandes. Esto se traduce a que los pasajes altos se disparan de acuerdo a Proceso de Poisson, pero cuando se disparan, pueden dispararse varios juntos. Esta es la base intuitiva del siguiente resultado.


```{theorem, label="th15", name="HLE para procesos estacionarios débilmente dependiente"}
Si $X_1,\dots,X_n,\dots$ es un proceso estacionario y débilmente dependiente que cumple la condición de la Observación $5$ y se considera el proceso puntual $N_n$ de “pasajes de altos niveles” como antes, entonces si

$$
\lim_{n \to \infty} N_n=PPC( \lambda;G )
$$
  
y $G$ puede ser identificado por una fórmula (o bien estimada a partir de la muestra de “saltos”).
```

Esto brinda un modelo razonablemente realista en muchas situaciones. Sin embargo, para modelar contaminación urbana por $O_3$, dada la presencia de covariables muy influyentes debimos lidiar con un modelo más complejo que veremos más adelante.

### Ejemplos concretos de HLE

#### Datos $iid$

<!---  [Text to display](https://otexts.com/fpp3/decomposition.html) 
Para “bajar a tierra” los conceptos vistos vamos a ver ejemplos concretos de HLE. Todos los ejemplos que veremos los instantes de registro de datos $n=5000$ y como se indicó en la observación 1, reescalaremos el tiempo para llevarlo a $[0,1]$.

Comenzaremos por un ejemplo en que los datos registrados son $iid$.
-->

```{r echo=FALSE, warning=TRUE}
# Parámetros
n <- 5000                # Número de observaciones
set.seed(123)            # Para reproducibilidad

# Simulación de datos iid de una distribución Pareto (cola pesada)
# Parámetro de forma (alpha > 0) controla la pesadez de la cola
alpha <- 2.5
x <- (runif(n))^(-1/alpha)

# Reescalado del tiempo a [0,1]
tiempo <- seq(1, n) / n

# Gráfico: Datos vs. Tiempo reescalado
plot(tiempo, x, type = 'l', col = 'blue', lwd = 1.5,
     xlab = 'Tiempo (reescalado en [0,1])', ylab = 'Valor observado',
     main = 'Ejemplo de Datos iid con Cola Pesada (Pareto)')

# Línea horizontal roja en y = 10
abline(h = 10, col = 'red', lwd = 2, lty = 2)

# Leyenda opcional para aclarar la línea roja
legend("topright", legend = "Umbral = 10",
       col = "red", lty = 2, lwd = 2, bty = "n")
```


```{r echo=FALSE}
# Definir el umbral
umbral <- 10

# Identificar los índices donde x excede el umbral
excesos_idx <- which(x > umbral)

# Valores excedentes y tiempos reescalados
excesos <- x[excesos_idx]
tiempos_excesos <- tiempo[excesos_idx]

# Calcular los tiempos inter-eventos (diferencias entre tiempos consecutivos)
tiempos_intereventos <- diff(tiempos_excesos)

# Crear tabla: para el último exceso no hay tiempo inter-evento (se coloca NA)
tabla_excesos <- data.frame(
  Exceso = excesos,
  Tiempo = tiempos_excesos,
  Interevento = c(tiempos_intereventos, NA)  # NA para la última fila
)

# Mostrar tabla
head(tabla_excesos, 10)  # Mostramos las 10 primeras filas
```

```{r echo=FALSE}
promedio_intereventos <- mean(tiempos_intereventos)
cat("Promedio intereventos:", promedio_intereventos, "\n")
```



Se pone a prueba: $H_0)\;\text{Los tiempos inter-eventos son independientes.}$ contra

$H_1)\;\text{Los tiempos inter-eventos son independientes.}$


```{r}
# 1. Test de Ljung-Box (autocorrelación global)
ljung <- Box.test(tiempos_intereventos, lag = 10, type = "Ljung-Box")

# 2. Runs Test (binarizando respecto a la mediana)
library(tseries)

# Crear secuencia binaria: 1 si > mediana, 0 si <= mediana
mediana_intereventos <- median(tiempos_intereventos)
secuencia_binaria <- ifelse(tiempos_intereventos > mediana_intereventos, 1, 0)

# Aplicar Runs Test a la secuencia binaria
runs <- runs.test(as.factor(secuencia_binaria))

# 4. Mostrar resultados
cat("Test de Ljung-Box: p-valor =", round(ljung$p.value, 4), "\n")
cat("Runs Test: p-valor =", round(runs$p.value, 4), "\n")
```

Se aplicaron pruebas de independencia a los tiempos inter-eventos obtenidos a partir de los excesos sobre un umbral de 10 en datos simulados de Pareto.
El Test de Ljung-Box arrojó un $p-valor = 0.9954$, indicando ausencia de autocorrelación significativa. Asimismo, el Runs Test obtuvo un 
$p-valor = 0.3215$, compatible con la hipótesis de aleatoriedad en la secuencia de tiempos inter-eventos. Estos resultados son consistentes con la generación de datos iid, y validan la hipótesis de independencia en este contexto.

Conclusión: Ambos tests no rechazan la hipótesis nula de independencia, no hay señales de dependencia temporal.

```{r echo=FALSE}
# 3. ACF visual
acf(tiempos_intereventos, main = "ACF de los tiempos inter-eventos (excesos > 10)")
```

El ACF mide la correlación entre los tiempos inter-eventos separados por diferentes lags (retardos).

Si los datos son independientes, la ACF debería oscilar cerca de 0 para todos los lags.

Se dibujan bandas azules (intervalos de confianza): si los valores están dentro → no hay autocorrelación significativa


----------
IMPORTANTE:
p. 75-89 de las notas de clase: No entendi hasta Observación 8.
----------

__Observación 8:__ Brindaremos alguna información adicional sobre los $PPNH(m)$. Supondremos que existe una función NO CONSTANTE $\lambda(x)  \geq 0$ para
todo $x>0$, continua por derecha tal que

\begin{equation}
m(A)=\int_A \lambda(x)\;dx
(\#eq:ma)
\end{equation}

y supondremos que la integral de la derecha en Eq. \@ref(eq:ma) es
finita cuando $A$ es acotado y que vale infinito cuando $A$ es el conjunto de todos los reales positivos.

Observar que si la función fuera constante se obtendría un $PP$, por eso se excluye explícitamente. Bajo las hipótesis anteriores veremos que los tiempos inter-eventos no sólo no son exponenciales (como probamos) sino que tampoco pueden ser ni
independientes, ni idénticamente distribuídos. Eso puede plantear dudas sobre cómo se simula un tal proceso, como se comprueba si se ajusta a datos
dados, como estimar la medida $m$ o la función $\lambda$ a partir de los datos, etc. Intentaremos aclarar estas inquietudes.


Intuitivamente $\lambda(t)$ representa la intensidad del
evento en el instante $t$. Por ende, un modelo sumamente razonable en
muchos eventos relacionados al clima es el anterior
con $\lambda$ una función constante a trozos, que va
tomando alternadamente 4 valores: uno para todos
los instantes del verano, otro para todos los
instantes del otoño, otro para todos los del invierno
y otros para todos los de la primavera.

Para entender más claramente, supongamos que la
unidad de tiempo es el día y que tenemos los datos
de 8 años, y veamos la gráfica de una tal función $\lambda$.


```{r echo=FALSE, warning=TRUE}
# Parámetros
dias_por_anio <- 365
anios <- 8
total_dias <- dias_por_anio * anios

# Eje de días
dias <- 1:total_dias

# Definir los valores de λ por estación (por año)
lambda_verano <- rep(10, 90)
lambda_otono  <- rep(5, 90)
lambda_invierno <- rep(2, 90)
lambda_primavera <- rep(8, 95)  # para completar los 365 días

# Un año completo de lambda
lambda_anio <- c(lambda_verano, lambda_otono, lambda_invierno, lambda_primavera)

# Repetir por 8 años
lambda_total <- rep(lambda_anio, anios)

# Graficar
plot(dias, lambda_total, type = "s", col = "blue", lwd = 2,
     xlab = "Día", ylab = expression(lambda(t)),
     main = expression("Intensidad " * lambda(t) * " por estación (8 años)"))
grid()
```

Si se tienen los datos de los registros observados durante esos 8 años (no graficados), es evidente cómo estimar el valor de cada estación: la submuestra de datos correspondiente a una estación dada se uede mostrara fácilmente que es un simple $PP$ cuyo parámetro es el valor de $\lambda$ correspondiente a dicha estación. Como ya
sabemos estimar el parámetro de un PP, sabemos cómo estimar los valores de $\lambda$ de cada estación.

Pero además, tenemos una forma de chequear que el modelo $PPNH(m)$ se ajusta a los registros observados, simplemente verificando que las 4
submuestras estacionales se ajustan cada una a un $PP$, cosa que ya sabemos hacer también!

Continuando ahora con los $PPNH$ con las suposiciones adicionales que hicimos sobre la
medida $m$ al principio de esta observación, puede demostrarse fácilmente que la densidad de $T_1$, instante del primer evento es


\begin{equation}
f_1(t)=\lambda(t)\; e^{-\int_0^t \lambda(x)\;dx}
(\#eq:f1)
\end{equation}

o sea,  $f_1(t)$ es la densidad del primer tiempo de ocurrencia del evento, donde $\lambda(t)$ es intensidad instantánea en el tiempo $t$ y $\int_0^t \lambda(x)\;dx$
la acumulación de la intensidad hasta el tiempo $t$. La función $e^{-\int_0^t \lambda(x)\;dx}$ es la función de supervivencia (probabilidad de que no haya ocurrido nada antes de $t$).


Por otro lado, puede probarse que la densidad de $T_2$, tiempo que transcurre entre el primer evento y el segundo evento es


\begin{equation}
f_2(t)=\int_0^{\infty}\lambda(u+s)\; e^{-\int_u^{u+s} \lambda(x)\;dx} \;f_1(u) du
(\#eq:f2)
\end{equation}

Puede probarse que $f_1(t)=f_2(t)$ solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO tienen idéntica distribución.

Además, la distribución conjunta de dichos dos primeros tiempos inter-eventos es

\begin{equation}
f_{1,2}(t,s)=\lambda(t+s)\;e^{-\int_t^{t+s} \lambda(x)\;dx}\;f_1(t)
(\#eq:f12)
\end{equation}


Puede probarse que $f_1(t)\:f_2(s)=f_{1,2}(t,s)$ para todo par $t>0$, $s>0$, solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO son independientes.

Quedando claro las complejidades de los $PPNH$, puede entonces preguntarse cómo se hace para
simular un $PPNH(m)$, con $m$ como supusimos al principio de la observación.
Un tal método es bastante sencillo de implementar, como veremos en lo siguiente.

Dividamos todo el eje del tiempo en intervalitos sucesivos de longitud $h$, llamémosle $\Delta_1, \Delta_2,\cdots$ y llamemos $t_j$ al extremo izquierdo de $\Delta_j$ . Simulemos
entonces para cada $j$, $N_j$, un $PP(\lambda(t_j)\;h)$, de forma tal que los distintos $PP$ sean independientes entre sí, y definamos

\begin{equation}
N(A)=\sum_j N_j(A),\quad \forall\; A
(\#eq:na)
\end{equation}

Si $h$ es muy pequeño entonces $N$ se aproxima a un $PPNH(m)$ y por lo tanto $N$ es lo que se simula.

Finalmente, uno podría desear describir un proceso de eventos con la complejidad de un $PPNH$
pero que además permita eventos múltiples, la respuesta es obviamente un Proceso de Poisson No
Homegéneo Compuesto de medida temporal $m$ y distribución de multiplicidad $G$ ($PPNHC(m;G)$),
donde un $PPNH(m)$ indica dónde se producen los eventos, e, independientemente, el orden de
multiplicidad de los eventos siguen la distribución $G$ (en los naturales).

El siguiente punto, por su relevancia práctica, amerita ser destacado especialmente.

__Observación 9:__ Estimación de la intensidad de un $PPNH$. Aquí veremos dos métodos diferentes, basados en distintos niveles de información previa y acceso a datos.
Concretamente:

a) Estimación paramétrica basada en una sola trayectoria del $PPNH$.

b) Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.

Comenzaremos con el primer abordaje, basado en el método de máxima verosimilitud.

__a. Estimación basada en una sola trayectoria del $PPNH$.__

Supondremos que observamos el $PPNH$ $N$ durante tanto tiempo como sea necesario, concretamente
que observamos $N$ sobre $[0,n)$, con $n$ muy grande. Asumiremos que $N$ tiene intensidad $\lambda$ continua. Hasta aquí, no hemos supuesto casi nada, por lo cual el abordaje sería muy general. En este punto introducimos una suposición bastante restrictiva: asumiremos que sabemos que la intensidad pertenece a una familia paramétrica, que depende
de $d$ parámetros reales para especificarse por completo. Escribiremos entonces que la intensidad es $\lambda_{\theta}$, donde $\theta$ es un parámetro $d$-dimensional, cuyo verdadero valor deseamos estimar de la forma más precisa posible. Para ello recurriremos al método de máxima verosimilitud.

Supongamos que en la observación disponible del $PPNH$ tenemos que $N([i,i+1))=n(i)$ para $i=0,1,\dots,n-1$. Los $n(i)$ son los valores concretos observados en cada intervalo. 

En máxima verosimilitud $\theta$ es estimado por $\theta_n$, valor que maximiza la función de $\theta$

\begin{equation}
L(\theta)= P(N([i, i+1))=n(i),\quad \text{para}\; i=0,1,….,n-1
(\#eq:ltheta)
\end{equation}


Usando la independencia de intervalos disjuntos, la distribución de Poisson en cada intervalo,
descartando factores que no dependen de $\theta$ y tomando la función estrictamente creciente $\log$, resulta que $\theta_n$ maximiza la siguiente función

\begin{equation}
l(\theta)=\left\{ \sum_{i=0}^{n-1} n(i)\; \log \left( \int_{[i,i+1)} \lambda_{\theta} (x) \;dx \right)\right\}-\left\{ \int_{[0,n)} \lambda_{\theta} (x) \;dx \right\}
(\#eq:lntheta)
\end{equation}

Asumiendo que $\lambda_{\theta}$ tiene dos derivadas continuas respecto a $\theta$, esta maximización puede ser realizada computacionalmente.

__b. Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.__

En este abordaje no supondremos el conocimiento previo de forma paramétrica alguna de la intensidad, por lo cual desde este punto de vista es un método más general que el anterior. En sentido contrario, supondremos que tenemos una cantidad $n$ muy grande de 'réplicas' de $N$. Esto significa que disponemos de trayectorias de $N(1),\dots,N(n)$ donde cada $N(i)$ es un $PPNH$ con la misma estructura (en particular misma intensidad, que $N$) y que $N(1),\dots,N(n)$ son independientes entre sí. El método a utilizar está basado en la estimación de densidades a partir de un núcleo (kernel). Antes de presentar el método puntualicemos que si $N$ es un $PPNH$ se pueden definir integrales (estocásticas, aleatorias) del tipo $\int_J f(x)\;dN(x)$ para $f$ continua en el intervalo $J$, como límite de sumas a la Riemann $\sum_{i=0}^{n-1}f(x(i)) \cdot N(\Delta(i))$ donde $\Delta(1),\dots,\Delta(n)$ es la partición del intervalo $J$ en intervalos muy pequeños y $x(i)$ punto intermedio de $\Delta(i)$.

El estimador que consideraremos es 

\begin{equation*}
\lambda_n(x) = \left\{ \frac{1}{n h(n)} \right\} \sum_{i=0}^{n-1} \int_{\mathbb{R}} K\left( \frac{x - t}{h(n)} \right) dN^{(i)}(t),
\end{equation*}

donde, para $n$ tendiendo a infinito, la 'ventana' $h(n)$ tiende a $0$ y $n h(n)$ tiende a infinito, y donde la función $K$ (kernel) debe ser no-negativa, par, nula fuera del intervalo $[-1,1]$ (lo cual implica que las integrales estocásticas no son sobre $\mathbb{R}$, sino sobre un intervalo acotado) y con derivadas de todos los órdenes.


En tales condiciones $\lim_{n \to 0} \lambda_n(x)=\lambda(x),\; \forall x$.

Si además se supone que $\lim_{n \to 0} \;n \cdot  h(n)^5=0$ (un ejemplo de $h(n)$ que
cumple con todas las condiciones impuestas es $h(N)= 1/nb$, con $1/5<b<1$), entonces cuando $n$ tiende
a infinito, la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{\lambda_n(x)-\lambda (x)  \right\}$ tiende a $N(0,\lambda (x))$.

Usando la fórmula de Taylor se deduce que si $g$ tiene dos derivadas continuas en un entorno de $\lambda (x)$, entonces la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{g\left( \lambda_n(x) \right)-g\left( \lambda (x) \right)  \right\}$ tiende a una $N\left( 0,g^{\prime}\left( \lambda(x) \right)^2\;\lambda(x) \right)$. 

Tomando entonces $g(x)=2\sqrt{x}$, resulta entonces que la distribución de $2\;\left\{ n \cdot h(n)) \right\}^{1/2} \left\{ \sqrt{\lambda_n(x)} \;\sqrt{\lambda(x)}\right\}$ tiende a una $N(0,1)$.

De aquí se deduce un Intervalo al $95\%$ de confianza para $\sqrt{\lambda(x)}$ y por lo tanto, el siguiente
Intervalo al $95\%$ de Confianza para $\lambda(x)$ tal que $\left[ A(x), B(x) \right]$ donde

\begin{align}
&A(x)=\lambda_n(x)-1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)},\\
&B(x)=\lambda_n(x)+1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)}.
\end{align}


