# El enfoque de conteo de eventos y los modelos de base Poissoniana

## Teoría de los procesos puntuales

Siguiendo a @coles2001introduction, se puede caracterizar un comportamiento de valores extremos de varias maneras, una de ellas es la __Teoría de los procesos puntuales__. Aunque los resultados de la inferencia obtenida bajo este esquema pueden ser equivalentes a los obtenidos con un modelo apropiado como los vistos antes, existen buenas razones para considerar este enfoque:

1- Su interpretación permite unificar a los modelos vistos anteriormente.

2- Se vincula a una verosimilitud que brinda una formulación más natural para los excesos sobre un umbral no estacionarios.

Un __proceso puntual__ es un modelo estocástico que se utiliza para describir la ocurrencia de eventos en el tiempo^[O en el espacio.]. Por ejemplo, si se desea estudiar cuándo ocurren eventos como tormentas eléctricas, terremotos o valores extremos en una serie temporal, un proceso puntual permite representar en qué momento  suceden esos eventos, sin modelar directamente su magnitud.

En este contexto, el conjunto $\mathcal{J}$ representa el dominio donde pueden ocurrir los eventos, es decir, un __intervalo de tiempo__, y el proceso puntual asigna de manera aleatoria posiciones a los eventos dentro de dicho intervalo. A partir del modelo, es posible calcular la probabilidad de que ocurra una cantidad determinada de eventos (como tormentas eléctricas o terremotos) dentro de un período de tiempo específico. Además, dado que ha ocurrido un evento, se puede estimar el tiempo esperado hasta el siguiente evento.

Una forma de caracterizar las propiedades estadísticas de un proceso puntual es definir un conjunto de variables aleatorias de valor entero no negativo, \( N(J) \), para cada subconjunto \( J \subset \mathcal{J} \), tal que \( N(J) \) representa la cantidad de puntos en el conjunto \( J \). Especificar de manera consistente la distribución de probabilidad de cada \( N(J) \) determina las medidas del proceso puntual, al cual denotamos como \( N \). 

También se pueden definir las medidas de resumen de un proceso puntual. En particular, definimos la __medida de intensidad del proceso__ como

\begin{equation}
\Lambda\left( J \right)=E\left\{ N\left( J \right) \right\}
\end{equation}

que da la cantidad esperada de puntos en cualquier subconjunto \( J \subset \mathcal{J} \).

Suponiendo que \( J = [j_1, x_1) \times \dots \times [j_k, x_k) \subset \mathbb{R}^k \), y siempre que exista, la función derivada

\begin{equation}
\lambda(x) = \frac{\partial \Lambda(J)}{\partial x_1 \cdots \partial x_k}
\end{equation}

se denomina __función de intensidad (densidad)__ del proceso.

A continucación veremos el Proceso de Poisson es el modelo más simple, natural y tractable de un proceso de conteo cuando los eventos cumplen ciertan condiciones.

<!--
1. Ocurren al azar: Los eventos son independientes unos de otros.

2. No tienen memoria (independencia de incrementos): El número de eventos en intervalos disjuntos es independiente.

3. Tienen una tasa constante (homogéneo):  
  - La probabilidad de un evento en un intervalo de longitud $h$ es aproximadamente $\lambda h$.
  - La probabilidad de más de un evento es de orden $o(h)$ (ver Def. \@ref(def:oh)).
-->


### Proceso de Poisson homogéneo unidimensional

El proceso de __Poisson homogéneo unidimensional__ es el modelo más simple para describir eventos que ocurren aleatoriamente a lo largo del tiempo, como terremotos, llamadas o llegadas de clientes. Se caracteriza por los siguientes supuestos:

\begin{itemize}
  \item Los eventos ocurren de manera independiente.
  \item La probabilidad de que ocurra un evento es constante en el tiempo.
\end{itemize}

Sea \( N(J) \) la cantidad de eventos en el intervalo \( J= [t_1, t_2] \). Esta cantidad sigue una distribución de Poisson, tal que 

\begin{equation}
N([t_1, t_2]) \sim \text{Poisson}(\lambda (t_2 - t_1))
\end{equation}

donde \( \lambda > 0 \) es la __tasa (intensidad) de ocurrencia de eventos por unidad de tiempo__.

Además, si los intervalos \( J \) y \(M  \) no se superponen, entonces $N(J) \perp\!\!\!\perp N(M)$. Es decir, el número de eventos en __intervalos disjuntos__ son variables aleatorias independientes. La medida de intensidad del proceso es $\Lambda([t_1, t_2]) = \lambda (t_2 - t_1)$, y la función de intensidad (densidad de intensidad) es simplemente constante $\lambda(t) = \lambda.$ 


Este modelo es apropiado cuando los eventos ocurren de manera aleatoria y uniforme en el tiempo.


### Proceso de Poisson no homogéneo unidimensional

El proceso de Poisson homogéneo puede generalizarse para modelar eventos que ocurren aleatoriamente en el tiempo, pero con una tasa variable \( \lambda(t) \). Esto da lugar al __proceso de Poisson no homogéneo unidimensional__, el cual mantiene la propiedad de contar eventos de forma independiente en subconjuntos disjuntos (como en el caso homogéneo), pero con una propiedad modificada:

Para todo subconjunto \( J = [t_1, t_2] \subset \mathcal{J} \), se cumple que $N(J) \sim \text{Poisson}(\Lambda(J))$, donde

\[
\Lambda(J) = \int_{t_1}^{t_2} \lambda(t)\, dt.
\]

De forma implícita, \( \Lambda(\cdot) \) es la medida de intensidad del proceso, y \( \lambda(\cdot) \) es su función de densidad de intensidad.

### Proceso de Poisson no homogéneo k-dimensional

El proceso de Poisson no homogéneo puede generalizarse aún más para describir puntos que ocurren aleatoriamente dentro de un subconjunto de un espacio \( k \)-dimensional. Un proceso puntual sobre \( \mathcal{J} \subset \mathbb{R}^k \) se dice que es un __proceso de Poisson no homogéneo \( k \)-dimensional__ con función de densidad de intensidad \( \lambda(x) \) si cumple con:

\begin{itemize}
  \item independencia de conteos sobre subconjuntos disjuntos de \( \mathcal{J} \), y
  \item para todo \( J \subset \mathcal{J} \),
  \[
  N(J) \sim \text{Poisson}(\Lambda(J)), \quad \text{donde} \quad \Lambda(J) = \int_J \lambda(x) \, dx.
  \]
\end{itemize}

La propiedad intrínseca de un proceso de Poisson es que los puntos ocurren independientemente unos de otros. La aparición de un punto en una ubicación \( x \in \mathcal{J} \) ni fomenta ni inhibe la aparición de otros puntos en las cercanías de \( x \) ni en ninguna otra parte del espacio.

Por esta razón, los procesos de Poisson son modelos ideales para fenómenos de __dispersión aleatoria__. Las variaciones en la cantidad de puntos entre diferentes subregiones del espacio \( \mathcal{J} \) pueden explicarse mediante una función de intensidad no constante, lo cual refleja una mayor cantidad esperada de puntos en ciertas regiones respecto de otras.  Sin embargo, esto no implica que la presencia de puntos en una región afecte la ocurrencia en otra. Esto también sugiere que hay fenómenos físicos para los que el proceso de Poisson no es un buen modelo, como aquellos con espaciamiento natural (por ejemplo, la ubicación de árboles en un bosque), o con agrupamiento natural (como los momentos de ocurrencia de tormentas).
 

En aplicaciones estadísticas, los modelos de procesos puntuales requieren estimar el proceso a partir de un conjunto de puntos observados \( x_1, \dots, x_n \) en una región o intervalo \( J \). Esto implica elegir una clase de modelos de procesos puntuales, y estimar los parámetros dentro de dicha clase. Nos centraremos en la estimación dentro de la familia de __procesos de Poisson no homogéneos__.

Supondremos que la función de intensidad \( \lambda(\cdot) \) pertenece a una familia paramétrica \( \lambda(\cdot\, ; \beta) \). Entonces, el problema se reduce a estimar el vector de parámetros desconocido \( \beta \), suponiendo que el modelo es válido. Esta aproximación es similar a la que se utiliza en la estimación de parámetros para distribuciones de probabilidad. Entre los métodos disponibles, el de MV se destaca como una metodología general con buenas propiedades estadísticas.

<!--
La verosimilitud se obtiene considerando la probabilidad de los datos observados como una función del parámetro desconocido \( \theta \)^[Presentamos el desarrollo en el caso más simple, donde el conjunto \( J \) es unidimensional, aunque el razonamiento es análogo para procesos de Poisson en dimensiones superiores.].
-->


### Estimación por MV

Supongamos que se han observado puntos $X_1, \ldots, X_n$ en una región $J \subset \mathbb{R}$, y que son una realización de un proceso de Poisson sobre $J$, con función de intensidad $\lambda(\cdot; \theta)$, para algún valor del parámetro $\theta$.

De los datos se sabe que algunos puntos ocurrieron para una cantidad de ubicaciones conocidas, pero en ningún otro lugar de la región. La verosimilitud se deriva utilizando estas dos informaciones.

Sea $I_i = [X_i, X_i + \delta_i]$, para $i = 1, \ldots, n$, un conjunto de pequeños intervalos centrados en las observaciones, y definimos $I = J \setminus \bigcup_{i=1}^n I_i$. Por la propiedad del proceso de Poisson,

\begin{equation}
\Pr\{N(I_i) = 1\} \approx \lambda(X_i)\delta_i,
\end{equation}

donde

\begin{equation}
\lambda(I_i; \theta) = \int_{I_i} \lambda(u; \theta) \, du \approx \lambda(X_i; \theta)\delta_i.
\end{equation}

Sustituyendo en la expresión anterior se obtiene

\[
\Pr\{N(I_i) = 1\} \approx \exp\{-\lambda(X_i)\delta_i\} \lambda(X_i)\delta_i \approx \lambda(X_i)\delta_i,
\]

donde usamos que $\exp\{-\lambda(X_i)\delta_i\} \approx 1$ para $\delta_i$ pequeños.

También,

\[
\Pr\{N(I) = 0\} = \exp\{-\lambda(I)\} \approx \exp\{-\lambda(A)\},
\]

ya que todos los $\delta_i$ son pequeños.

Por lo tanto, la verosimilitud es

\[
\Pr\{N(I) = 0, N(I_1) = 1, \ldots, N(I_n) = 1\} = \Pr\{N(I) = 0\} \prod_{i=1}^n \Pr\{N(I_i) = 1\}
\]

\[
\approx \exp\{-\Lambda(A; \theta)\} \prod_{i=1}^n \lambda(X_i; \theta)\delta_i.
\]


Dividiendo entre los $\delta_i$ para obtener una densidad, se llega a:

\begin{equation}
L(\theta; X_1, \ldots, X_n) = \exp\{-\Lambda(J; \theta)\} \prod_{i=1}^n \lambda(X_i; \theta)
(\#eq:73)
\end{equation}



donde

\[
\Lambda(A; \theta) = \int_A \lambda(x; \theta) \, dx.
\]

La verosimilitud dada en \@ref(eq:73) también es válida en el caso más general de un proceso de Poisson sobre un conjunto $J$ de dimensión $k$.

La aplicación más sencilla de la Eq. \@ref(eq:73) es la situación en la que $X_1, \ldots, X_n$ son los puntos de un proceso de Poisson homogéneo unidimensional, con parámetro de intensidad desconocido $\lambda$, observado sobre un intervalo $J = [0, t)$. En ese caso, $\Lambda(J; \lambda) = \lambda t$, entonces $L(\lambda,X_1,\dots,X_n)=exp\left\{ -\lambda t \right\}\lambda^n$.

Por lo tanto, como es habitual, es más sencillo maximizar el logaritmo de la verosimilitud

\begin{equation}
\ell(\lambda) = \log L(\lambda; X_1, \ldots, X_n) = -\lambda t + n \log \lambda,
\end{equation}

lo que conduce al estimador de máxima verosimilitud, tal que

\[
\hat{\lambda} = \frac{n}{t},
\]

que representa la tasa empírica de ocurrencia de puntos.

La maximización de la Eq. \@ref(eq:73) para modelos de procesos de Poisson no homogéneos generalmente requiere técnicas numéricas [@coles2001introduction].

<!--

\bigskip

Para aprovechar los procesos puntuales como representación de valores extremos, necesitamos una noción de convergencia que sea análoga a la convergencia de variables aleatorias.

\medskip

\noindent \textbf{Definición 7.1.} Sea $N_1, N_2, \ldots$ una sucesión de procesos puntuales sobre $J$. Se dice que la sucesión converge en distribución a un proceso $N$, denotado

\[
N_n \xrightarrow{d} N,
\]

si, para toda elección de $m$ y para todo conjunto acotado $A_1, \ldots, A_m$ tal que

\[
\Pr\{N(\partial A_j) = 0\} = 1, \quad j = 1, \ldots, m,
\]

donde $\partial A_j$ denota la frontera del conjunto $A_j$, la distribución conjunta

\[
(N_n(A_1), \ldots, N_n(A_m)) \xrightarrow{d} (N(A_1), \ldots, N(A_m)).
\]

\medskip

De forma menos formal, decimos que $N_n \xrightarrow{d} N$ si las propiedades probabilísticas de $N_n$ y $N$ son arbitrariamente similares para valores grandes de $n$.

-->


## Procesos de conteo y distribuciones Poisson

### Proceso de conteo

Considerando la Teoría de los procesos puntuales presentada anteriormente, introducimos a continuación algunas de las definiciones de los conceptos ya vistos para seguir avanzando en el estudio de los procesos de conteo.

Fijaremos un cierto __umbral__ ($u$), llamaremos __evento__ cuando la variable observada ($X_t$ para $t=1,\dots,T$) supera dicho umbral tal que $X_t>u$. Si además, consideramos un cierto intervalo de tiempo $J$, podemos definir 

$$N(J)= \text{número de eventos en el intervalo}\; J.$$

Por ejemplo, si lo que registramos son velocidades de vientos máximas medidas cada 10
minutos, y fijamos como umbral 80 km/h (aproximadamente $22.22\; m/s$), entonces
$N(Enero)=$ cantidad de períodos de 10 minutos durante enero en la que se registró una velocidad de viento superior a los $80\; km/h$.

Obviamente, dadas las fluctuaciones que tienen los fenómenos que estudiamos, intentamos realizar estadísticas sobre el número de eventos $N$ y para ello primero debemos considerar los __modelos probabilísticos__, de más simples a más complejos, presentados en la sección anterior.

$N$ es lo que se llama un __proceso de conteo__^[_Counting process_ en inglés.] o __proceso
puntual__^[_Point process_ en inglés.], un tipo de modelos empleados en logística, telecomunicaciones, estudios de contaminación atmosférica o costera, entre otros.

### Proceso de Poisson

El proceso de conteo más simple es el llamado __Proceso de Poisson__, que puede caracterizarse de la
siguiente manera.



<!-- definition #d13 name="Proceso de Poisson"
Si $N$ es un proceso de conteo y $\lambda >0$, entonces diremos que $N$
es un Proceso de Poisson de parámetro $\lambda$, $PP(\lambda)$, si se cumple que :
  
a) Para todo intervalo $J$ de los reales positivos, $N(J)$
es una variable aleatoria que tiene distribución de Poisson de parámetro $\lambda$ $longitud(J)$.

b) Si $J, L, M, \dots$ es una cantidad arbitraria de intervalos de reales positivos DISJUNTOS,
entonces $N(J), N(L), N(M),\dots$ son variables aleatorias independientes.

-->



::: {.definition #d13 name="Proceso de Poisson"}
Sea $\{N(t),\, t \geq 0\}$ un proceso de conteo. 
Se dice que es un proceso de Poisson con tasa $\lambda > 0$ ($PP(\lambda)$) si se cumplen las siguientes condiciones:


1- $N(0) = 0$

2- Tiene incrementos independientes: el número de eventos que ocurren en intervalos disjuntos de tiempo son variables aleatorias independientes.

3- El número de eventos en cualquier intervalo de longitud $t$ tiene distribución de Poisson con media $\lambda t$, es decir, para todo $\:s, t \geq 0\:$ y $\:n = 0, 1, 2, \ldots\:$ tal que

$$
\mathbb{P}(N(t + s) - N(s) = n) = \frac{e^{-\lambda t} (\lambda t)^n}{n!}
$$



Como consecuencia del punto (3), el proceso tiene incrementos estacionarios y se cumple que
\[
\mathbb{E}[N(t)] = \lambda t.
\]

Es por este motivo que decimos que $\lambda$ es la tasa del proceso [@ross2014].
:::


::: {.definition #dfinter name="Distribuciones de Tiempos inter-eventos y de Espera"}

Consideremos un proceso de Poisson y definamos el tiempo del primer evento como $T_1$. 

Para $n > 1$, sea $T_n$ el tiempo transcurrido entre el $(n-1)$-ésimo y el $n$-ésimo evento. 
La secuencia $\{T_n,\; n = 1, 2, \dots\}$ se llama la __secuencia de tiempos inter-eventos__. Por ejemplo, si $T_1 = 5$ y $T_2 = 10$, entonces el primer evento del proceso de Poisson ocurre en el instante 5 y el segundo en el instante $5 + 10 = 15$.

\vspace{0.3cm}
Determinemos ahora la distribución de los tiempos $T_n$. Comenzamos observando que el evento $\{T_1 > t\}$ ocurre si y sólo si no ocurre ningún evento del proceso de Poisson en el intervalo $[0, t]$, por lo tanto:

\[
P(T_1 > t) = P(N(t) = 0) = e^{-\lambda t}
\]

Esto implica que $T_1$ tiene una distribución exponencial con media $1/\lambda$.
\vspace{0.3cm}

Ahora bien, $P(T_2 > t) = \mathbb{E}[P(T_2 > t \mid T_1)]$.

\vspace{0.3cm}
Sin embargo,

\begin{align}
P(T_2 > t \mid T_1 = s) &= P(\text{0 eventos en } (s, s + t] \mid T_1 = s) \nonumber \\
&= P(\text{0 eventos en } (s, s + t]) \nonumber \\
&= e^{-\lambda t} 
(\#eq:512)
\end{align}

donde las últimas dos igualdades se deducen de las propiedades de incrementos independientes y estacionarios del proceso de Poisson.
\vspace{0.3cm}
Por lo tanto, a partir de la Ecuación \@ref(eq:512) concluimos que $T_2$ también es una variable aleatoria exponencial con media $1/\lambda$ y, además, que $T_2$ es independiente de $T_1$. Repitiendo el mismo argumento para $T_3, T_4,\dots$, se obtiene la misma conclusión para toda la secuencia (@ross2014).
:::

::: {.proposition #exp} 

$T_n,\; n = 1, 2, \dots,\;$son variables aleatorias exponenciales independientes e idénticamente distribuidas con media $\frac{1}{\lambda}$.

\vspace{0.3cm}
El proceso de Poisson no tiene memoria: desde cualquier punto en el tiempo, su evolución futura es independiente del pasado y sigue la misma distribución. Por eso, los tiempos entre eventos (interarribos) siguen una __distribución exponencial__ (@ross2014).
:::

El siguiente teorema brinda una visualización muy interesante de los Procesos de Poisson, que nos
servirá mucho para introducir otros modelos y que es ideal para poder simular computacionalmente
Procesos de Poissson.

```{theorem, label="d31", name="Otra visión de los Procesos de Poisson"}
Si $T_1,\dots,T_n,\dots$ es $iid$ con distribución Exponencial de
parámetro $\lambda>0$ y definimos que ocurre el primer
evento en el instante $T_1$, el segundo en el instante
$T_1+T_2$, el tercero en el instante $T_1+T_2+T_3$ y asì
sucesivamente, el proceso $N$ de conteo de tales
eventos, es un proceso de Poisson.
```

Dicho de otro modo el Proceso de Poisson representa eventos aislados (“que ocurren de a uno y claramente separados”), con tiempos inter-eventos $iid$ y exponenciales.
Obviamente, esto muchas veces es “too good to be true”, pero variaciones de este modelo tan simple nos brindarán a menudo modelos realistas.



__Observación 1.__ En la práctica, si se toman datos
en los instantes $1,\dots,n$ suele reescalarse el tiempo
dividiendo por $n$ y los instantes quedan en $[0,1]$. Allí se define un PP de manera casi idéntica, obviamente modificando en la Def. \@ref(def:d13), tanto en
a) como en b) que los intervalos deben estar
contenidos en $[0,1]$.




__Observación 2.__ Conviene recordar que si $X$ es
una $V.A.$ Poisson de parámetro $\lambda>0$ y $T$ es una $V.A.$
exponencial de parámetro $\lambda$, entonces $E(X)= \lambda$ y $E(T)=1/\lambda$.
Si $T_1,\dots,T_n,\dots$ $iid$ son los tiempos inter-eventos de un
$PP(\lambda)$ se deduce entonces de la ley de los grandes
números que

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n T_i = \frac{1}{\lambda}
$$

Es decir que el tiempo promedio entre eventos “a
la larga ” es $\frac{1}{\lambda}$. De manera similar, si $J1,\dots,J_n,\dots$ son
intervalos disjuntos de longitud 1, por la definición
1 y la ley de los grandes números se tiene que

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^n N(J_i) = \lambda
$$

Más aún, puede probarse que:

$$
\lim_{t \to \infty} \frac{N(0,t)}{t} = \lambda
$$
Esto permite observar una consecuencia del Teorema \@ref(thm:d31), que es una propiedad intuitivamente
muy atractiva.

La tasa promedial de incidencia de los eventos en un $PP(\lambda)$ es inversamente proporcional al tiempo
promedial inter-eventos.



::: {.definition #tiempoar name="Tiempo de espera hasta el n-ésimo evento"}

Otra cantidad de interés es \( S_n \), el tiempo de arribo del \( n \)-ésimo evento, también llamado tiempo de espera hasta el \( n \)-ésimo evento. Se define

\[
S_n = \sum_{i=1}^{n} T_i, \quad n \geq 1,
\]

y por lo tanto, a partir de que los \( T_i \) son variables aleatorias exponenciales independientes e idénticamente distribuidas con parámetro \( \lambda \), se deduce que \( S_n \) sigue una distribución Gamma con parámetros \( n \) y \( \lambda \) [@ross2014]. 

:::
<!--
Es decir, la función de densidad de probabilidad de \( S_n \) está dada por:

\[
f_{S_n}(t) = \lambda e^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1)!}, \quad t \geq 0.
\]

Esta expresión también puede obtenerse observando que el \( n \)-ésimo evento ocurre en o antes del instante \( t \) si, y solo si, han ocurrido al menos \( n \) eventos en ese intervalo.

-->



```{example, label="ex13", name="Proceso de ocupación de las líneas entre dos centrales"}
En las primeras dos décadas del siglo XX, un
creador genial como Agner Erlang modeló mediante Procesos de Poisson las llamadas que
arribaban a una central telefónica, así como (con parámetros muy distintos) el proceso de ocupación
de las líneas entre dos centrales. 

Eso condujo no sólo al desarrollo de las primeras centrales de
telefonía conmutada por circuitos por CTC, la filial danesa de Bell, sino además a que Erlang
desarrollara su “fórmulas de bloqueo”, fino cálculo por el cual, según los parámetros del proceso de
arribo y del proceso de ocupación de líneas, se calcula la probabilidad de “saturación” (no hay
ninguna línea disponible) dado el número de líneas entre centrales, o, dada una probabilidad de
saturación “tolerable” $\varepsilon$ DISEÑAR (determinar el mínimo número de lineas necesarias para que la probabilidad de bloqueo no exceda $\varepsilon$). 

Si el tiempo entre arribos de llamadas a la central es Exponencial de parámetro $\lambda$, y la duración media de una llamada es Exponencial de parámetro $\mu$, entonces el __parámetro crucial__ de la fórmula de Erlang es

$$
\rho=\frac{\lambda}{\mu}=\frac{\text{Duración media de la llamada}}{\text{Tiempo medio entre llamadas}}
$$  
  
y a mayor valor de $\rho$, mayor probabilidad de saturación para una conectividad dada. 


Esta fórmula aún sigue en uso en algunos problemas y
dió pie al desarrollo de fórmulas de bloqueo más sofisticadas para situaciones más complejas. 

La unidad en la que se mide la intensidad de tráfico en redes se llama “erlang” y este ejemplo nos parece una clara muestra de cuán útil ha sido el muy sencillo Proceso de Poisson.

```

\vspace{0.5cm}

Sin embargo, en otros problemas, por ejemplo en modernas redes de datos en las que los eventos relacionados a la demanda de servicio pueden ocurrir simultáneamente en muy grandes cantidades (clustering), aparece un modelo más sofisticado, que puede ser definido a partir del Proceso de Poisson, el __Proceso de Poisson Compuesto__. 

Cuando hay dependencia temporal en las observaciones, los extremos pueden aparecer agrupados (en clusters) por lo que es adecuado emplear este modelo [@poissonComp].


\vspace{0.5cm}

### Generalizaciones del Proceso de Poisson

#### Proceso de Poisson Compuesto

::: {.definition #def13 name="Proceso de Poisson Compuesto"}

Sea \( N(t) \) un proceso de Poisson homogéneo con tasa \( \lambda > 0 \), y sea \( \{S_i\}_{i \in \mathbb{N}} \) una sucesión de variables aleatorias independientes e idénticamente distribuidas (iid) con valores en \( \mathbb{N} = \{1, 2, 3, \dots\} \), y además independientes de \( N(t) \). Entonces, el proceso estocástico \( \{M(t),\ t \geq 0\} \) definido por

\[
M(t) = \sum_{i=1}^{N(t)} S_i, \quad t \geq 0,
\]


se llama **Proceso de Poisson Compuesto** con tasa \( \lambda \) y distribución de saltos \( G \), donde $G$ es la ley de los \( S_i \) [@ross2014]. 

Seguimos la notación \( M \sim PPC(\lambda; G) \).
:::



<!--
::: {.definition #def13 name="Proceso de Poisson Compuesto"}
Si $N$ es un Proceso de Poisson de parámetro $\lambda>0$ , $G$ es una distribución de probabilidad en los naturales $(1,2,3,\dots)$, consideramos $S_1,\dots,S_n,\dots$ iid con distribución $G$ y construímos un nuevo proceso de conteo $M$ de la forma siguiente:
  
- Cuando $N$ tiene su primer evento, $M$ tiene $S_1$ eventos simultáneos; 

- Cuando $N$ tiene su segundo evento, $M$ tiene $S_2$ eventos simultáneos.....y así sucesivamente.

decimos que $M$ es un Proceso de Poisson Compuesto de parámetro $\lambda>0$ y distribución de eventos $G$, abreviaremos $M$ es $PPC(\lambda;G)$.
:::



Matematicamente, un proceso estocástico \( \{X(t),\ t \geq 0\} \) se dice que es un __proceso de Poisson compuesto__ si puede representarse como

\[
X(t) = \sum_{i=1}^{N(t)} Y_i, \quad t \geq 0,
\]

donde \( \{N(t),\ t \geq 0\} \) es un proceso de Poisson, y \( \{Y_i,\ i \geq 1\} \) es una familia de variables aleatorias independientes e idénticamente distribuidas ($iid$) que además son independientes del proceso \( N(t) \). Denominamos \( X(t) \) como la variable aleatoria de Poisson compuesta [@ross2014].

-->








::: {.exercise #ppc name="PPC"}
Demostrar que para un $PPC(\lambda;G)$ el tiempo medio inter-eventos sigue siendo $1/\lambda$, pero que la tasa de incidencia media de eventos ahora es $\lambda E(G)$.
:::






__Observación 3.__ Para aclarar, si $G$ es una distribución degenerada otorga al $1$ probabilidad $1$, el correspondiente $PPC(\lambda;G)$ en realidad es un $PPC(\lambda)$. Ergo, el $PP$ es un caso particular de $PPC$.

<!--
En otras palabras, el proceso de Poisson $PP(\lambda)$ puede verse como un caso particular de un proceso de Poisson Compuesto donde siempre ocurre un único evento en cada arribo del proceso de Poisson base.
-->

__Observación 4.__ Para evitar confusiones frecuentes, distinguiremos explícitamente estos procesos de los llamados _Procesos de Poisson no-homogéneos_. Para ello recordemos, sin entrar en tecnicismos, que una medida en los reales positivos es una función que a los conjuntos asocia números positivos con las mismas propiedades formales,
excepto que no tiene por qué dar a todo el conjunto de los reales positivos (a todo el universo) el valor 1. 

Dicho de otro modo, una probabilidad es una medida particular, que a todo el universo asigna el valor 1. Puede pensarse como ejemplo típico de una medida, la que asigna a un conjunto la integral sobre ese conjunto de una función no negativa (no
necesariamente de integral total 1, puede ser incluso infinita). La longitud es el ejemplo más simple de medida (llamada también _Medida de Lebesgue_) y la longitud de todos los reales positivos es infinito. Puede demostrarse que la longitud multiplicada por una constante no negativa son las únicas medidas invariantes por traslaciones, punto importante para la distinción que queremos hacer.


#### Procesos de Poisson no-homogéneos



::: {.definition #oh name="Orden pequeño respecto de h"}
Una función \( f(h) \) se dice que es \( o(h) \) si
\[
\lim_{h \to 0} \frac{f(h)}{h} = 0.
\] 
:::

Por ejemplo, la función \( f(x) = x^2 \) es \( o(h) \) ya que

\[
  \frac{f(h)}{h} = \frac{h^2}{h} = h \quad \Rightarrow \quad \lim_{h \to 0} \frac{f(h)}{h} = \lim_{h \to 0} h = 0.
 \]




::: {.definition #ppnh name="Proceso de Poisson No Homogéneo"}
Un proceso de conteo \( \{N(t),\ t \geq 0\} \) se denomina \textbf{proceso de Poisson no homogéneo} con función de intensidad \( \lambda(t) \), si cumple las siguientes condiciones:

\begin{enumerate}
  \item \textbf{Condición inicial:} 
  \[
  N(0) = 0.
  \]

  \item \textbf{Incrementos independientes:} Para cualquier elección de intervalos de tiempo no solapados, los incrementos del proceso en esos intervalos son independientes.

  \item \textbf{Probabilidad de múltiples eventos en un intervalo pequeño:}
  \[
  \mathbb{P}(N(t+h) - N(t) \geq 2) = o(h),
  \]
  es decir, la probabilidad de que ocurran dos o más eventos en un intervalo infinitesimal \( h \) es despreciable en comparación con \( h \) cuando \( h \to 0 \).

  \item \textbf{Probabilidad de un solo evento en un intervalo pequeño:}
  \[
  \mathbb{P}(N(t+h) - N(t) = 1) = \lambda(t) h + o(h),
  \]
  lo que significa que la probabilidad de que ocurra un único evento en el pequeño intervalo \( [t, t+h) \) es aproximadamente proporcional a \( h \), con constante de proporcionalidad \( \lambda(t) \).
\end{enumerate}
:::

De manera equivalente, podemos plantear la siguiente Definición.
<!--
::: {.definition #def14 name="Proceso de Poisson No Homogéneo (PPNH)"}
Sea \( m \) una medida definida en los subconjuntos de los reales positivos. Un proceso de conteo \( N \) se llama Proceso de Poisson No Homogéneo de medida \( m \), y se denota \( N \sim \text{PPNH}(m) \), si:

- Para todo intervalo \( J \subset \mathbb{R}_+ \), la variable \( N(J) \sim \text{Poisson}(m(J)) \).

- Si \( J_1, J_2, \dots, J_k \) son intervalos disjuntos, entonces \( N(J_1), N(J_2), \dots, N(J_k) \) son variables aleatorias independientes.
:::

Si la medida \( m \) admite una densidad respecto de la medida de Lebesgue, es decir, si existe una función no negativa \( \lambda(t) \) tal que
\[
m([a,b]) = \int_a^b \lambda(t)\,dt,
\]
entonces \( N \) es un Proceso de Poisson No Homogéneo con función de intensidad \( \lambda(t) \), y cumple

- \( N(0) = 0 \),
- Incrementos independientes,
- Para \( h \to 0 \),
  \[
  \mathbb{P}(N(t+h) - N(t) = 1) = \lambda(t) h + o(h), \quad \mathbb{P}(N(t+h) - N(t) \geq 2) = o(h).
  \]

Este enfoque es útil cuando la función de intensidad \( \lambda(t) \) tiene una expresión conocida, lo que permite modelar y simular el proceso más fácilmente.
-->

```{definition, label="def14", name="Proceso de Poisson No Homogéneo"}
Si $N$ es un proceso de conteo y $m$ es una medida que no puede expresarse como una constante por la longitud, diremos que $N$ es un Proceso de Poisson No Homogéneo de medida $m$ ($N$ es $PPNH(m)$ ) si se cumple:
  
a) Para todo intervalo $J$ de los reales positivos, $N(J)$ es una variable aleatoria que tiene distribución de Poisson de parámetro $m(J)$.


b) Si $J, L, M, \dots$ es una cantidad arbitraria de intervalos positivos DISJUNTOS, entonces $N(J), N(L), N(M),\dots$ son variables aleatorias independientes.
```


La Definición \@ref(def:def14)  usa una medida $m$ para definir la intensidad. Si esa medida fuera simplemente proporcional a la longitud (como pasa en el Poisson homogéneo), entonces el proceso sería homogéneo. Por eso, para evitar confusiones, en la definición de proceso de Poisson no homogéneo se aclara que $m$ no debe ser constante por longitud.

Para dejar en claro la diferencia entre los $PPNH$ y los $PPC$ (o el simple $PP$), recordemos que en los $PPC$, los tiempos inter-eventos son exponenciales de parámetro $\lambda>0$ e $iid$. El siguiente resultado muestra la diferencia de conceptos. 

Para destacar la diferencia entre los procesos de Poisson no homogéneos ($PPNH$) y los procesos de Poisson compuestos ($PPC$) —o incluso el proceso de Poisson simple ($PP$)— recordemos que en los $PPC$, los tiempos entre eventos son variables aleatorias exponenciales independientes e idénticamente distribuidas ($iid$) con parámetro $\lambda>0$.


```{theorem, label="th13", name="PPNH no es PPC"}
Si $N$ es un $PPNH$ y $T_1$ es el tiempo del primer evento, la distribución de $T_1$ no es exponencial. Por lo tanto, un $PPNH$ no es $PPC$.
```


__Demostración:__

\begin{align}
P(T_1 \leq t) &= P(N((0,t)) \geq 1) \\
&= 1 - P(N((0,t)) = 0) \\
&= 1 - e^{-m((0,t))}, \quad \forall\, t > 0.
\end{align}

Si $T_1$ fuera una variable aleatoria exponencial, entonces, para algún $\lambda > 0$ y para todo $t > 0$, se tendría:

\[
P(T_1 \leq t) = 1 - e^{-\lambda t} \quad \Rightarrow \quad m((0,t)) = \lambda t.
\]

Entonces, para cualesquiera $a < b$,

\begin{align}
m((a,b)) &= m((0,b)) - m((0,a)) \\
&= \lambda b - \lambda a = \lambda (b - a) \\
&= \lambda \cdot \text{longitud}((a,b)).
\end{align}

Por lo tanto, $m = \lambda \times \text{longitud}$, lo cual contradice nuestra hipótesis de que $m$ **no** es una medida proporcional a la longitud. \hfill $\diamond$




<!--
__Demostración:__  texto de Gonza

\begin{align}
P(T_1\leq t)&= P(N((0,t))\geq 1)\\
&=1-P(N((0,t))=0)\\
&=1-e^{-m((0,t))},\;\forall\;t>0.
\end{align}

Si $T_1$ fuera exponencial entonces, para algún $\lambda>0$ y para todo $t>0$, sería $m((0,t))=\lambda t$. Por ende, si $a<b$ cualquiera, 

\begin{align}
m((a,b))&= m((0,b))-m((0,a))\\
&=\lambda b - \lambda a=\lambda(a-b)\\
&=\lambda x \;\text{longitud}((a,b)).
\end{align}

Por lo cual se concluye $m=\lambda x \;\text{longitud}$, lo cual es absurdo $\diamond$.

-->
\newpage

## Técnica de pasajes de altos niveles (HLE)

### Distribución Generalizada de Pareto

Esta aproximación se basa en la Distribución de Pareto Generalizada (Def. \@ref(def:dgp)), la cual tiene una interpretación como distribución límite, similar a la distribución Gumbel (@gumbel1958). Esta idea fue sugerida por primera vez por @pickands1975 y ha sido desarrollada en la literatura [@davison1990]. 

Los modelos más simples de excedencias sobre umbrales se basan en procesos de Poisson de tiempos de excedencia combinado con excedencias independientes sobre el umbral. En este sentido, se fija un umbral $u$ y sea $N$ la cantidad de excedencias sobre $u$ en un período de $m$ años. Por ejemplo, muchas aplicaciones medioambientales consideran un año como la unidad temporal [@davison1990].

Supongamos que las excedencias, o diferencias entre las observaciones por encima del umbral y el propio umbral, son independientes y comparten una función de distribución común $G$.  


Asumimos que $G$ es una Distribución Generalizada de Pareto dada por


<!--
Supongamos que las excedencias, o diferencias entre las observaciones por encima del umbral y el propio umbral, son independientes y comparten una función de distribución común $G$.  Asumimos que $G$ es una Distribución Generalizada de Pareto dada por 


\begin{equation}
G(x; \alpha, \kappa) = 1 - \left(1 - \frac{\kappa x}{\alpha} \right)^{1/\kappa},
(\#eq:pickands)
\end{equation}

donde \( \alpha > 0 \) y \( \kappa \) es un parámetro arbitrario. 

- El rango de \( x \) es \( 0 < x < \infty \) si \( \kappa \leq 0 \), y \( 0 < x < \alpha / \kappa \) si \( \kappa > 0 \).

- El caso \( \kappa = 0 \) se interpreta como el límite cuando \( \kappa \to 0 \), es decir, la distribución exponencial con media \( \alpha \). 

- El caso \( \kappa < 0 \) es simplemente una reparametrización de una de las formas usuales de la distribución de Pareto, pero la extensión para \( \kappa \geq 0 \) fue dada por @pickands1975.

-->

```{definition, label="dgp", name="Distribución Pareto Generalizada."}
Si $k$ real y $\sigma >0$, la Distribución de Pareto Generalizada $G_{k, \sigma}$ se define de la siguiente manera

\begin{equation}
G_{k,\sigma}(x) = 
\begin{cases}
1 - \left(1 + \dfrac{kx}{\sigma} \right)^{-1/k} & \text{si } k \neq 0,\quad
\begin{cases}
x \geq 0 & \text{si } k > 0 \\
0 \leq x \leq -\sigma/k & \text{si } k < 0
\end{cases} \\
1 - e^{-x/\sigma} & \text{si } k = 0,\quad x \geq 0
\end{cases}
(\#eq:dpg)
\end{equation}

```


- Cuando $k=0$ la Eq.\@ref(eq:dpg) corresponde a la distribución
exponencial de parámetro $1/\sigma$, por lo cual $\sigma$ sería la media de la distribución. 

- El caso $k=-1$ corresponde a la distribución uniforme en $[0, \sigma]$, por lo cual la media sería $\sigma/2$. 

- El caso $k>0$ corresponde a la distribución de Pareto.

La familia de Distribuciones de Pareto Generalizada es continua, en el sentido que cuando $k$ tiende a cero por derecha o izquierda, $G_{k, \sigma}$ tiende a $G_{0,\sigma}$. Lo mismo ocurre con las distribuciones extremales vistas en el capítulo 1, como el lector puede verificar.



@pickands1975 demostró que la Eq.\@ref(eq:dpg) surge como una distribución límite de las excedencias sobre umbrales si y solo si la distribución original pertenece al dominio de atracción de una de las distribuciones de valores extremos. 

__Propiedad 1. (Estabilidad respecto al umbral)__ Si \( X \) sigue una distribución de Pareto generalizada y \( u > 0 \), entonces la distribución condicional de \( X - u \) dado que \( X > u \), también es una distribución de Pareto generalizada. 

__Propiedad 2. (Del máximo con Poisson)__ Si \( N \) tiene una distribución de Poisson y, condicionalmente a \( N \), \( X_1, \ldots, x_N \) son variables aleatorias $iid$ con distribución de Pareto generalizada, entonces el máximo \( \max(X_1, \ldots, X_N) \) sigue una distribución de valores extremos generalizada. Por lo tanto, un proceso de Poisson de tiempos de excedencia con excedencias de Pareto generalizada implica las distribuciones clásicas de valores extremos.


Ambas propiedades caracterizan a la Distribución de Pareto Generalizada en el sentido de que ninguna otra familia las posee. Estas propiedades sugieren que la distribución de Pareto generalizada será una familia útil para la estimación estadística, siempre que se tome un umbral suficientemente alto. 

Una pregunta importante es cuán alto debe ser ese umbral. Si bien existen en la literatura resultado teóricos, no se han desarrollado lo suficiente como para tener un valor práctico. En su lugar, adoptaremos un enfoque más pragmático en el que se emplea un conjunto de técnicas diagnósticas para evaluar el ajuste del modelo [@davison1990].


<!--
__Observación 5.__ Si $n$ es grande y se dispone de una muestra $X_1,\dots,Xn,\dots$ donde se intenta describir evento
s extremos de este tipo de datos, la técnica de pasajes de altos niveles, $HLE$ (High Level Exceedances, en inglés), implica hacer un _tunning_ del umbral en función de $n$, respecto al cual se considerará un evento “extremo”, pues es lógico que para mayor $n$, sea posible distinguir con mayor precisión eventos de extremos de mayor o menor intensidad. 
-->

__En suma:__ Cuando el tamaño muestral \( n \) es grande y se dispone de una secuencia de datos \( X_1, \dots, X_n \), una estrategia común para el análisis de eventos extremos es la técnica de \emph{pasajes de altos niveles}^[High Level Exceedances, HLE]. Esta consiste en fijar un umbral \( u_n \) dependiente de \( n \), a partir del cual se considera que una observación corresponde a un evento “extremo”. Dado que una muestra mayor permite discriminar mejor entre niveles de rareza, es natural que el umbral se ajuste con \( n \) para capturar eventos cada vez más extremos [@hle].

En este enfoque, se suele asumir que el umbral \( u_n \) se elige de modo tal que:

\begin{equation}
n \, P(X_1 > u_n) \to \lambda, \quad \text{cuando } n \to \infty, \quad \text{con } \lambda > 0.
(\#eq:condicion)
\end{equation}

__Observación 5.__ Si se cumple la condición anterior, entonces asintóticamente 

$$ P(X_1 > u_n) \approx \lambda/n.$$ 

Es decir, el evento de que una observación supere el umbral \( u_n \) se vuelve cada vez más raro a medida que crece \( n \). Por este motivo, en la literatura estadística algunos autores denominan a este tipo de enfoques \emph{Leyes de los pequeños números}^[Laws of Small Numbers, @flaws], ya que se enfocan en contar eventos poco frecuentes.

En lo que sigue, desarrollaremos resultados concretos bajo este paradigma. Para facilitar el tratamiento asintótico, reescalaremos el índice \( j \) de cada observación al intervalo unitario mediante la transformación \( j \mapsto j/n \in [0,1] \).


<!--
Si se cumple a., entonces $P(X_1>u_n)\approx \lambda/n$, por lo cual, si $n$ es grande, el evento de que los datos observados superen el umbral un es un _evento raro_, con probabilidad muy baja, poco frecuente. Esto hace que al conteo de eventos extremos respecto a un tal umbral en la Literatura Estadística, algunos autores le llamen _Laws of Small Numbers_ (@flaws), pues se cuenta algo poco frecuente. Iremos a resultados concretos en tal dirección. Reescalamos el instante $j$ al instante $j/n$ de $[0,1]$. 
-->

```{theorem, label="th14", name="HLE en el caso iid"}
Si $X_1,\dots,X_n,\dots$ es $iid$, se cumple la condición de la Eq. \@ref(eq:condicion) y se
considera el proceso puntual $N_n$ de pasajes de altos niveles, definido para todo intervalo $J$ de $[0,1]$ por
$$
N_n(J)= \text{Cantidad de i en}\; nJ\; \text{tales que} X_i>u_n,
$$
entonces cuando $n$ tiende a infinito, $N_n$ tiende a un $PP( \lambda )$.
```


__Observación 6.__ Uno de los elementos más notorios que produce la dependencia entre los
datos, aunque sea débil, es el “clustering” o agrupamiento de los datos muy grandes. Esto se traduce a que los pasajes altos se disparan de acuerdo a Proceso de Poisson, pero cuando se disparan, pueden dispararse varios juntos. Esta es la base intuitiva del siguiente resultado.


```{theorem, label="th15", name="HLE para procesos estacionarios débilmente dependiente"}
Si $X_1,\dots,X_n,\dots$ es un proceso estacionario y débilmente dependiente que cumple la condición de la Observación $5$ y se considera el proceso puntual $N_n$ de “pasajes de altos niveles” como antes, entonces si

$$
\lim_{n \to \infty} N_n=PPC( \lambda;G )
$$
  
y $G$ puede ser identificado por una fórmula (o bien estimada a partir de la muestra de “saltos”).
```

Esto brinda un modelo razonablemente realista en muchas situaciones. Sin embargo, para modelar contaminación urbana por $O_3$, dada la presencia de covariables muy influyentes debimos lidiar con un modelo más complejo que veremos más adelante.


\newpage

### Ejemplos concretos de HLE

Para “bajar a tierra” los conceptos vistos vamos a ver ejemplos concretos de HLE.  En todos los casos vamos a suponer que $n=5000$ y reescalemos el tiempo para llevarlo a $[0,1]$.

#### HLE para datos $iid$

<!---  [Text to display](https://otexts.com/fpp3/decomposition.html) -->


- Simulamos datos $iid$
- Introducimos \textit{outliers} grandes (por ejemplo, 20 valores artificialmente altos).
- Elegimos un umbral \( u_n \) tal que aproximadamente
    \[
    \mathbb{P}(X > u_n) \approx \frac{\lambda}{n}
    \]
    Por ejemplo, con \( \lambda = 20 \), entonces usamos \( u_n \) tal que:
    \[
    \texttt{quantile}(X, \texttt{probs} = 1 - 20/n)
    \]
    



```{r echo=TRUE}
set.seed(123)

# Parámetro del proceso de Poisson (intensidad esperada de eventos)
lambda <- 20
n <- 5000

# 1. Simular 20 tiempos inter-evento exponenciales
inter_event_times <- rexp(lambda, rate = lambda)

# 2. Obtener los tiempos acumulados y reescalarlos a [0,1]
rescaled_event_times <- cumsum(inter_event_times)
rescaled_event_times <- rescaled_event_times / max(rescaled_event_times)  

# 3. Generar una señal de fondo iid normal
X <- rnorm(n)

# 4. Convertir los tiempos de ocurrencia a posiciones de índice
event_idx <- as.integer(rescaled_event_times * n)
# asegurar que estén dentro de [1, n]
event_idx <- pmin(pmax(event_idx, 1), n)  

# 5. Insertar outliers en esos puntos
X[event_idx] <- rnorm(lambda, mean = 8, sd = 2)

# 6. Definir umbral
u_n <- quantile(X, probs = 1 - lambda / n)

# 7. Obtener los tiempos donde hay excedencias
exceed_idx <- which(X > u_n)
rescaled_times <- exceed_idx / n

# 8. Calcular los tiempos inter-eventos reales
inter_times <- diff(rescaled_times)
```



```{r hleiid, fig.cap = "Datos iid + outliers aleatorios y espaciados", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
plot((1:n)/n, X, type = "l", col = "grey40",
     xlab = "Tiempo reescalado", ylab = "Valor")

abline(h = u_n, col = "red", lwd = 2, lty = 2)
points(rescaled_times, X[exceed_idx], col = "blue", pch = 19)

legend("topright", legend = paste("Umbral u_n =", round(u_n, 2)),
       col = "red", lty = 2, lwd = 2, bty = "n")
```
El umbral es \( u_n \) y se observan 20 pasajes de dicho umbral, claramente separados. 
Parece razonable intentar modelarlo mediante un proceso de Poisson. 
Esto será posible si los tiempos inter-eventos son independientes e idénticamente distribuidos ($iid$) con distribución exponencial.

Veamos primero en qué instantes se dan los pasajes del umbral  (recordar que reescalamos el tiempo dividiendo por \( n = 5000 \))  y luego analizaremos los tiempos inter-eventos.



Los eventos (excedencias del umbral \( u_n \)) ocurren en los siguientes instantes de tiempo reescalado $\{ t_1, t_2, \dots, t_{20} \}$, donde cada \( t_i \in [0,1] \) indica el momento en que un valor \( X_i > u_n \).


```{r echo=TRUE}
# Mostrar los tiempos de ocurrencia de eventos (excedencias)
round(rescaled_times, 4)
```


Los tiempos inter-eventos se calculan como las diferencias entre los instantes de ocurrencia de las excedencias del umbral:
\[
\Delta_i = t_{i+1} - t_i, \quad i = 1, \dots, 19.
\]

```{r echo=TRUE}
# 1. Calcular los tiempos inter-eventos
inter_times <- diff(rescaled_times)
# 2. Mostrar los tiempos inter-eventos con 4 decimales
round(inter_times, 4)
```

Obsérvese que el tiempo promedio entre eventos es
\[
\bar{\Delta} = \frac{1}{19} \sum_{i=1}^{19} (t_{i+1} - t_i), 
\]
dando un valor aproximado de \( \bar{\Delta} \approx x \).  

La estimación del parámetro \( \lambda \) de una distribución exponencial es entonces $\hat{\lambda} = \frac{1}{\bar{\Delta}}$.


```{r echo=TRUE}
mean_inter <- mean(inter_times)
lambda_hat <- 1 / mean_inter

# Mostrar resultados
cat("Tiempo promedio inter-eventos:", round(mean_inter, 4), "\n")
cat("Estimación del parámetro de la exponencial:", round(lambda_hat, 2), "\n")
```

La estimación de $\lambda$ es $1/ 0.0499=20.04.$




Tests de aleatoriedad a los tiempos inter-eventos:

1- Prueba de Kolmogorov-Smirnov (KS): Verifica si los tiempos inter-eventos provienen de una distribución exponencial (como en un Proceso de Poisson Homogéneo).



```{r}
# Estimar parámetro lambda
lambda_hat <- 1 / mean(inter_times)

# Prueba KS: ¿los tiempos inter-evento siguen Exp(lambda_hat)?
ks_result <- ks.test(inter_times, "pexp", rate = lambda_hat)

# Mostrar resultado
print(ks_result)
```


Dado que el p-valor es alto ($0.6686 > 0.05=\alpha$), no se rechaza $H_0)$. Por lo tanto, los datos son compatibles con una distribución exponencial, lo cual respalda el supuesto de que los tiempos inter-evento pueden haber sido generados por un proceso de Poisson

2- Prueba de Runs (Rachas)


```{r warning=FALSE}
# install.packages("randtests")
# Runs test
library(randtests)
runs.test(inter_times)
```
No se rechaza la hipótesis nula de aleatoriedad ($p-value = 0.1449>0.05=\alpha$).
Es decir, no hay evidencia suficiente para afirmar que los tiempos inter-eventos no sean aleatorios. Esto es coherente con la hipótesis de que los eventos siguen un proceso de Poisson, donde los tiempos entre eventos son $iid$ exponenciales.



```{r hleiidacf, fig.cap = "ACF de los tiempos inter-eventos", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# ACF
acf(inter_times, main="")
```

La Fig. \@ref(fig:hleiidacf) para los tiempos inter-eventos permite visualizar si hay dependencia serial entre los valores. En un proceso de Poisson, los tiempos inter-eventos deberían ser $iid$ exponenciales y por lo tanto, sin autocorrelación.



```{r histiid, fig.cap = "Histograma de los tiempos inter-eventos", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
breaks <- c(0, 0.025, 0.05, 0.075, 0.1, 0.125, Inf)
classes <- cut(inter_times, breaks = breaks, right = TRUE)
plot(classes)
```

La Fig. \@ref(fig:histiid) muestra una alta concentración cerca de cero y una rápida caída, lo cual sería es compatible con una distribución exponencial, aunque hay poca cantidad de datos, lo que limita la evidencia visual.

Para tener mayor rigurosidad,  se podria realizar un __test de Lilliefors__ de ajuste a la distribución exponencial sobre los datos inter-eventos.

En conclusión, en este caso de HLE se puede modelar el conteo de eventos por un PP(20.04).

#### Datos no iid, estacionarios y débilmente dependientes
 

Vayamos ahora a un caso en que los datos no son $iid$ pero son estacionarios y débilmente dependientes, por lo cual veremos si podemos aplicar un modelo PPC.

Esto será posible si los tiempos interventos son $iid$ y con distribución exponencial nuevamente, pero deberíamos también informar sobre $G$, la distribución que indica cuántos se producen simultáneamente.


```{r}
set.seed(123)

# Parámetros
n <- 5000
lambda <- 20
grupo_max <- 4
media_outlier <- 200

# 1. Proceso AR para datos dependientes
phi <- 0.8
X <- arima.sim(model = list(ar = phi), n = n)

# 2. Insertar outliers agrupados en bloques separados
bloques <- split(1:n, cut(1:n, lambda))
outlier_idx <- c()

for (b in bloques) {
  k <- sample(1:grupo_max, 1)
  idxs <- sample(b, k)
  outlier_idx <- c(outlier_idx, idxs)
}

X[outlier_idx] <- rnorm(length(outlier_idx), mean = media_outlier, sd = 300)

# 3. Definir umbral
u_n <- 150
```


```{r datosnoiid, fig.cap = "Datos no iid, estacionarios y débilmente dependientes", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# 4. Graficar señal con umbral
plot(X, type = "h", col = "darkgrey", ylab = "valor", xlab = "tiempo")
abline(h = u_n, col = "red", lty = 2, lwd = 2, bty = "n")
```


El umbral ahora es 150 y se observan algunos  _clusters_ de eventos. Esto quedará en evidencia al
ver los tiempos en que ocurren los eventos, lo cual nos hará nuevamente estudiar la distribución de los tiempos inter-eventos, el orden de los mismos (si son eventos simples, dobles, triples, etc.) y qué podemos decir de su distribución $G$.

```{r echo=TRUE}
# 1. Obtener los tiempos (índices) de excedencia
exceed_idx <- which(X > u_n)
# 2. Reescalar a [0,1] 
rescaled_times <- exceed_idx / n
```


```{r echo=TRUE}
# Calcular tiempos entre eventos
inter_event_times <- diff(rescaled_times)
mean_inter <- mean(inter_event_times)
```



```{r echo=FALSE}
cat("Tiempo promedio inter-evento:", round(mean_inter, 4), "\n")
cat("Estimación parámetro exponencial (1/prom):", round(1 / mean_inter, 2), "\n")
```



Ahora, calculamos el orden de los eventos es decir, el tamaño del clúster.

```{r echo=TRUE}
# 6. Detectar clústers (eventos con separación < 0.02)
delta <- 0.02
clust_id <- rep(1, length(rescaled_times))
for (i in 2:length(rescaled_times)) {
  if ((rescaled_times[i] - rescaled_times[i - 1]) < delta) {
    clust_id[i] <- clust_id[i - 1]
  } else {
    clust_id[i] <- clust_id[i - 1] + 1
  }
}

# 7. Calcular orden de cada evento
orden <- as.numeric(table(clust_id))
orden_evento <- rep(orden, orden)

```
\newpage

```{r echo=FALSE}
# 8. Mostrar salida como tabla
df <- data.frame(
  T = round(rescaled_times, 3),
  Event = 1:length(rescaled_times),
  Orden = orden_evento
)
print("Tiempos de ocurrencia y orden de eventos:")
print(df)

# 9. Tiempos inter-eventos
cat("\nTiempos inter-eventos:\n")
print(round(inter_event_times, 3))
```


```{r echo=FALSE}
runs.test(inter_event_times)
```
No se rechaza la hipótesis nula de aleatoriedad ($p-value>0.05=\alpha$).
Es decir, no hay evidencia suficiente para afirmar que los tiempos inter-eventos no sean aleatorios.




```{r hlenoiidacf, fig.cap = "ACF de los tiempos inter-eventos", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# ACF
acf(inter_event_times, main="")
```




Hipótesis del test de Kolmogorov–Smirnov para exponencialidad: 

- $H_0)$ los datos siguen una distribución exponencial con parámetro estimado $\hat{\lambda} = \frac{1}{\bar{X}}$.
- $H_1)$ los datos no siguen una distribución exponencial.



```{r warning=FALSE}
lambda_hat <- 1 / mean(inter_event_times)
ks.test(inter_event_times, "pexp", rate = lambda_hat)
```
Como $p-value > 0.05$, no se rechaza la hipótesis nula.

Conclusión: los datos de tiempos inter-evento son compatibles con una distribución exponencial (al menos según este test). Entonces, con estos datos estaríamos en codiciones de aplicar conteo de eventos con un modelo $PPC$.




Moralejas:

1) Los datos originales o bien no son estacionarios, o bien no son débilmente dependientes (o ni lo uno ni lo otro).

2) Es natural preguntarse si no hay modelos de HLE más generales.



#### Modelo HLE basado en mezclas de procesos puntuales de conteo (PPC)

Un análisis de datos de polución por ozono en la región parisina reveló una marcada influencia de covariables climáticas. Estas covariables, además de ser no estacionarias y presentar una estructura compleja de dependencia, demostraron tener un alto valor predictivo. Por lo tanto, su incorporación al modelo resultaba fundamental.

En este contexto, se desarrolló un modelo HLE basado en mezclas de procesos puntuales de conteo (PPC), una herramienta ampliamente utilizada en estadística moderna. Se consideró un proceso $Y$ que en cada día registra los valores de un conjunto de covariables relevantes (como distintas medidas de temperatura).

Se buscó un modelo en el cual, dada una trayectoria completa del proceso de covariables, el número de eventos extremos de polución pudiera aproximarse mediante un PPC condicional. Es decir, se modeló la variable $N_n|Y=y$ como un proceso puntual de conteo cuya estructura depende de la trayectoria $y$. 

Como consecuencia, $N_n$ se aproxima a una mezcla de PPC, de acuerdo con las probabilidades de las distintas trayectorias posibles (lo que se corresponde con la fórmula de descondicionamiento). En el caso en que el proceso $Y$ sea débilmente dependiente y estacionario, la mezcla se reduce a un PPC clásico.

Sin embargo, se presentaron ejemplos teóricos y empíricos donde el proceso de covariables no es ni estacionario ni débilmente dependiente, y en los que se obtiene naturalmente una mezcla de varios PPC.

Los fundamentos teóricos del método, así como su demostración de aplicabilidad, se encuentran desarrollados en el trabajo @bellanger2003, incluido en la bibliografía del curso. Esta metodología resulta especialmente recomendable cuando se dispone de un volumen considerable de datos y hay evidencia de que el enfoque clásico basado en PPC no es adecuado.




<!--
::: {.remark}
Esta es una observación.
:::
-->

#### Proceso Puntual de Poisson No Homogéneo (PPNH)

__Observación 8:__ Brindaremos alguna información adicional sobre los $PPNH(m)$. Supondremos que
existe una función __no constante__ $\lambda(x)≥0$ para todo $x>0$, continua por derecha tal que

\begin{equation}
m(A)=\int_A \lambda (x)\;dx
\end{equation}

y supondremos que la integral de la derecha es finita cuando \( A \) es un conjunto acotado, y que, por otro lado, la integral tiende a infinito cuando \( A \) es el conjunto de todos los reales positivos. 


Observar que, si la función \( \lambda \) fuera constante, se obtendría un proceso puntual de Poisson homogéneo (PPH), motivo por el cual esa posibilidad se excluye explícitamente.



Bajo las hipótesis anteriores, veremos que los \emph{tiempos inter-evento} no solo no siguen una distribución exponencial (como ya se demostró), sino que tampoco pueden considerarse independientes ni idénticamente distribuidos. Esta situación puede generar diversas dudas: ¿cómo simular un proceso de este tipo?, ¿cómo verificar si se ajusta a un conjunto de datos?, ¿cómo estimar la medida \( m \) (o la función de intensidad \( \lambda \)) a partir de observaciones?, entre otras. En lo que sigue, intentaremos abordar estas cuestiones.

De manera intuitiva, la función \( \lambda(t) \) representa la \emph{intensidad del evento en el instante \( t \)}.



Por tanto, un modelo razonable en muchos fenómenos climáticos consiste en considerar una función \( \lambda \) \emph{constante a trozos}, que alterna entre cuatro valores fijos: uno correspondiente a todos los días del verano, otro para el otoño, otro para el invierno y otro para la primavera.

Para ilustrarlo con mayor claridad, supongamos que la unidad de tiempo es el día, y que se dispone de datos durante 8 años. En ese caso, se puede graficar una función \( \lambda \) que tome valores distintos en función de la estación del año correspondiente a cada día (Fig. \@ref(fig:trozos)).



```{r trozos, fig.cap = "Lambda diario 2008–2015", echo=FALSE, fig.height=3, fig.width=6, message=FALSE, warning=FALSE}
# Parámetros
anios <- 2008:2015
dias_por_anio <- 365
total_dias <- length(anios) * dias_por_anio

# Crear vector de días
dia <- 1:total_dias

# Crear lambda constante a trozos por estaciones
lambda <- rep(NA, total_dias)

# Valores por estación (invierno, primavera, verano, otoño)
valores_estacion <- c(1, 4, 8, 6)

# Asignar valores por día
for (i in seq_along(anios)) {
  base <- (i - 1) * dias_por_anio
  lambda[base + 1 : 90]     <- valores_estacion[1]  # Invierno
  lambda[base + 91 : 181]   <- valores_estacion[2]  # Primavera
  lambda[base + 182 : 273]  <- valores_estacion[3]  # Verano
  lambda[base + 274 : 365]  <- valores_estacion[4]  # Otoño
}

# Graficar
plot(dia, lambda, type = "s", lwd = 2,
     xlab = "día", ylab = "valor")
```


En la Fig. \@ref(fig:trozos), la intensidad de ocurrencia de eventos extremos (por ejemplo, de polución) cambia según la estación del año. En cada estación, $\lambda(t)$ se mantiene constante, reflejando que el comportamiento del proceso se estabiliza en ese período. Pero como $\lambda(t)$ no es constante en el tiempo, el proceso es no homogéneo. Por último, se asume que el patrón estacional se repite cada año, es decir, hay periodicidad anual.



Ahora, si se dispone de los datos observados durante esos 8 años (no graficados aquí), es claro cómo estimar el valor de \( \lambda \) correspondiente a cada estación. La submuestra de eventos correspondiente a una estación dada puede modelarse, de manera razonable, como un proceso de Poisson homogéneo, cuyo parámetro es precisamente el valor de \( \lambda \) en esa estación. 

Dado que conocemos la forma de estimar el parámetro de un proceso de Poisson homogéneo, podemos aplicar ese conocimiento para obtener una estimación de \( \lambda \) para cada estación del año. En particular, si \( n \) es el número de eventos observados en una estación y \( T \) es la cantidad total de días de esa estación a lo largo del período de observación, entonces:
\[
\hat{\lambda} = \frac{n}{T}
\]
proporciona una estimación natural del valor de la función de intensidad durante dicha estación.

<!--
Pero además, tenemos una forma de chequear que el modelo $PPNH(m)$ se ajusta a los registros observados, simplemente verificando que las 4
submuestras estacionales se ajustan cada una a un $PP$, cosa que ya sabemos hacer también!
-->

Continuando ahora con los $PPNH$ con las suposiciones adicionales que hicimos sobre la
medida $m$ al principio de esta observación, puede demostrarse fácilmente que la densidad de $T_1$, instante del primer evento es


\begin{equation}
f_1(t)=\lambda(t)\; e^{-\int_0^t \lambda(x)\;dx}
(\#eq:f1)
\end{equation}

o sea,  $f_1(t)$ es la densidad del primer tiempo de ocurrencia del evento, donde $\lambda(t)$ es intensidad instantánea en el tiempo $t$ y $\int_0^t \lambda(x)\;dx$
la acumulación de la intensidad hasta el tiempo $t$. La función $e^{-\int_0^t \lambda(x)\;dx}$ es la función de supervivencia (probabilidad de que no haya ocurrido nada antes de $t$).


Por otro lado, puede probarse que la densidad de $T_2$, tiempo que transcurre entre el primer evento y el segundo evento es


\begin{equation}
f_2(t)=\int_0^{\infty}\lambda(u+s)\; e^{-\int_u^{u+s} \lambda(x)\;dx} \;f_1(u) du
(\#eq:f2)
\end{equation}

Puede probarse que $f_1(t)=f_2(t)$ solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO tienen idéntica distribución.

Además, la distribución conjunta de dichos dos primeros tiempos inter-eventos es

\begin{equation}
f_{1,2}(t,s)=\lambda(t+s)\;e^{-\int_t^{t+s} \lambda(x)\;dx}\;f_1(t)
(\#eq:f12)
\end{equation}


Puede probarse que $f_1(t)\:f_2(s)=f_{1,2}(t,s)$ para todo par $t>0$, $s>0$, solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO son independientes.

Quedando claro las complejidades de los $PPNH$, puede entonces preguntarse cómo se hace para
simular un $PPNH(m)$, con $m$ como supusimos al principio de la observación.
Un tal método es bastante sencillo de implementar, como veremos en lo siguiente.

Dividamos todo el eje del tiempo en intervalitos sucesivos de longitud $h$, llamémosle $\Delta_1, \Delta_2,\cdots$ y llamemos $t_j$ al extremo izquierdo de $\Delta_j$ . Simulemos
entonces para cada $j$, $N_j$, un $PP(\lambda(t_j)\;h)$, de forma tal que los distintos $PP$ sean independientes entre sí, y definamos

\begin{equation}
N(A)=\sum_j N_j(A),\quad \forall\; A
(\#eq:na)
\end{equation}

Si $h$ es muy pequeño entonces $N$ se aproxima a un $PPNH(m)$ y por lo tanto $N$ es lo que se simula.

Finalmente, uno podría desear describir un proceso de eventos con la complejidad de un $PPNH$
pero que además permita eventos múltiples, la respuesta es obviamente un Proceso de Poisson No
Homegéneo Compuesto de medida temporal $m$ y distribución de multiplicidad $G$ ($PPNHC(m;G)$),
donde un $PPNH(m)$ indica dónde se producen los eventos, e, independientemente, el orden de
multiplicidad de los eventos siguen la distribución $G$ (en los naturales).

El siguiente punto, por su relevancia práctica, amerita ser destacado.

### Estimación de la intensidad de un $PPNH$

__Observación 9:__ Estimación de la intensidad de un $PPNH$. Aquí veremos dos métodos diferentes, basados en distintos niveles de información previa y acceso a datos.
Concretamente:

a) Estimación paramétrica basada en una sola trayectoria del $PPNH$.

b) Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.

Comenzaremos con el primer abordaje, basado en el método de máxima verosimilitud.

__a) Estimación paramétrica basada en una sola trayectoria del $PPNH$__



Supondremos que observamos el $PPNH$ $N$ durante tanto tiempo como sea necesario, concretamente
que observamos $N$ sobre $[0,n)$, con $n$ muy grande. Asumiremos que $N$ tiene intensidad $\lambda$ continua. Hasta aquí, no hemos supuesto casi nada, por lo cual el abordaje sería muy general. 

En este punto introducimos una suposición bastante restrictiva: asumiremos que sabemos que la intensidad pertenece a una familia paramétrica, que depende de $d$ parámetros reales para especificarse por completo. 

Escribiremos entonces que la intensidad es $\lambda_{\theta}$, donde $\theta$ es un parámetro $d$-dimensional, cuyo verdadero valor deseamos estimar de la forma más precisa posible. Para ello recurriremos al método de máxima verosimilitud.

Supongamos que en la observación disponible del $PPNH$ tenemos que 
$$N([i,i+1))=n(i), \;\text{para} \; i=0,1,\dots,\;n-1,$$
donde los $n(i)$ son los valores concretos observados en cada intervalo. 

En máxima verosimilitud $\theta$ es estimado por $\theta_n$, valor que maximiza la función de $\theta$

\begin{equation}
L(\theta)= P(N([i, i+1))=n(i),\quad \text{para}\; i=0,1,….,n-1
(\#eq:ltheta)
\end{equation}


Usando la independencia de intervalos disjuntos, la distribución de Poisson en cada intervalo,
descartando factores que no dependen de $\theta$ y tomando la función estrictamente creciente $\log$, resulta que $\theta_n$ maximiza la siguiente función

\begin{equation}
l(\theta)=\left\{ \sum_{i=0}^{n-1} n(i)\; \log \left( \int_{[i,i+1)} \lambda_{\theta} (x) \;dx \right)\right\}-\left\{ \int_{[0,n)} \lambda_{\theta} (x) \;dx \right\}
(\#eq:lntheta)
\end{equation}

Asumiendo que $\lambda_{\theta}$ tiene dos derivadas continuas respecto a $\theta$, esta maximización puede ser realizada computacionalmente [@daley2003].




<!-- 
Cuando trabajamos con fenómenos donde los eventos ocurren en el tiempo de forma aleatoria pero con una frecuencia que puede variar, un modelo natural es el \textbf{Proceso de Poisson No Homogéneo} (PPNH). Este modelo generaliza el clásico proceso de Poisson, permitiendo que la \emph{intensidad de ocurrencia} —es decir, la frecuencia esperada de eventos por unidad de tiempo— sea una función del tiempo, en lugar de una constante.

Supongamos entonces que observamos un PPNH \( N \) a lo largo de un intervalo temporal largo, concretamente \( [0, n) \), donde \( n \) es suficientemente grande. Supondremos también que el proceso tiene una intensidad \( \lambda(t) \), función continua del tiempo. Hasta aquí, el planteo es muy general: sólo estamos asumiendo que existe una función de intensidad que regula la aparición de los eventos.

Sin embargo, para poder estimar esa intensidad, vamos a introducir una hipótesis adicional: supondremos que \( \lambda(t) \) pertenece a una \emph{familia paramétrica conocida}, que depende de un número finito de parámetros reales. Escribiremos esta familia como \( \lambda_{\theta}(t) \), donde \( \theta \in \mathbb{R}^d \) es el vector de parámetros desconocidos que queremos estimar.

Una herramienta clásica para estimar parámetros en estadística es el \emph{método de máxima verosimilitud}. La idea es encontrar el valor de \( \theta \) que hace que los datos observados sean lo más “probables” posible bajo el modelo \( \lambda_{\theta} \). Para eso, debemos construir la función de verosimilitud.

#### Dos enfoques para construir la verosimilitud

Existen dos formas habituales de abordar la construcción de esta función de verosimilitud, que dependen de cómo se presenta la información observada:

__1. Usando los tiempos exactos de ocurrencia de los eventos:__

Si contamos con los tiempos exactos en los que ocurrieron los eventos (por ejemplo, los momentos precisos en que se superó un umbral de contaminación), y esos tiempos son \( x_1, x_2, \dots, x_m \) dentro del intervalo \( [0, n) \), entonces la verosimilitud tiene una forma elegante y bien conocida:

\[
L(\theta) = \exp\left( -\int_{0}^{n} \lambda_{\theta}(x)\, dx \right) \cdot \prod_{j=1}^{m} \lambda_{\theta}(x_j)
\]

Esta expresión tiene una interpretación clara: el primer factor representa la probabilidad de no observar eventos fuera de los tiempos registrados, y el segundo, la probabilidad de haber observado eventos en los tiempos registrados. Esta formulación es clásica y puede encontrarse, por ejemplo, en \cite{coles2001introduction} y en literatura sobre procesos puntuales.

__2. Usando cuentas por intervalos disjuntos:__

En muchos contextos prácticos —por ejemplo, cuando los datos se recolectan de forma agregada por día o por hora— no se observan los tiempos individuales, sino sólo la cantidad de eventos ocurridos en intervalos. En ese caso, dividimos \( [0,n) \) en subintervalos de longitud unitaria: \( [0,1), [1,2), \dots, [n-1, n) \), y observamos las cuentas \( N([i,i+1)) = n(i) \) en cada uno de ellos.

Gracias a la independencia de los incrementos del proceso de Poisson, y al hecho de que \( N([i,i+1)) \sim \text{Poisson}(\mu_i) \) con \( \mu_i = \int_{i}^{i+1} \lambda_{\theta}(x)\, dx \), la función de verosimilitud se escribe como:

\[
L(\theta) = \prod_{i=0}^{n-1} \frac{\mu_i^{n(i)} e^{-\mu_i}}{n(i)!}
\]

Despreciando constantes que no dependen de \( \theta \) (como los factoriales), y tomando logaritmos para facilitar la maximización, obtenemos la \emph{log-verosimilitud}:

\begin{equation}
l(\theta)= \sum_{i=0}^{n-1} n(i)\; \log \left( \int_{i}^{i+1} \lambda_{\theta}(x)\, dx \right) - \int_{0}^{n} \lambda_{\theta}(x)\, dx
(\#eq:lntheta)
\end{equation}

Este es el enfoque que desarrollamos en esta sección, especialmente útil cuando los datos están agrupados en bloques.

__Equivalencia conceptual y uso práctico:__ Ambas formulaciones de la verosimilitud —la continua basada en los tiempos individuales, y la discreta basada en cuentas por intervalos— se apoyan en la misma estructura teórica: el proceso de Poisson con independencia en subconjuntos disjuntos del dominio. Cuando los intervalos son muy pequeños (por ejemplo, de longitud infinitesimal), ambas expresiones tienden a coincidir.

En resumen:
\begin{itemize}
  \item Si se observan los \emph{tiempos exactos} de los eventos, se recomienda usar la formulación continua.
  \item Si se observan \emph{cuentas agregadas}, se utiliza la formulación discreta, que lleva naturalmente a la expresión \@ref(eq:lntheta).
\end{itemize}

En ambos casos, se puede obtener una estimación de \( \theta \) —denotada \( \hat{\theta}_n \)— maximizando la verosimilitud con métodos numéricos, siempre que \( \lambda_{\theta} \) sea lo suficientemente regular (por ejemplo, dos veces diferenciable respecto a \( \theta \)).

-->




__b. Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.__

En este abordaje no supondremos un conocimiento previo de forma paramétrica alguna de la intensidad, por lo cual desde este punto de vista es un método más general que el anterior. En sentido contrario, supondremos que tenemos una cantidad $n$ muy grande de __réplicas__ de $N$. Esto significa que disponemos de trayectorias de $N(1),\dots,N(n)$ donde cada $N(i)$ es un $PPNH$ con la misma estructura (en particular misma intensidad, que $N$) y que $N(1),\dots,N(n)$ son independientes entre sí. 

El método a utilizar está basado en la estimación de densidades a partir de un núcleo (kernel). Antes de presentar el método puntualicemos que si $N$ es un $PPNH$ se pueden definir integrales (estocásticas, aleatorias) del tipo $\int_J f(x)\;dN(x)$ para $f$ continua en el intervalo $J$, como límite de sumas a la Riemann $\sum_{i=0}^{n-1}f(x(i)) \cdot N(\Delta(i))$ donde $\Delta(1),\dots,\Delta(n)$ es la partición del intervalo $J$ en intervalos muy pequeños y $x(i)$ punto intermedio de $\Delta(i)$.

El estimador que consideraremos es 

\begin{equation*}
\lambda_n(x) = \left\{ \frac{1}{n h(n)} \right\} \sum_{i=0}^{n-1} \int_{\mathbb{R}} K\left( \frac{x - t}{h(n)} \right) dN^{(i)}(t),
\end{equation*}

donde, para $n$ tendiendo a infinito, la ventana $h(n)$ tiende a $0$ y $n h(n)$ tiende a infinito, y donde la función $K$ (kernel) debe ser no-negativa, par, nula fuera del intervalo $[-1,1]$ (lo cual implica que las integrales estocásticas no son sobre $\mathbb{R}$, sino sobre un intervalo acotado) y con derivadas de todos los órdenes.


En tales condiciones $\lim_{n \to 0} \lambda_n(x)=\lambda(x),\; \forall x$.

Si además se supone que $\lim_{n \to 0} \;n \cdot  h(n)^5=0$ (un ejemplo de $h(n)$ que
cumple con todas las condiciones impuestas es $h(N)= 1/nb$, con $1/5<b<1$), entonces cuando $n$ tiende
a infinito, la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{\lambda_n(x)-\lambda (x)  \right\}$ tiende a $N(0,\lambda (x))$.

Usando la fórmula de Taylor se deduce que si $g$ tiene dos derivadas continuas en un entorno de $\lambda (x)$, entonces la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{g\left( \lambda_n(x) \right)-g\left( \lambda (x) \right)  \right\}$ tiende a una $N\left( 0,g^{\prime}\left( \lambda(x) \right)^2\;\lambda(x) \right)$. 

Tomando entonces $g(x)=2\sqrt{x}$, resulta entonces que la distribución de 

$$2\;\left\{ n \cdot h(n)) \right\}^{1/2} \left\{ \sqrt{\lambda_n(x)} -\sqrt{\lambda(x)}\right\}\; \text{tiende a una }\;N(0,1).$$

De aquí se deduce un Intervalo al $95\%$ de confianza para $\sqrt{\lambda(x)}$ y por lo tanto, el siguiente
Intervalo al $95\%$ de Confianza para $\lambda(x)$ tal que $\left[ A(x), B(x) \right]$ donde

\begin{align}
&A(x)=\lambda_n(x)-1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)},\\
&B(x)=\lambda_n(x)+1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)}.
\end{align}




<!--
```{r echo=FALSE, warning=TRUE}
# Parámetros
n <- 5000                # Número de observaciones
set.seed(123)            # Para reproducibilidad

# Simulación de datos iid de una distribución Pareto (cola pesada)
# Parámetro de forma (alpha > 0) controla la pesadez de la cola
alpha <- 2.5
x <- (runif(n))^(-1/alpha)

# Reescalado del tiempo a [0,1]
tiempo <- seq(1, n) / n

# Gráfico: Datos vs. Tiempo reescalado
plot(tiempo, x, type = 'l', col = 'blue', lwd = 1.5,
     xlab = 'Tiempo (reescalado en [0,1])', ylab = 'Valor observado',
     main = 'Ejemplo de Datos iid con Cola Pesada (Pareto)')

# Línea horizontal roja en y = 10
abline(h = 10, col = 'red', lwd = 2, lty = 2)

# Leyenda opcional para aclarar la línea roja
legend("topright", legend = "Umbral = 10",
       col = "red", lty = 2, lwd = 2, bty = "n")
```


```{r echo=FALSE}
# Definir el umbral
umbral <- 10

# Identificar los índices donde x excede el umbral
excesos_idx <- which(x > umbral)

# Valores excedentes y tiempos reescalados
excesos <- x[excesos_idx]
tiempos_excesos <- tiempo[excesos_idx]

# Calcular los tiempos inter-eventos (diferencias entre tiempos consecutivos)
tiempos_intereventos <- diff(tiempos_excesos)

# Crear tabla: para el último exceso no hay tiempo inter-evento (se coloca NA)
tabla_excesos <- data.frame(
  Exceso = excesos,
  Tiempo = tiempos_excesos,
  Interevento = c(tiempos_intereventos, NA)  # NA para la última fila
)

# Mostrar tabla
head(tabla_excesos, 10)  # Mostramos las 10 primeras filas
```

```{r echo=FALSE}
promedio_intereventos <- mean(tiempos_intereventos)
cat("Promedio intereventos:", promedio_intereventos, "\n")
```



Se pone a prueba: $H_0)\;\text{Los tiempos inter-eventos son independientes.}$ contra

$H_1)\;\text{Los tiempos inter-eventos son independientes.}$


```{r}
# 1. Test de Ljung-Box (autocorrelación global)
ljung <- Box.test(tiempos_intereventos, lag = 10, type = "Ljung-Box")

# 2. Runs Test (binarizando respecto a la mediana)
library(tseries)

# Crear secuencia binaria: 1 si > mediana, 0 si <= mediana
mediana_intereventos <- median(tiempos_intereventos)
secuencia_binaria <- ifelse(tiempos_intereventos > mediana_intereventos, 1, 0)

# Aplicar Runs Test a la secuencia binaria
runs <- runs.test(as.factor(secuencia_binaria))

# 4. Mostrar resultados
cat("Test de Ljung-Box: p-valor =", round(ljung$p.value, 4), "\n")
cat("Runs Test: p-valor =", round(runs$p.value, 4), "\n")
```

Se aplicaron pruebas de independencia a los tiempos inter-eventos obtenidos a partir de los excesos sobre un umbral de 10 en datos simulados de Pareto.
El Test de Ljung-Box arrojó un $p-valor = 0.9954$, indicando ausencia de autocorrelación significativa. Asimismo, el Runs Test obtuvo un 
$p-valor = 0.3215$, compatible con la hipótesis de aleatoriedad en la secuencia de tiempos inter-eventos. Estos resultados son consistentes con la generación de datos iid, y validan la hipótesis de independencia en este contexto.

Conclusión: Ambos tests no rechazan la hipótesis nula de independencia, no hay señales de dependencia temporal.

```{r echo=FALSE}
# 3. ACF visual
acf(tiempos_intereventos, main = "ACF de los tiempos inter-eventos (excesos > 10)")
```

El ACF mide la correlación entre los tiempos inter-eventos separados por diferentes lags (retardos).

Si los datos son independientes, la ACF debería oscilar cerca de 0 para todos los lags.

Se dibujan bandas azules (intervalos de confianza): si los valores están dentro → no hay autocorrelación significativa


----------
IMPORTANTE:
p. 75-89 de las notas de clase: No entendi hasta Observación 8.
----------

__Observación 8:__ Brindaremos alguna información adicional sobre los $PPNH(m)$. Supondremos que existe una función NO CONSTANTE $\lambda(x)  \geq 0$ para
todo $x>0$, continua por derecha tal que

\begin{equation}
m(A)=\int_A \lambda(x)\;dx
(\#eq:ma)
\end{equation}

y supondremos que la integral de la derecha en Eq. \@ref(eq:ma) es
finita cuando $A$ es acotado y que vale infinito cuando $A$ es el conjunto de todos los reales positivos.

Observar que si la función fuera constante se obtendría un $PP$, por eso se excluye explícitamente. Bajo las hipótesis anteriores veremos que los tiempos inter-eventos no sólo no son exponenciales (como probamos) sino que tampoco pueden ser ni
independientes, ni idénticamente distribuídos. Eso puede plantear dudas sobre cómo se simula un tal proceso, como se comprueba si se ajusta a datos
dados, como estimar la medida $m$ o la función $\lambda$ a partir de los datos, etc. Intentaremos aclarar estas inquietudes.


Intuitivamente $\lambda(t)$ representa la intensidad del
evento en el instante $t$. Por ende, un modelo sumamente razonable en
muchos eventos relacionados al clima es el anterior
con $\lambda$ una función constante a trozos, que va
tomando alternadamente 4 valores: uno para todos
los instantes del verano, otro para todos los
instantes del otoño, otro para todos los del invierno
y otros para todos los de la primavera.

Para entender más claramente, supongamos que la
unidad de tiempo es el día y que tenemos los datos
de 8 años, y veamos la gráfica de una tal función $\lambda$.


```{r echo=FALSE, warning=TRUE}
# Parámetros
dias_por_anio <- 365
anios <- 8
total_dias <- dias_por_anio * anios

# Eje de días
dias <- 1:total_dias

# Definir los valores de λ por estación (por año)
lambda_verano <- rep(10, 90)
lambda_otono  <- rep(5, 90)
lambda_invierno <- rep(2, 90)
lambda_primavera <- rep(8, 95)  # para completar los 365 días

# Un año completo de lambda
lambda_anio <- c(lambda_verano, lambda_otono, lambda_invierno, lambda_primavera)

# Repetir por 8 años
lambda_total <- rep(lambda_anio, anios)

# Graficar
plot(dias, lambda_total, type = "s", col = "blue", lwd = 2,
     xlab = "Día", ylab = expression(lambda(t)),
     main = expression("Intensidad " * lambda(t) * " por estación (8 años)"))
grid()
```

Si se tienen los datos de los registros observados durante esos 8 años (no graficados), es evidente cómo estimar el valor de cada estación: la submuestra de datos correspondiente a una estación dada se uede mostrara fácilmente que es un simple $PP$ cuyo parámetro es el valor de $\lambda$ correspondiente a dicha estación. Como ya
sabemos estimar el parámetro de un PP, sabemos cómo estimar los valores de $\lambda$ de cada estación.

Pero además, tenemos una forma de chequear que el modelo $PPNH(m)$ se ajusta a los registros observados, simplemente verificando que las 4
submuestras estacionales se ajustan cada una a un $PP$, cosa que ya sabemos hacer también!

Continuando ahora con los $PPNH$ con las suposiciones adicionales que hicimos sobre la
medida $m$ al principio de esta observación, puede demostrarse fácilmente que la densidad de $T_1$, instante del primer evento es


\begin{equation}
f_1(t)=\lambda(t)\; e^{-\int_0^t \lambda(x)\;dx}
(\#eq:f1)
\end{equation}

o sea,  $f_1(t)$ es la densidad del primer tiempo de ocurrencia del evento, donde $\lambda(t)$ es intensidad instantánea en el tiempo $t$ y $\int_0^t \lambda(x)\;dx$
la acumulación de la intensidad hasta el tiempo $t$. La función $e^{-\int_0^t \lambda(x)\;dx}$ es la función de supervivencia (probabilidad de que no haya ocurrido nada antes de $t$).


Por otro lado, puede probarse que la densidad de $T_2$, tiempo que transcurre entre el primer evento y el segundo evento es


\begin{equation}
f_2(t)=\int_0^{\infty}\lambda(u+s)\; e^{-\int_u^{u+s} \lambda(x)\;dx} \;f_1(u) du
(\#eq:f2)
\end{equation}

Puede probarse que $f_1(t)=f_2(t)$ solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO tienen idéntica distribución.

Además, la distribución conjunta de dichos dos primeros tiempos inter-eventos es

\begin{equation}
f_{1,2}(t,s)=\lambda(t+s)\;e^{-\int_t^{t+s} \lambda(x)\;dx}\;f_1(t)
(\#eq:f12)
\end{equation}


Puede probarse que $f_1(t)\:f_2(s)=f_{1,2}(t,s)$ para todo par $t>0$, $s>0$, solamente cuando la función $\lambda$ es constante, por lo cual en un $PPNH$ los dos primeros tiempos inter-eventos NO son independientes.

Quedando claro las complejidades de los $PPNH$, puede entonces preguntarse cómo se hace para
simular un $PPNH(m)$, con $m$ como supusimos al principio de la observación.
Un tal método es bastante sencillo de implementar, como veremos en lo siguiente.

Dividamos todo el eje del tiempo en intervalitos sucesivos de longitud $h$, llamémosle $\Delta_1, \Delta_2,\cdots$ y llamemos $t_j$ al extremo izquierdo de $\Delta_j$ . Simulemos
entonces para cada $j$, $N_j$, un $PP(\lambda(t_j)\;h)$, de forma tal que los distintos $PP$ sean independientes entre sí, y definamos

\begin{equation}
N(A)=\sum_j N_j(A),\quad \forall\; A
(\#eq:na)
\end{equation}

Si $h$ es muy pequeño entonces $N$ se aproxima a un $PPNH(m)$ y por lo tanto $N$ es lo que se simula.

Finalmente, uno podría desear describir un proceso de eventos con la complejidad de un $PPNH$
pero que además permita eventos múltiples, la respuesta es obviamente un Proceso de Poisson No
Homegéneo Compuesto de medida temporal $m$ y distribución de multiplicidad $G$ ($PPNHC(m;G)$),
donde un $PPNH(m)$ indica dónde se producen los eventos, e, independientemente, el orden de
multiplicidad de los eventos siguen la distribución $G$ (en los naturales).

El siguiente punto, por su relevancia práctica, amerita ser destacado especialmente.

__Observación 9:__ Estimación de la intensidad de un $PPNH$. Aquí veremos dos métodos diferentes, basados en distintos niveles de información previa y acceso a datos.
Concretamente:

a) Estimación paramétrica basada en una sola trayectoria del $PPNH$.

b) Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.

Comenzaremos con el primer abordaje, basado en el método de máxima verosimilitud.

__a. Estimación basada en una sola trayectoria del $PPNH$.__

Supondremos que observamos el $PPNH$ $N$ durante tanto tiempo como sea necesario, concretamente
que observamos $N$ sobre $[0,n)$, con $n$ muy grande. Asumiremos que $N$ tiene intensidad $\lambda$ continua. Hasta aquí, no hemos supuesto casi nada, por lo cual el abordaje sería muy general. En este punto introducimos una suposición bastante restrictiva: asumiremos que sabemos que la intensidad pertenece a una familia paramétrica, que depende
de $d$ parámetros reales para especificarse por completo. Escribiremos entonces que la intensidad es $\lambda_{\theta}$, donde $\theta$ es un parámetro $d$-dimensional, cuyo verdadero valor deseamos estimar de la forma más precisa posible. Para ello recurriremos al método de máxima verosimilitud.

Supongamos que en la observación disponible del $PPNH$ tenemos que $N([i,i+1))=n(i)$ para $i=0,1,\dots,n-1$. Los $n(i)$ son los valores concretos observados en cada intervalo. 

En máxima verosimilitud $\theta$ es estimado por $\theta_n$, valor que maximiza la función de $\theta$

\begin{equation}
L(\theta)= P(N([i, i+1))=n(i),\quad \text{para}\; i=0,1,….,n-1
(\#eq:ltheta)
\end{equation}


Usando la independencia de intervalos disjuntos, la distribución de Poisson en cada intervalo,
descartando factores que no dependen de $\theta$ y tomando la función estrictamente creciente $\log$, resulta que $\theta_n$ maximiza la siguiente función

\begin{equation}
l(\theta)=\left\{ \sum_{i=0}^{n-1} n(i)\; \log \left( \int_{[i,i+1)} \lambda_{\theta} (x) \;dx \right)\right\}-\left\{ \int_{[0,n)} \lambda_{\theta} (x) \;dx \right\}
(\#eq:lntheta)
\end{equation}

Asumiendo que $\lambda_{\theta}$ tiene dos derivadas continuas respecto a $\theta$, esta maximización puede ser realizada computacionalmente.

__b. Estimación no-paramétrica a partir de muchas réplicas del $PPNH$.__

En este abordaje no supondremos el conocimiento previo de forma paramétrica alguna de la intensidad, por lo cual desde este punto de vista es un método más general que el anterior. En sentido contrario, supondremos que tenemos una cantidad $n$ muy grande de 'réplicas' de $N$. Esto significa que disponemos de trayectorias de $N(1),\dots,N(n)$ donde cada $N(i)$ es un $PPNH$ con la misma estructura (en particular misma intensidad, que $N$) y que $N(1),\dots,N(n)$ son independientes entre sí. El método a utilizar está basado en la estimación de densidades a partir de un núcleo (kernel). Antes de presentar el método puntualicemos que si $N$ es un $PPNH$ se pueden definir integrales (estocásticas, aleatorias) del tipo $\int_J f(x)\;dN(x)$ para $f$ continua en el intervalo $J$, como límite de sumas a la Riemann $\sum_{i=0}^{n-1}f(x(i)) \cdot N(\Delta(i))$ donde $\Delta(1),\dots,\Delta(n)$ es la partición del intervalo $J$ en intervalos muy pequeños y $x(i)$ punto intermedio de $\Delta(i)$.

El estimador que consideraremos es 

\begin{equation*}
\lambda_n(x) = \left\{ \frac{1}{n h(n)} \right\} \sum_{i=0}^{n-1} \int_{\mathbb{R}} K\left( \frac{x - t}{h(n)} \right) dN^{(i)}(t),
\end{equation*}

donde, para $n$ tendiendo a infinito, la 'ventana' $h(n)$ tiende a $0$ y $n h(n)$ tiende a infinito, y donde la función $K$ (kernel) debe ser no-negativa, par, nula fuera del intervalo $[-1,1]$ (lo cual implica que las integrales estocásticas no son sobre $\mathbb{R}$, sino sobre un intervalo acotado) y con derivadas de todos los órdenes.


En tales condiciones $\lim_{n \to 0} \lambda_n(x)=\lambda(x),\; \forall x$.

Si además se supone que $\lim_{n \to 0} \;n \cdot  h(n)^5=0$ (un ejemplo de $h(n)$ que
cumple con todas las condiciones impuestas es $h(N)= 1/nb$, con $1/5<b<1$), entonces cuando $n$ tiende
a infinito, la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{\lambda_n(x)-\lambda (x)  \right\}$ tiende a $N(0,\lambda (x))$.

Usando la fórmula de Taylor se deduce que si $g$ tiene dos derivadas continuas en un entorno de $\lambda (x)$, entonces la distribución de $\left\{ n\cdot h(h) \right\}^{1/2}\; \left\{g\left( \lambda_n(x) \right)-g\left( \lambda (x) \right)  \right\}$ tiende a una $N\left( 0,g^{\prime}\left( \lambda(x) \right)^2\;\lambda(x) \right)$. 

Tomando entonces $g(x)=2\sqrt{x}$, resulta entonces que la distribución de $2\;\left\{ n \cdot h(n)) \right\}^{1/2} \left\{ \sqrt{\lambda_n(x)} \;\sqrt{\lambda(x)}\right\}$ tiende a una $N(0,1)$.

De aquí se deduce un Intervalo al $95\%$ de confianza para $\sqrt{\lambda(x)}$ y por lo tanto, el siguiente
Intervalo al $95\%$ de Confianza para $\lambda(x)$ tal que $\left[ A(x), B(x) \right]$ donde

\begin{align}
&A(x)=\lambda_n(x)-1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)},\\
&B(x)=\lambda_n(x)+1.96\left\{ \frac{\lambda_n(x)}{n\cdot h(n)} \right\}^{1/2}+\frac{0.98^2}{n\cdot h(n)}.
\end{align}
-->

