# La teoría asintótica clásica, las distribuciones extremales y sus dominios de atracción


---
output:
  pdf_document: default
  html_document: default
---

<!-- ## Introducción: las distribuciones extremales {#intro}-->

 

```{r message=FALSE, warning=FALSE, include=FALSE}
library(evd)
library(ggplot2)
```


<!-- ecuaciones, su label y ref
\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).
-->


<!-- 
{.unlisted .unnumbered}. 
para sacar la numeracion a la seccion
-->

## Datos extremos

Se dice que tenemos _datos extremos_ cuando cada dato corresponde al máximo o mínimo de varios
registros. Ejemplos de este tipo de datos son:

- La máxima altura semanal de la ola en una
plataforma marina o portuaria $(m)$.
- La máxima velocidad de viento en determinada
dirección a lo largo de un mes $(km/h)$.
- La temperatura ambiental mínima a lo largo de
un día $(\dot{C})$.
- La temperatura ambiental mínima a lo largo de
un día ($\dot{C}$)
- La máxima velocidad de tráfico en un enlace de
una red de datos de datos en una hora ($Mb/s$).
- El mayor registro en un conteo de Coliformes
fecales sobre agua costeras al cabo de quince días.

Son un caso particular de evento raro o gran desviación respecto a la media. En resumen, para una gran variedad de dominios disciplinares suele ser de gran interés el trabajo con datos extremos, por ejemplo, medioambiente, telecomunicaciones, portafolio y riesgo, entre otros.

El comienzo del curso se centra en la teoría más clásica de estadística de datos extremos, basada en los trabajos de @frechet1927, @gumbel1958statistics, @weibull1951, @fisher1928limiting, @gnedenko1943, entre otros. En este marco, los argumentos que manejamos son de tipo __asintóticos__^[El análisis asintótico es un método de descripción del comportamiento en el límite.] y podríamos decir que nos ubicamos en lo que se conoce en la literatura como _paradigma de los valores extremos_ [@coles2001introduction].



__Observación 1.1__ (Distribución del máximo de dos variables independientes).
Se recuerda que si \( X \) e \( Y \) son variables aleatorias independientes, con funciones de distribución acumulada (cdf) \( F \) y \( G \), respectivamente. Entonces, la función de distribución acumulada de la variable aleatoria

\begin{equation}
Z = \max(X, Y)
\label{eq:defZ}
\end{equation}

es

\begin{equation}
H(t) = \Pr(\max(X, Y) \le t) = \Pr(X \le t, Y \le t) = F(t) \cdot G(t)
(\#eq:HZ)
\end{equation}

donde \( t \in \mathbb{R} \) es un valor real arbitrario.

Esto se debe directamente a la independencia de \( X \) e \( Y \), que implica que

\begin{equation}
\Pr(X \le t, Y \le t) = \mathbb{P}(X \le t) \cdot \Pr(Y \le t).
(\#eq:indep)
\end{equation}


__Observación 2.1__(Datos independientes e idénticamente distribuidos ($iid$)).  En esta parte inicial del curso
asumiremos que nuestros datos son $iid$. Esta doble suposición suele no ser realista en aplicaciones concretas pero
para comenzar a entender la teoría clásica, la utilizaremos por un tiempo.


__Observación 3.1__ Resulta de la Observación 1.1, que si tenemos la secuencia de datos $X_1,\dots,X_n$ que son $iid$ con distribución $F$, entonces

\begin{equation}
X_n^{\ast}= \max \left( X_1,\dots,X_n \right)
\end{equation}

tiene distribución $F_n^\ast$ dada por

\begin{equation}
F_n^\ast (t) = F_n(t)
\end{equation}

Si conocemos la distribución $F$ conoceríamos la distribución $F_n^\ast$, pero en algunos casos la lectura que queda registrada es la del dato máximo, y no la de cada observación que dio lugar al mismo, por lo que a veces ni siquiera es viable estimar $F$. Pero aún en los casos en que $F$ es conocida o estimable, si $n$ es grande, la fórmula de $F_n^\ast$ puede resultar prácticamente inmanejable. En una línea de trabajo similar a la que aporta el Teorema Central del Límite (TCL) [@bertsekas2008introduction] en la estadística de valores medios (ver Apéndice  \@ref(sec:ap)), vamos a ver un teorema que nos va a permitir aproximar $F_n^\ast$ por distribuciones más sencillas. Este es el Teorema de Fischer-Tippet-Gnedenko (FTG) que presentaremos en breve (@fisher1928limiting,  @gnedenko1943).



__Observación 4:__ Si la secuencia de datos $X_1,\dots,X_n\;$ es $iid\;$ y definimos
$\;Y_i = -X_i\;$ para todo valor de $i,\dots,n$, entonces $Y_1,\dots,Y_n\;$ es $iid\;$ y además

\begin{equation}
min(X_1,\dots,X_n) = - max(Y_1,\dots,Y_n)
\end{equation}

la teoría asintótica de los mínimos de datos $iid$ se reduce a la de los máximos, razón por la que
nos concentramos aquí en estudiar el comportamiento asintótico de los máximos exclusivamente. A modo de ejemplo, si suponemos que $R_1, R_2, \dots$ es la secuencia de rendimientos diarios de un índice bursátil. Entonces

\begin{equation}
M_n=\max\left\{ R_1, \dots, R_t\right\}
\end{equation}

es el máximo rendimiento diario observado en un período de “t observaciones”, con $t=1,\dots, T$.


## Teoría de los valores extremos: formulación del modelo

En un marco asintótico, se puede partir de la siguiente formulación. Supongamos que nuestro modelo se basa en el comportamiento estadístico de 

\begin{equation}
M_n=\max\left\{ X_1, \dots, X_n\right\}
\end{equation}

donde $X_1, \dots, X_n$ es la secuencia de variables aleatorias independientes que tienen una distribución común $F$.

En teoría, la distribución de \( M_n \) podría derivarse exactamente para todos los valores de \( n \):
\begin{align}
\Pr\{M_n \leq z\} &= \Pr\{X_1 \leq z, \ldots, X_n \leq z\}\nonumber \\
&= \Pr\{X_1 \leq z\} \times \cdots \times \Pr\{X_n \leq z\} \nonumber \\
&= \left[F(z)\right]^n
(\#eq:Mn)
\end{align}

donde \( F(z) = \Pr\{X_i \leq z\} \) es la función de distribución de cada variable \( X_i \). Sin embargo, en la práctica esto no es útil porque la función de distribución $F$ es desconocida. Por este motivo, una solución consiste en aproximar _familias de modelos_ para $F^n$ que se estiman únicamente en base a datos extremos:

- Vamos a emplear argumentos análogos a los del TCL: estudiamos el comportamiento de $F^n$ cuando $n\rightarrow \infty$. 

- Lo anterior por sí solo no es suficiente: para cualquier \( z < z^+ \), donde \( z^+ \) es el extremo superior^[Sea \( z^+ \) el menor valor de \( z \) tal que \( F(z) = 1 \). O sea, es el valor más bajo a partir del cual la función de distribución ya no crece más. Es el "techo" natural de la variable aleatoria.] de la función de distribución \( F \), se cumple que \( F_n(z) \to 0 \) cuando \( n \to \infty \), lo que implica que la distribución de \( M_n \) degenera en una masa puntual en \( z^+ \).

<!--__¿Qué significa lo anterior?__ Si tomás infinitos datos, el máximo va a ser siempre, con probabilidad 1, \( z^+ \).-->

Lo anterior se puede evitar normalizando la variable

\begin{equation}
M^{\ast}_n=\frac{M_n-b_n}{a_n}.
(\#eq:Mnr)
\end{equation}

<!-- \quad \text{para las secuencias de constantes}\;\left\{  a_n \right\}\;,  \left\{  b_n \right\}.-->
Si elegimos apropiadamente los valores de las secuencias de constantes $\left\{  a_n \right\}$ y $\left\{  b_n \right\}$ es posible estabilizar la escala de $M^{\ast}_n$ a medida que $n$ aumenta. Es por esto, que vamos a buscar las distribuciones límite de $M^{\ast}_n$ [@coles2001introduction]. 




## Las distribuciones extremales


Las distribuciones extremales son tres: la *distribución de Gumbel*, la *distribución de Weibull* y
la *distribución de Fréchet*. En su versión *standard* o *típica* se definen del modo siguiente^[En el Apéndice \@ref(sec:ap) se encuentra el Teorema \@ref(thm:thap1) de los tipos de extremales.].


Bajo ciertas condiciones, \( M_n^* \) tiende, cuando \( n \to \infty \), a una función límite \( G \) que puede pertenecer a una de las siguientes tres familias^[Donde \( x = \frac{z - b}{a} \), y se ha fijado \( b = 0 \) y \( a = 1 \) para expresar las distribuciones en su forma estándar.]:


-__Gumbel__ si su distribución es
 
$$\Lambda(x) = e^{\{-e^{-x}\}}\hspace{0.3cm},\text{ para todo }\: x \;\text{real}$$


-__Weibull__ de orden $\alpha>0$ si su distribución es


$$\Psi_{\alpha}(x)=\begin{cases}
e^{\left\{-(-x)^{\alpha}  \right\}} &, si\;x<0\\
1 &,\text{en otro caso}
\end{cases}$$


-__Fréchet__ de orden $\alpha>0$ si su distribución es

$$
\Phi_{\alpha}(x)=\begin{cases}
e^{\left\{ -x^{-\alpha}\right\}} &, \:si\;x>0\\
0&, \:\text{en otro caso}
\end{cases}
$$


Las familias Fréchet y Weibull tienen un parámetro de forma \( \alpha \).

Como los máximos en general son valores grandes, importa particularmente observar el comportamiento de estas distribuciones cuando \( x \to \infty \). En toda distribución, el límite es 1, pero no todas se acercan a ese valor de la misma manera. La Weibull se aproxima rápidamente, seguida por la Gumbel y luego la Fréchet. Esto indica que la distribución Fréchet modela datos \emph{más extremos}, es decir, valores máximos provenientes de datos con colas más pesadas que la Gumbel, y ésta más pesadas que la Weibull (Figura \@ref(fig:colas)).


```{r colas, fig.cap="CDF extremales" ,echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE}

# Márgenes reducidos
par(mar = c(5, 4, 1, 1))  # abajo, izquierda, arriba, derecha
library(ggplot2)
library(tidyr)

# Rango de x extendido para mostrar Weibull
x <- seq(-20, 100, length.out = 10000)
alpha <- 2

# Funciones CDF
cdf_frechet <- function(x, alpha) ifelse(x > 0, exp(-x^(-alpha)), 0)
cdf_weibull <- function(x, alpha) ifelse(x < 0, exp(-(-x)^alpha), 1)
cdf_gumbel  <- function(x) exp(-exp(-x))

# Evaluar las distribuciones
df <- data.frame(
  x = x,
  Fréchet = cdf_frechet(x, alpha),
  Gumbel = cdf_gumbel(x),
  Weibull = cdf_weibull(x, alpha)
)

# Reorganizar para ggplot
df_long <- pivot_longer(df, cols = -x, names_to = "cdf", values_to = "CDF")

# Gráfico
ggplot(df_long, aes(x = x, y = CDF, color = cdf)) +
  geom_line(size = 1.2) +
  coord_cartesian(xlim = c(-10, 100), ylim = c(0.85, 1)) +
  labs(
    x = "x",
    y = "F(x)"
  ) +
theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.box.margin = margin(t = 10)
  )
```



En la distribución Fréchet, cuanto menor es el parámetro de forma \( \alpha \), más pesada es la cola y más lenta la convergencia a 1 (Figura \@ref(fig:frcolas)). En cambio, la Weibull tiene soporte acotado superior y el parámetro \( \alpha \) afecta principalmente la forma de la cola izquierda^[Porque la Wibull tiene soporte en $(-\infty, 0]$.], hacia \( -\infty \) (Figura \@ref(fig:weicolas)). 

```{r frcolas, fig.cap="Distribución Fréchet para distintos parámetros de forma" ,echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE}
# Valores de x y alphas
x <- seq(0.01, 50, length.out = 10000)
alphas <- c(0.8, 2.5, 5)

# Crear data frame con todas las curvas
df <- lapply(alphas, function(alpha) {
  data.frame(
    x = x,
    CDF = exp(-x^(-alpha)),
    alpha = paste("", alpha)
  )
}) %>% bind_rows()

# Graficar con ggplot
ggplot(df, aes(x = x, y = CDF, color = alpha)) +
  geom_line(size = 1.2) +
  coord_cartesian(ylim = c(0.85, 1)) +
  scale_color_manual(values = c("orange", "forestgreen", "blue")) +
  labs(
    x = "x",
    y = "F(x)",
  ) +
theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.box.margin = margin(t = 20)
  )
```


```{r weicolas, fig.cap="Distribución Weibull para dististos parámetros de forma" ,echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE}
# Dominio negativo
x <- seq(-50, -0.01, length.out = 1000)
alphas <- c(0.4, 1, 5)

# CDF Weibull extremal
cdf_weibull <- function(x, alpha) ifelse(x < 0, exp(-(-x)^alpha), 1)

# Armar data frame para ggplot
df <- lapply(alphas, function(alpha) {
  data.frame(
    x = x,
    CDF = cdf_weibull(x, alpha),
    alpha = paste("", alpha)
  )
}) %>% bind_rows()

# Gráfico
ggplot(df, aes(x = x, y = CDF, color = alpha)) +
  geom_line(size = 1.2) +
  coord_cartesian(ylim = c(0, 1), xlim = c(-50, 0)) +
  scale_color_manual(values = c("orange", "forestgreen", "blue")) +
  labs(
    x = "x",
    y = "F(x)"
  ) +
theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.box.margin = margin(t = 20)
  )
```


\newpage

Estos fenómenos se visualizarán con mayor claridad al presentar el Teorema \@ref(thm:foo1) del curso. Además, observar las funciones de densidad correspondientes puede ayudar a comprender mejor el peso relativo de las colas de cada distribución. En la Figura \@ref(fig:distributions), se presentan las funciones de densidad de probabilidad (PDF) extremales. Las PDF permiten visualizar de forma clara las diferencias en la forma, simetría y comportamiento en las colas^[En el Apéndice \@ref(sec:pdf) se explica la diferencia entre PDF y CDF.].
\vspace{0.5cm}




<!--
Como los máximos en general son valores grandes, importa particularmente observar el comportamiento de estas distribuciones para $x$ tendiendo a infinito. En toda distribución, el límite es $1$. Tiende más rápido a 1 la Weibull, luego la Gumbel y luego la Fréchet. Esto es indica que la distribución Fréchet modela datos *más extremos*, es decir, máximos de datos de colas más pesadas que la Gumbel y ésta que la Weibull. 

En la Fréchet, la velocidad de convergencia a 1 crece al aumentar el orden. En cambio en la Weibull el orden afecta la velocidad con que va a 0 cuando $x$ tiende a menos infinito, que crece cuanto mayor el orden. Esto quedará más claro con el Teorema 1 del curso. La visualización de las densidades de cada tipo quizás ayude a comprender mejor los pesos relativos de las colas.
-->

```{r distributions, fig.cap="PDF extremales (alpha=1)" ,echo=FALSE, fig.height=3, fig.width=3, message=FALSE, warning=FALSE}

# Define the range of x values, ensuring negative values are included
x_gumbel <- seq(-5, 5, length.out = 1000)
x_weibull <- seq(-5, 5, length.out = 1000)  
x_frechet <- seq(-5, 5, length.out = 1000)  # Extended to include negative values

# Define the PDFs based on given formulas
gumbel_pdf <- function(x) exp(-x) * exp(-exp(-x))
weibull_pdf <- function(x, alpha = 1) ifelse(x < 0, alpha * (-x)^(alpha - 1) * exp(-(-x)^alpha), 0)
frechet_pdf <- function(x, alpha = 1) ifelse(x > 0, alpha * x^(-alpha - 1) * exp(-x^(-alpha)), 0)  # Explicitly defined for x ≤ 0

# Compute PDFs
gumbel_vals <- gumbel_pdf(x_gumbel)
weibull_vals <- weibull_pdf(x_weibull, alpha = 1)  # Order 1
frechet_vals <- frechet_pdf(x_frechet, alpha = 1)  # Order 1

# Create data frames for ggplot
df_gumbel <- data.frame(x = x_gumbel, y = gumbel_vals, PDF = "Gumbel")
df_weibull <- data.frame(x = x_weibull, y = weibull_vals, PDF = "Weibull")
df_frechet <- data.frame(x = x_frechet, y = frechet_vals, PDF = "Fréchet")

# Combine all data
df <- rbind(df_gumbel, df_weibull, df_frechet)

# Plot using ggplot2
ggplot(df, aes(x, y, color = PDF))+
  geom_line(size = 1) +
  labs(x = "x", y = "F'(x)=f(x)") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green"))+
theme(
  legend.position = "bottom",  # or "right", etc.
  legend.title = element_text(size = 8),
  legend.text = element_text(size = 7),
  legend.key.size = unit(0.4, "lines"),  # smaller symbol boxes
  legend.spacing.x = unit(0.2, "cm"),    # tighter horizontal spacing
  legend.box.margin = margin(t = 2)      # optional: slight padding above
)
```

\newpage
A estas versiones standard se las puede extender agregando un parámetro de recentramiento $(\mu)$ y
un parámetro de escala $(\beta)$. 

Se dice que $X$ tiene distribución: 

- __Gumbel__ : $\Lambda^{(\mu, \beta)}$ si $\;X=\mu + \beta Y\;$, donde $Y$ tiene distribución $\Lambda$.
- __Weibull__: $\;\Psi^{(\mu, \beta)}\;$ si $\;X=\mu + \beta Y\;$, donde $Y$ tiene distribución $\Psi_{\alpha}$.

- __Fréchet__: $\;\Phi^{(\mu, \beta)}\;$ si $X=\mu + \beta Y$, donde $Y$ tiene distribución $\Phi_{\alpha}$.


En general, es en este sentido que diremos que una variable es Gumbel, Weibull o Fréchet (incluyendo recentramiento y reescalamiento), pero en cálculos donde los parámetros $\mu$ y $\beta$ no sean relevantes, por simplicidad, usaremos las versiones standard. 


El siguiente teorema vincula las distribuciones extremales en sus formatos standard (ver Apéndice \@ref(sec:gev)) que resulta de gran utilidad práctica, sobre todo al hacer tests de ajustes, etc.



::: {.theorem #foo1 name="Relaciones entre las versiones standard de las distribuciones extremales"}
$X$ tiene distribución Fréchet $\Phi_{\alpha}$ $\Leftrightarrow$  $(-1/X)$ tiene distribución Weibull $\Psi_{\alpha}$ $\Leftrightarrow$ $\log(X^{\alpha})$ tiene distribución Gumbel $\Lambda$.
:::


Nota: en otros contextos de la Estadística (en particular enalguna rutinas de R), se le llama Weibull a una variable que corresponde a $-X$, con $X$ Weibull como definimos nosotros.

__Observación 5:__  Recordamos que la función Gamma ($\Gamma$ ), que extiende a la función factorial
($\;\Gamma(n)=n-1!\quad \forall n\;$ natural)^[La función Gamma permite expresar la media, la varianza y otras propiedades estadísticas de la GEV (ver Apéndice \@ref(sec:gev)) en función de su parámetro de forma $\xi$.] definida por


\begin{equation}
\Gamma(x)=\int_{0}^{\infty} t^{u-1}e^{-t}dt
\end{equation}

es una función disponible tanto en el software R como en planillas de cálculo, etc.


::: {.theorem #foo2 name="Algunos datos de las distribuciones extremales"}
Tres partes:
  
__Parte 1__

Si $X$ tiene distribución $\Lambda^{(\mu,\beta)}$ entonces tiene:



a) __Esperanza__: $E(X) = \mu + \beta\gamma$, donde $\gamma$ es la constante de Euler-Mascheroni, cuyo valor aproximado es $0.5772156649$.

b) __Moda__:  $\text{moda}(X)=\mu$

c)  __Mediana__: $\text{med}(X)=\mu - \beta \log(\log 2) \approx \mu - 0.36651 \beta$

d)  __Desviación estándar__: $\sigma(X)=\frac{\beta \pi}{\sqrt{6}}   \approx 1.2825 \beta$

e) Si $X^+ = \max(X,0)$, entonces $E(X^{+k})$ es finito para todo valor de $k$ natural

f) Para simular computacionalmente $X$, se puede tomar $U$ uniforme en $(0,1)$ y hacer $X = \mu - \beta \log(-\log U)$.


__Parte 2__

Si $X$ tiene distribución $\Psi^{(\mu, \beta)}$ entonces tiene:

a) $E(X)=\mu -\beta \Gamma (1+1/\alpha)$

b) \begin{equation*}\text{moda}(X) =\begin{cases} 
  \mu  & \text{si }\; \alpha \leq 1 \\
    \mu-\beta\left\{ \frac{\left( \alpha-1 \right)}{\alpha} \right\}^{1/\alpha} & \text{si }\; \alpha >1
\end{cases}\end{equation*}


c) $\text{med}(X)=\mu - \beta (\log 2)^{\frac{1}{\alpha}}$

d) $\sigma(X)=\beta\left\{\Gamma\left( 1+\frac{2}{\alpha} \right)-\Gamma\left( 1+\frac{1}{\alpha} \right)^2  \right\}^{1/2}$.

__Parte 3__

Si $X$ tiene distribución $\Phi_{\alpha}^{(\mu, \beta)}$ entonces tiene:

a) 
\begin{equation*}
E(x) =
\begin{cases} 
    \mu + \beta\;\Gamma\left( 1-\frac{1}{\alpha} \right) & \text{si } \alpha>1 \\
    \infty & \text{en otro caso}
\end{cases}
\end{equation*}

b)  $\text{moda}(X)=\mu+ \beta\;\left\{ \frac{\alpha}{\left( 1+ \alpha\right)}\right\}^{1/\alpha}$

c) $\text{med}(X)=\mu + \beta \;\left( \log 2 \right)^{\left( -1/\alpha \right)}$

d) \begin{equation*}
\sigma(x) =
\begin{cases} 
    \beta \left| \Gamma \left( 1 - \frac{2}{\alpha} \right) - \Gamma \left(  1 - \frac{1}{\alpha}\right)^2\right|  & \text{si } \; \alpha>2 \\
    \infty & \text{si } \; 1<\alpha \leq 2
\end{cases}
\end{equation*}
:::




__Observación 6:__ El item e) de la Parte 1 es trivialmente cierto para Weibull y tomando en
cuenta el item a) de la Parte 3, es claramente falso
para Fréchet.

__Observación 7:__ El item f) de la Parte 1 en conjunto con el Teorema \@ref(thm:foo1) brinda fórmulas
sencillas para simular computacionalmente distribuciones Weibull o Fréchet.

__Observación 8:__ Se generaron mil números aleatorios y aplicando el item f) de la Parte 1: se simularon mil variables Gumbel standard $iid$, calculándose su promedio, su desviación standard empírica y su mediana
empírica. 
\newpage

```{r echo=TRUE}
## Observación 8:
# Fijar semilla para reproducibilidad
set.seed(123)
# Definir parámetros
mu <- 0       # Centro
beta <- 1     # Escala
gamma <- 0.5772156649  # Constante de Euler-Mascheroni
# Número de simulaciones
n <- 1000
# Generar 1000 valores de una variable uniforme en (0,1)
U <- runif(n)
# Simular la variable Gumbel con parámetros (mu, beta)
X_gumbel <- mu - beta * log(-log(U))
# Calcular estadísticas
esperanza <- mu + beta * gamma
moda <- mu
mediana_teorica <- mu - beta * log(log(2))
desviacion_std_teorica <- beta * pi / sqrt(6)
# Calcular estadísticas empíricas
promedio_empirico <- mean(X_gumbel)
desviacion_std_empirica <- sd(X_gumbel)
mediana_empirica <- median(X_gumbel)
```

\vspace{0.5cm}

Los resultados fueron los siguientes:

```{r echo=FALSE}
# Mostrar resultados teóricos
cat("----- Resultados teóricos: ----- \n")
cat("Esperanza teórica:", esperanza, "\n")
cat("Moda teórica:", moda, "\n")
cat("Mediana teórica:", mediana_teorica, "\n")
cat("Desviación estándar teórica:", desviacion_std_teorica, "\n\n")

# Mostrar resultados empíricos
cat("----- Resultados empíricos (simulación con n =", n, "): -----\n")
cat("Promedio empírico:", promedio_empirico, "\n")
cat("Mediana empírica:", mediana_empirica, "\n")
cat("Desviación estándar empírica:", desviacion_std_empirica, "\n")
```

Observar que los resultados empíricos están cerca del valor esperado, desvío standard y mediana de la Gumbel standard.

\newpage

A continuación presentaremos el Teorema medular de esta primera parte, expresado de la manera más simple posible (@fisher1928limiting, @gnedenko1943). Veremos posteriormente algunos detalles con más cuidado. En particular, veremos que la continuidad de la distribución $F$ no es una hipótesis real (ni es necesaria ni es suficiente, por eso la
entrecomillamos), pero ayuda a visualizar que no vale el teorema para toda distribución $F$, así como veremos con cierto detalle más adelante...



::: {.theorem #FTG name="De Fischer-Tippet-Gnedenko (FTG)"}
Si $X_1,\dots,X_n$ es $iid$ con distribución $F$ continua,
llamamos $F^{\ast}_n$ a la distribución de $max(X_1,\dots,X_n)$. Si $n$
es grande, entonces existen $\mu$ real y $\beta > 0$ tales que
alguna de las siguientes tres afirmaciones es
correcta:

a. $F^{\ast}_n$ se puede aproximar por la distribución de $\mu+\beta Y$, con $Y$ variable con distribución $\Lambda$.

b. Existe $\alpha>0$ tal que $F_n^{\ast}$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$. 

c. Existe $\alpha>0$ tal que $F_n^{\ast}$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$. 
:::


Lo anterior equivale a decir que la distribución del máximo de datos _continuos_ e $iid$, si $n$ es grande, puede aproximarse por una Gumbel, una Fréchet o una Weibull. 

__Observación 9:__ Cuál de las tres aproximaciones es la válida depende de cómo sea la distribución $F$.

Por ejemplo, veremos que:

- Si $F$ es normal o exponencial, se aplica a $F_n^{\ast}$ la aproximación por una Gumbel .
- Si $F$ es uniforme, vale para $F_n^{\ast}$ la aproximación por una Weibull.
- Si $F$ es Cauchy, la aproximación válida para $F_n^{\ast}$ es por una Fréchet.

Más precisamente, cuál de las tres aproximaciones es la aplicable depende de la cola de $F$: los valores de $F(t)$ para valores grandes de $t$.

En concreto, Weibull aparece cuando $F$ es la distribución de una variable acotada por arriba
(como la Uniforme), Gumbel para distribuciones de variables no acotadas por arriba pero con colas muy livianas (caso Exponencial y Normal) y Fréchet para colas pesadas (caso Cauchy).

Finalmente, si bien aclaramos que la hipótesis de continuidad de $F$ no es esencial, veremos que si $F$ es la distribución Binomial o Poisson, por mencionar dos ejemplos muy conocidos y sencillos, NO se
puede aplicar ninguna de las tres aproximaciones anteriores.


__Observación 10.__ Como consecuencia del Teorema \@ref(thm:FTG) si se tienen datos de máximos, las distribuciones extremales son _candidatas_ razonables para proponer en un ajuste.
Sin embargo no debe pensarse que siempre se va a lograr ajustar a una de las tres distribuciones extremales, ya que hay al menos dos causas evidentes que podrían desbaratar la aplicación del Teorema \@ref(thm:FTG):

1) Que la cantidad de registros que se consideran al calcular cada máximo no sea suficientemente
grande. 

2) Que los registros que se consideran al calcular cada máximo no sean $iid$^[Al final del capítulo 2 se verá que esto puede subsanarse con versiones más generales del Teorema \@ref(thm:FTG).].

Por consiguiente el \@ref(thm:FTG) alienta a intentar ajustar datos extremales a una de las tres distribuciones extremales, pero no siempre un tal ajuste dará un resultado afirmativo.

<!--- EJEMPLO DEL LIBRO A REVEER -->



::: {.example #ejemplo1 }
Veamos un ejemplo de ajuste. Los siguientes datos corresponden a los valores, en $80$ puntos geográficos distintos de la región parisina, del máximo estival del contaminante atmosférico $O_3$ (no perceptible sensorialmente y con impacto sanitario serio). Cada dato es el máximo registro en cada sensor a lo largo de todo un verano; el contaminante se mide diariamente, por lo cual, cada uno de nuestros $80$ datos es el máximo de unas $100$ lecturas diarias (Figura \@ref(fig:parismax)).
:::



```{r echo=FALSE, paged.print=TRUE}
data <- data.frame(
  X_i = c(430.3, 115.7, 4.48, 26.95, 72.27, 206.4, 22.79, 25.03, 226.8, 11.1,
        1572, 100, 104.5, 37.1, 20.22, 106.9, 47.2, 62.82, 39.3, 18.52,
        41.47, 429.5, 1228, 127.6, 9.93, 90.4, 201.7, 295.1, 20.62, 20.58,
        13.27, 538.1, 804, 321.6, 16.11, 22.05, 100.2, 40.76, 262.7, 19.32,
        7.79, 58.02, 28.02, 18.38, 13.12, 572.8, 44.46, 40.72, 25.07, 24.07,
        511.8, 38.12, 15.86, 75.48, 24.09, 119.4, 174.7, 104.7, 140, 79.67,
        158, 25.46, 462.5, 35.53, 876.4, 462.5, 53.47, 23.59, 38.77, 494.2,
        164.2, 52.06, 54.13, 15.53, 29, 14.35, 1675, 15.01, 72.07, 22.99))
```

```{r echo=FALSE, paged.print=TRUE}
n <- length(X_i)
```


\vspace{0.3cm}

```{r parismax, fig.cap="Máximos diarios durante el verano", fig.height=3, fig.width=6, echo=FALSE, message=FALSE, warning=FALSE}
# Agregar variable día (índice de 1 a 80)
data$dia <- 1:nrow(data)

# Gráfico
plot(data$dia, data$X_i, type = "o", pch = 16, col = "darkblue",
     xlab = "Día", ylab = "Máximo diario (03)")
grid()
```



Los valores se miden en unidades de referencia standarizadas que, en particular, permiten comparar las medidas de lugares diferentes, independientemente de variables relevantes como altura e incidencia solar, por trabajo previo de calibración.

El __objetivo__ del estudio en esta etapa es conocer la distribución de estos datos y en particular estimar la probabilidad de que el máximo estival en los 80 puntos supere el valor 50 (correspondiente a
existencia de riesgo moderado).

Veamos los datos que tenemos:

  
```{r echo=FALSE}  
print('Cálculo de estadísticos básicos')
summary(data$X_i)
```

Como la mayoría de tests de ajustes suponen datos $iid$, realizaremos dos tests de aleatoriedad^[En inglés es _randomness_.]: 

- Runs test (Up & Down)
- Spearman correlation of ranks

Para realizar el ajuste utilizaremos el test $\chi^2$ de ajuste^[Una excelente referencia para la temática de los test $\chi^2$ de ajuste es la introducción del trabajo Pearsonian Tests and Modifications (Jorge Graneri, CMAT, Facultad de Ciencias, 2002).].  Este test requiere elegir una partición más o menos arbitraria de la recta real en intervalos; sin embargo es importante que en cada intervalo caiga una cantidad suficiente de datos de la muestra; en este caso hemos tomado como extremos de los intervalos los quintiles empíricos de nuestra muestra.  Una aclaración mucho más importante es que este test requiere estimar parámetros por el
método de Máxima Verosimilitud Categórica, que da resultado distintos al método de Máxima Verosimilitud a secas^[Este hecho es frecuentemente ignorado y presentado erróneamente en los textos y
cursos básicos de Estadística.].

\vspace{0.3cm}

Aplicamos los tests:
```{r echo=TRUE, warning=FALSE}
library(tseries)
# ------------------------------
# 1. Tests de aleatoriedad
# ------------------------------
# (a) Runs up and down
runs_test_result <- runs.test(factor(X_i > median(X_i)))
# (b) Spearman
spearman_pval <- cor.test(X_i[-n], X_i[-1], method = "spearman")$p.value
```
\vspace{0.3cm}

Resultados de los tests: 

```{r echo=FALSE}
cat("p-valor (runs up and down):", runs_test_result$p.value, "\n")
cat("p-valor (Spearman):", spearman_pval, "\n")
```

\vspace{0.3cm}
Interpretación de tus resultados:

 - Runs up and down:  No se rechaza la hipótesis nula de aleatoriedad. La secuencia de datos no presenta un patrón sistemático de subidas o bajadas.

- Spearman: No hay evidencia de correlación entre observaciones sucesivas. De hecho, este valor sugiere una independencia aún más clara que la del test de corridas.

Ambos tests son coherentes con la hipótesis de que los datos se comportan como independientes e idénticamente distribuidos (iid), lo cual justifica el uso de modelos estadísticos que requieren esta suposición.





<!--El p-valor en runs up and down es 0,868 y en
Spearman es 0,474.-->

Como cada dato de los 80 que disponemos es un máximo de un centenar de observaciones, intentaremos ajustarlos a una distribución extremal sabiendo que no necesariamente tendremos éxito.  Observemos en particular que lo que pasamos por dos tests de aleatoriedad son los 80 máximos, pero no el centenar de lecturas que forman cada uno de los 80 máximos (ni siquiera tenemos esos datos originales). 

Dado que visualmente se aprecian valores muy apartados (Figura \@ref(fig:parishist)), se
presume una distribución de colas pesadas y por ese motivo se intenta un ajuste a una Fréchet.

```{r parishist, fig.cap="Histograma de los máximos diarios", fig.height=3, fig.width=6, echo=FALSE, message=FALSE, warning=FALSE}
hist(data$X_i, breaks = 30, main="",
     xlab = "Valor observado", ylab="frecuencia", col = "lightblue", border = "white")
```



Ajuste a una Fréchet:

```{r echo=TRUE, warning=TRUE}
# ----------------------------
# 1. CDF de Fréchet
# ----------------------------
frechet_cdf <- function(x, alpha, mu, beta) {
  ifelse(x > mu,
         exp(-((x - mu) / beta)^(-alpha)),
         0)
}
```


```{r echo=TRUE, warning=FALSE}
# ----------------------------
# 2. Datos y cortes (quintiles)
# ----------------------------
X_i <- data$X_i
n <- length(X_i)
breaks <- quantile(X_i, probs = seq(0, 1, 0.2))
observed <- hist(X_i, breaks = breaks, plot = FALSE)$counts
```


```{r echo=TRUE}
# ----------------------------
# 3. Log-verosimilitud categórica
# ----------------------------
loglik_cat <- function(par) {
  alpha <- par[1]
  mu <- par[2]
  beta <- par[3]
  if (alpha <= 0 || beta <= 0) return(1e10)  # penalizar fuera del dominio
  p <- diff(frechet_cdf(breaks, alpha, mu, beta))
  p <- pmax(p, 1e-10)  # evitar log(0)
  -sum(observed * log(p))  # log-verosimilitud categórica negativa
}
```



```{r echo=TRUE}
# ----------------------------
# 4. Optimización de parámetros
# ----------------------------
# Valores iniciales: manual
start_par <- c(alpha = 1.04, mu = -6.5, beta = 44)
fit <- optim(par = start_par, fn = loglik_cat, method = "L-BFGS-B",
             lower = c(0.01, -100, 1), upper = c(10, 100, 1000))
```

\vspace{0.3cm}
Estos son los parámetros estimados por MVC:

```{r echo=FALSE}
fit$par  # parámetros estimados por MVC
```


\vspace{0.3cm}

Ahora con los parámetros estimados, se realiza el test $\chi^2$:

```{r echo=TRUE}
# Parámetros estimados por MVC
alpha <- 0.9095118
mu    <- 0.1325914
beta  <- 35.9094567

# Probabilidades teóricas en cada intervalo (quintiles)
p_expected <- diff(frechet_cdf(breaks, alpha, mu, beta))
expected <- p_expected * n

# Recalcular test Chi2
chi_sq <- sum((observed - expected)^2 / expected)
df <- length(observed) - 1 - 3
p_val_chi <- 1 - pchisq(chi_sq, df)
```

\vspace{0.3cm}

Se obtiene como resultados:

```{r echo=FALSE, warning=FALSE}
cat("Chi2:", chi_sq, "gl:", df, "p-valor:", p_val_chi, "\n")
```


Con los parámetros estimados por Máxima Verosimilitud Categórica ($\alpha = 0{,}91$, $\mu = 0{,}13$, $\beta = 35{,}91$), el test $\chi^2$ de ajuste arroja un valor de $2{,}96$ con $1$ grado de libertad, lo que da un valor-$p$ de $0{,}085$. Esto implica que:


- Para un nivel de significancia del $10\%$ ($\alpha = 0{,}10$), no se rechaza la hipótesis nula, por lo tanto el modelo Fréchet es considerado adecuado.
- Para un nivel más estricto del $5\%$ ($\alpha = 0{,}05$), tampoco se rechaza  la hipótesis, sin embargo el valor-$p$ se encuentra muy cerca del umbral.


Por lo tanto, adoptaremos el modelo Fréchet ajustado por MVC como razonable para describir estos datos.


\newpage

```{r echo=TRUE, warning=FALSE}
# -------------------------------------
# Excedencia del nivel 50
# -------------------------------------
# 1. Proporción empírica de valores que superan 50
empirical_prop <- mean(X_i > 50)
# 2. Intervalo de confianza para proporción (binomial)
exceed_count <- sum(X_i > 50)
CI <- prop.test(exceed_count, n)$conf.int  # IC del 95%
# 3. Parámetros estimados previamente por Máxima Verosimilitud Categórica
alpha <- fit$par[1]
mu    <- fit$par[2]
beta  <- fit$par[3]
# 4. Cálculo de la probabilidad de exceder 50 bajo Fréchet ajustada
p_frechet <- 1 - frechet_cdf(50, alpha, mu, beta)
```

\vspace{0.3cm}
Resultados: 

```{r echo=FALSE}
# 5. Mostrar resultados
cat("Proporción empírica de X_i > 50:", round(empirical_prop, 4), "\n")
cat("IC (95%) para la proporción empírica: [", round(CI[1], 4), ",", round(CI[2], 4), "]\n")
cat("Probabilidad bajo el modelo Fréchet ajustado:", round(p_frechet, 4), "\n")
```

La proporción empírica de excedencias del nivel 50 es $\hat{p} = 0{,}5125$, con un intervalo de confianza del 95\% igual a $[0{,}3989,\ 0{,}6248]$.  Bajo el modelo Fréchet ajustado por máxima verosimilitud categórica, se estima una probabilidad de excedencia $\mathbb{P}(X > 50) = 0{,}5238$. Ambos valores son coherentes, lo que apoya la adecuación del modelo propuesto.

<!--
Adoptando pues este modelo, un sencillo cálculo
muestra que la probabilidad de que el máximo
exceda 50 es 0.455, lo cual es absolutamente
consistente con lo observado en la muestra, donde
la proporción empírica de excedencia del nivel 50
es 0.5125 con un intervalo de confianza al 95%
para esta proporción de (0.403, 0.622).
-->

<!-- Análisis del Q-Q Plot

- Sección Inicial (Cuantiles Bajos): En los valores bajos, los puntos se alinean bien con la diagonal roja, indicando un buen ajuste en la parte central de la distribución.
Colas Extremas:

- Para los valores más extremos, se observan desviaciones importantes de la diagonal, especialmente en los cuantiles más altos.
Esto sugiere que la distribución Fréchet ajustada podría no estar capturando completamente la cola extrema de los datos observados.
-->


<!-- Análisis del Histograma con Densidad Ajustada (Fréchet)

- Ajuste General:La curva de densidad Fréchet (línea roja) sigue la tendencia general del histograma.
Sin embargo, la altura de la primera barra es significativamente mayor que la densidad esperada, lo que sugiere una mayor concentración de valores pequeños.

- Colas Pesadas: La distribución Fréchet modela bien las colas pesadas, aunque podría estar subestimando la frecuencia en las colas extremas.

------------------------

Conclusión: 
- El test de ajuste $\chi^2$ mostró un p-valor aceptable (0.3553), lo cual apoya la hipótesis de un buen ajuste.

Sin embargo, las visualizaciones gráficas indican que:

- Hay una sobreestimación de la densidad en la parte inicial (valores bajos).
- Hay una subestimación de la probabilidad de los valores extremadamente altos.

Entonces, podría ser beneficioso:

Probar otras distribuciones de colas pesadas, como Weibull o Pareto Generalizada (GPD).
Realizar un análisis de valores extremos específicamente en la cola superior.


------- ESTA ES LA CCL DE GONZA, COMO HIZO EL EJ? ------

Conclusión del libro a reveer ejercicio porque no tengo los calculos: 
Adoptando pues este modelo, un sencillo cálculo
muestra que la probabilidad de que el máximo
exceda 50 es 0.455, lo cual es absolutamente
consistente con lo observado en la muestra, donde
la proporción empírica de excedencia del nivel 50
es 0.5125 con un intervalo de confianza al 95%
para esta proporción de (0.403, 0.622). Se llega a la conclusión que hay una incidencia
muy seria de niveles moderados de riesgo (se
prevee que cerca de la mitad de los puntos estén
afectados). 
<!--- FIN EJEMPLO DEL LIBRO A REVEER -->

__Observación 10.__ Una distribución \( H \) se dice __degenerada__ si \( H(t) = 0 \) o \( H(t) = 1 \) para todo \( t \in \mathbb{R}\). Este tipo de distribuciones representan variables aleatorias constantes. Es decir, si la distribución de \( X \) es degenerada, entonces \( X = c \) con probabilidad uno, para alguna constante \( c \). En tal caso, no tiene sentido realizar inferencia sobre \( X \), por lo que sólo consideramos de interés las distribuciones no degeneradas.



<!---
Una distribución $H$ se dice degenerada si $H(t)=0 \text{ ó } 1$ para todo valor de $t$. Representan a variables que no son tales, si la distribución de $X$ es degenerada, entonces $X$ es una constante, y no tiene sentido hacer estadística sobre $X$, por lo tanto sólo tienen
interés para nosotros las distribuciones no-degeneradas.
-->

\newpage
## Distribución extremal asintótica (DEA)
::: {.definition #dea name="Distribución extremal asintótica (DEA)"}
Si $X_1,\dots,X_n$ es $iid$ con distribución $F$, diremos que $H$ no-degenerada es la Distribución Extremal Asintótica (DEA) de $F$, si existen dos sucesiones de números reales, $d_n$ y $c_n>0$, tales que la distribución de
\begin{equation}
\frac{max(X_1,\dots,X_n)- d_n}{c_n}\;\text{ tiende a } H \text{ cuando } n \text{ tiende a infinito.}
\end{equation}

De manera equivalente, diremos que $F$ tiene DEA $H$.
:::


::: {.definition #supremo name="Supremo esencial de una variable aleatoria o distribución"}
Si $X$ tiene distribución $F$,
se llama  $M_X$ al supremo esencial de $X$ o,
indistintamente, supremo esencial de $F$ (denotado
$M_F$) a

\begin{equation}
M_X = M_F = \sup\{t \; / \; F(t) < 1\}
\end{equation}
:::


__Observación 11.__

- Si $F$ es $U(a,b)$, $M_F=b$.
- Si $F$  es $Bin(m,p)$, $M_F=m$.
- Si $F$  es Normal, Exponencial, Cauchy o Poisson entonces $M_F$ es infinito.



::: {.theorem #th4}
Si $X_1,\dots,X_n$ es $iid$ con distribución $F$ cualquiera, entonces, para $n \rightarrow \infty$,

\begin{equation}
X_n^{\ast} =max(X_1,\dots,X_n) \rightarrow M_F
\end{equation}
:::



__Observación 12.__ El resultado anterior vale incluso si $M_F$ es infinito, pero si $M_F$ es finito, como
$Xn* - Mf$ tiende a cero, por analogía con el Teorema Central del Límite para promedios, buscaríamos
una sucesión $c_n>0$ y que tienda a cero de modo tal que $(X_n^{\ast}- M_F )/ c_n$ tienda a una distribución no-degenerada y de allí surge buscar la DEA.


::: {.theorem #th5}
Si $F$ es una distribución con $M_F$ finito, y para $X$ con distribución $F$ se cumple que
\begin{equation}
P(X=M_F)>0
\end{equation}
entonces $F$ no admite DEA.
:::

 

__Observación 13.__ Si $F$ es $Bin(m,p) \Rightarrow M_F=m$. Si $X$ tiene distribución $F$, entonces
$P( X=M_F)= P( X=m)= p^m>0$, asi que la distribucion $Bin(m,p)$ no admite DEA, no se puede aproximar la distribución del máximo
de una muestra iid de variables $Bin(m,p)$.

El Teorema anterior es un caso particular del próximo.


::: {.theorem #th6}
Si $F$ es una distribución con $M_F$ finito o infinito que
admite DEA, y $X$ tiene distribución $F$, entonces el
limite cuando $t$ tiende a $M_F$ por izquierda de

\begin{equation}
P(X>t)/P(X \geq t) \;\text{debe ser}\;1.
\end{equation}
:::




__Observación 13.__ Si $F$ es una distribución de Poisson de parámetro $\lambda >0$, $M_F$ es infinito. Si $k$ es un natural, entonces

\begin{align}
P(X>t)/P(X \geq t)& = P(X \geq k+1)/P(X \geq k)\\
& = 1-\left\{ P(X=k)/P(X \geq k) \right\} \approx 1-(1- \lambda/k)
\end{align}

que tiende a 0 cuando $k$ tiende a infinito, por lo cual $F$ __no__ admite DEA, o sea que no se puede aproximar el máximo de una sucesión $iid$ de variables de Poisson.

__Observación 14.__ El Teorema \@ref(thm:th6) brinda una condición __necesaria__ pero __no suficiente__
para DEA. Un ejemplo de ello lo aportó @vonmises1954, mostrando que la distribución

\begin{equation}
F(x)= 1- e^{(-x-\sin(x))}
\end{equation}

cumple con la condicion del Teorema \@ref(thm:th6) pero no admite DEA. El tema será cerrado al estudiar los
dominios de atracción maximal, en breve.

Veamos ahora ejemplos donde la DEA resulta aplicable y que ratifican algunos hechos que anticipáramos.

__Observación 15.__  Si $F$ es $U(0,1)$ y consideramos
$X_1,\dots,X_n$ iid con distribución $F$, resulta que
la distribución de $n( X_n^{\ast} - 1)$ tiende a $\Psi_1$ por lo cual la distribución uniforme tiene DEA
Weibull.

__Observación 16.__ Si $F$ es Exponencial de
parámetro 1 y consideramos $X_1,\dots,X_n$ iid con
distribución $F$, se tiene que la distribución de $X_n^{\ast} - \log n$ tiende a $\Lambda$ por lo cual la distribución exponencial tiene DEA Gumbel.

__Observación 17.__ Si $F$ es $N(0,1)$ y consideramos
$X_1,\dots,X_n$ iid con distribución $F$, definimos la función continua y estrictamente decreciente (para $u>0$)

\begin{equation}
g(u)= \frac{e^{-u^2/4\pi}}{u}.
\end{equation}

Como $\lim_{u \to 0}\; g(u) \rightarrow \infty$ y $\lim_{u \to \infty}\; g(u) \rightarrow 0$,
para todo natural $n$ existe un único valor $u_n$ tal que 

\begin{equation}
g(u_n)=\frac{1}{n}
\end{equation}

y resulta que $\frac{u_n}{\sqrt{2\pi} (X_n^{\ast}- u_n /\sqrt 2\pi)} \rightarrow \Lambda$, por lo cual la distribución normal tiene DEA Gumbel.


__Observación 18.__ Si $F$ es Cauchy standard que se expresa como $C(0,1)$ 
y consideramos $X_1,\dots,X_n$ iid con distribución $F$, se tiene que
la distribución de $\pi X_n^{\ast}/n$ tiende a $F_1$ por lo cual la distribución Cauchy tiene DEA Fréchet.

Los ejemplos anteriores no son sorprendentes, en el sentido que aunque presentamos el Teorema \@ref(thm:FTG) en una versión simplificada,
dicho teorema sugiere que cuando $F$ admite DEA, la distribución $H$ deberá ser una distribución extremal. De hecho, el Teorema \@ref(thm:FTG)
 resulta de combinar dos teoremas, basadas en una nueva definición: la de distribución __max-estable__.
 
## Distribuciones max-estables

::: {.definition #def3}
Se dice que una distribución $F$ es __max-estable__ si, para todo natural \( k \geq 2 \), existen las constantes \( a_k > 0 \) y \( b_k \in \mathbb{R} \) tales que:

\[
F^k(a_k x + b_k) = F(x), \quad \text{para todo } x \in \mathbb{R}.
\]

Dado que \( F^n \) es la función de distribución de \( M_n = \max\{X_1, \dots, X_k\} \), donde las variables \( X_i \) son $iid$ con distribución \( F\), la máx-estabilidad es una propiedad que caracteriza a aquellas distribuciones que se mantienen invariantes (salvo cambio de escala y traslación) bajo la operación de tomar máximos muestrales [@coles2001introduction]. Esto que implica que, bajo estas condiciones, $max(X_1,...,X_k)$ tiene la misma
distribución que $a_k X+ b_k$.
:::


El Teorema \@ref(thm:FTG) resulta de superponer los dos siguientes teoremas:

::: {.theorem #teo7}
a. Si $F$ admite DEA $H$ entonces $H$ es max-estable.

b. Si $H$ es max-estable, es la DEA de sí misma.
:::


::: {.theorem #teo8}
Una distribución es max-estable si y solo si es extremal: Gumbel, Weibull, Fréchet.
:::

El Teorema \@ref(thm:teo7) es bastante intuitivo y análogo a los Teoremas de Lévy sobre distribuciones estables en
aproximaciones asintóticas de las distribuciones de sumas. Para el Teorema \@ref(thm:teo8) haremos enseguida un ejercicio sencillo que nos ayudará a hacerlo creíble.


Luego precisaremos, para terminar con esta parte, cómo son las distribuciones que tienen por DEA cada uno de los tres tipos de distribuciones extremales. Para eso recordamos algunas definiciones, como la siguiente.

__Observación 19.__ Si $F$ y $G$ son dos distribuciones, van a tener colas equivalentes si $M_F=M_G$, y cuando $t$
tiende a $M_F$ por izquierda entonces $(1-F(t))/(1-G(t))$ tiende a un valor $c>0$.

Recordando ahora cómo se calcula la distribución del máximo de dos variables independientes, es
muy sencillo calcular la distribución del $max\left\{ X,Y \right\}$, cuando $X$ e $Y$ son independientes y cada una de ellas es una distribución extremal. Considerando lo anterior, se lleg al siguiente resultado.



| $X$ | $Y$| $max(X,Y)$ |
|-------|-------|--------------|
| \textcolor{red}{Weibull} | \textcolor{red}{Weibull} | \textcolor{red}{Weibull} |
| \textcolor[rgb]{0.0,0.5,0.0}{Weibull} | \textcolor[rgb]{0.0,0.5,0.0}{Gumbel} | \textcolor[rgb]{0.0,0.5,0.0}{Cola equivalente Gumbel} |
| \textcolor{blue}{Weibull} | \textcolor{blue}{Fréchet} | \textcolor{blue}{Fréchet} |
| \textcolor[rgb]{0.0,0.5,0.0}{Gumbel} | \textcolor[rgb]{0.0,0.5,0.0}{Weibull} | \textcolor[rgb]{0.0,0.5,0.0}{Cola equivalente Gumbel} |
| \textcolor{red}{Gumbel} | \textcolor{red}{Gumbel} | \textcolor{red}{Gumbel} |
| \textcolor{blue}{Gumbel} | \textcolor{blue}{Fréchet} | \textcolor{blue}{Cola equivalente Fréchet} |
| \textcolor{blue}{Fréchet} | \textcolor{blue}{Weibull} | \textcolor{blue}{Fréchet} |
| \textcolor{blue}{Fréchet} | \textcolor{blue}{Gumbel} | \textcolor{blue}{Cola equivalente Fréchet} |
| \textcolor{red}{Fréchet} | \textcolor{red}{Fréchet}| \textcolor{red}{Fréchet} |


\textcolor{red}{\rule{1em}{1em} Las extremales son max-estables: tomar máximos de dos del mismo tipo queda en el mismo tipo.}


\textcolor[rgb]{0.0,0.5,0.0}{\rule{1em}{1em} Gumbel es más pesada que Weibull. En la cola, que es lo que cuenta para máximos, prima Gumbel.}


\textcolor{blue}{\rule{1em}{1em} Fréchet es más pesada que Gumbel y mucho más pesada que Weibull.}

Además de la tabla anterior, se deduce lo siguiente.




::: {.theorem #teo9}
Si $X_1,\dots,X_n$ independientes y cada $X_i$ tiene uno de los tres tipos de distribución extremal, entonces la distribución del $max(X_1,\dots,X_n)$ es:
  
a. Cola equivalente a Fréchet, si alguna de las variables es Fréchet y alguna otra es Gumbel.

b. Fréchet, si alguna es Fréchet y ninguna es Gumbel.

c. Cola equivalente Gumbel ninguna es Fréchet pero algunas son Gumbel y otras Weibull.

d. Gumbel si todas son Gumbel.

e. Weibull si todas son Weibull.  
:::

## Dominio de Atracción Maximal (DAM)


__¿Qué es el dominio de atracción máximal (DAM)?__ El dominio de atracción máximal de una distribución límite es el conjunto de todas las distribuciones cuyos valores extremos, como los máximos de muestras grandes, muestran un comportamiento similar en el límite. Esto ocurre después de aplicar un cambio adecuado de escala y ubicación. Aunque las distribuciones originales pueden ser muy distintas, si sus valores extremos se parecen en el límite, decimos que pertenecen al mismo dominio de atracción. Esta idea permite clasificar fenómenos extremos en solo tres tipos posibles (Weibull, Fréchet y Gumbel), según la forma de su cola: con tope, pesada o ligera [@deHaanFerreira2006].

El artículo de @gnedenko1943 es un trabajo fundacional de la teoría de valores extremos: establece de forma rigurosa los tres dominios de atracción posibles para los máximos de variables aleatorias independientes e idénticamente distribuidas $(iid)$.



Vamos ahora a ver el concepto de *Dominio de Atracción Maximal*. Las funciones de variación lenta y regular permiten cuantificar cómo se comporta la cola de una distribución. Ese comportamiento de la cola es lo que define a qué tipo de ley límite (Weibull, Fréchet, Gumbel) convergerán los máximos. Por eso, la teoría del valor extremo y la teoría de funciones de variación están profundamente conectadas.

__Observación 20.__  Se dice que una distribución \( F \) tiene cola de __variación regular__ de orden \( -\alpha \) (con \( \alpha \geq 0 \)) si, para todo \( t > 0 \),
\[
\lim_{x \to \infty} \frac{1 - F(tx)}{1 - F(x)} = t^{-\alpha}.
\]
En tal caso se escribe que \( F \in R_{-\alpha} \). 

::: {.example #varregular}
Sea $F(u) = 1 - \frac{1}{u^3}, \quad u \geq 1$.

Entonces
\[
1 - F(u) = \frac{1}{u^3}, \quad \text{y} \quad
\frac{1 - F(tu)}{1 - F(u)} = \frac{1/(t^3 u^3)}{1/u^3} = \frac{1}{t^3}.
\]
Por lo tanto
\[
\lim_{u \to \infty} \frac{1 - F(tu)}{1 - F(u)} = t^{-3},
\]
lo que muestra que \( F \in \mathcal{R}_{-3} \), es decir, tiene cola de variación regular de orden \( -3 \).
:::


Por otra parte, se dice que $L$ es una función de __variación lenta__ si $\forall t>0$

$$
\lim_{x \to \infty} \frac{L(tx)}{L(u)} = 1.
$$



::: {.example}
La función \( L(u) = \log u \) es de variación lenta, ya que cumple
$$\frac{\log(tu)}{\log(u)} = \frac{\log t + \log u}{\log u} = 1 + \frac{\log t}{\log u} \longrightarrow 1 \quad \text{cuando } u \to \infty.$$
:::


::: {.definition #d4 name="Dominio de Atracción Maximal"}
Si $H$ es una distribución extremal (Gumbel, Weibull o Fréchet) su Dominio de Atracción Maximal ($DAM(H)$) está constituído por todas las distribuciones $F$ que tienen $DEA\;H$.
::: 



::: {.theorem #th9 name="DAM de la Fréchet"}

Sea \( F \) una función de distribución. Entonces:

$$
F \in DAM(\Phi_{\alpha}) \quad \Leftrightarrow \quad 1 - F(x) = x^{-\alpha} L(x),
$$

para alguna \( \alpha > 0 \) y alguna función \( L(x) \) de variación lenta.

Esto es equivalente a decir que \( F \in R_{-\alpha} \), es decir, que \( F \) tiene cola de variación regular de orden \( -\alpha \).
:::


<!-- Un ejemplo típico es $1 - F(x) = x^{-\alpha}, \quad x \geq 1$. En este caso, puede tomarse $d_n = 0, \quad c_n = n^{1/\alpha}$. -->


::: {.example name="Ejemplo típico del DAM de Fréchet"}

Un caso típico donde se aplica el Teorema \@ref(thm:th9) es cuando la cola de la distribución tiene la forma 
$$
1 - F(x) = x^{-\alpha}, \quad x \geq 1,
$$
para algún \( \alpha > 0 \). En este caso, la función \( L(x) \equiv 1 \) es constante, y por lo tanto, de variación lenta.

Esta distribución pertenece claramente al dominio de atracción de la Fréchet de parámetro \( \alpha \), es decir, \( F \in DAM(\Phi_{\alpha}) \).

Además, para normalizar el máximo \( M_n = \max(X_1, \dots, X_n) \), se puede tomar
$$
d_n = 0, \quad c_n = n^{1/\alpha}.
$$
En particular, si \( \alpha = 1 \), entonces \( c_n = n \).

:::

<!-- __Ejercicio 2 :__ Recompruebe en función de lo anterior que la distribución de Cauchy tiene DEA Fréchet. -->



::: {.exercise #ejercicio2}
Compruebe, en función de lo anterior, que la distribución de Cauchy tiene dominio de atracción máximo (DAM) de la distribución de Fréchet.
:::

La solución del Ej. \@ref(exr:ejercicio2)  se encuentra en el Apéndice \@ref(sec:soluciones).

::: {.corollary name="DAM de la Fréchet" #damfrechet}
Sea \( F \) una distribución con densidad \( f \). Si se verifica que
$$
\lim_{x \to \infty} \frac{x f(x)}{1 - F(x)} = \alpha,
$$
se dice que \( F \) cumple la __Condición de Von Mises I__.

En tal caso, \( F \in DAM(\Phi_\alpha) \). Más aún, el dominio de atracción de Fréchet de orden \( \alpha \) ($\Phi_{\alpha}$) está formado por todas las distribuciones cuya cola es equivalente a la de alguna distribución que satisface la Condición de Von Mises I (ver Apéndice \@ref(sec:vonmises)).
:::




<!--

```{corollary, label="corollary1", name="DAM de la Fréchet"}
Si $F$ es una distribución con densidad $f$ que cumple
que $xf(x)/(1-F(x))$ tiende a $\alpha$ cuando $x \rightarrow \infty$, se dice que $F$ cumple la *Condición de Von Mises I*.
En tal caso, $F$ pertenece a la DAM de $\Phi_{\alpha}$ y mas aún, la $DAM$ de $\Phi_{\alpha}$ son todas las distribuciones que tienen cola equivalente a alguna distribución que
cumpla la Condición de Von Mises I.
```
-->

Del Colorario \@ref(cor:damfrechet) y del Teorema \@ref(thm:foo1) surge lo siguiente.

::: {.theorem #th10 name="DAM de la Weibull"}

a. $F$ pertenece a la $DAM$ de $\Psi_{\alpha}$ si y solo si $M_F$ es finito y además $1-F(M_F-1/x)=x^{-\alpha} L(x)$ para alguna $L$ de variación lenta, es decir que pertenece a $R_{-\alpha}$. Observar que con el cambio de variable $u=M_F-1/x$, resulta que $1-F(u)=(M_F-u)^{\alpha} L(1/(M_F-u))$ para
alguna $L$ de variación lenta, para $u< M_F$. Un ejemplo típico sería $1-F(u)=(M_F -u)^{\alpha} ,\; u< MF$. Además puede tomarse $d_n= M_F$ y $c_n= n^{-\alpha}$.

b. Si $F$ distribución con densidad $f$ positiva en
$(a,M_F)$ para algun $a< M_F$ y $(M_F -x)f(x)/(1-F(x))$
tiende a $\alpha$ cuando $x \rightarrow M_F$, se dice que $F$ cumple la *Condición de Von Mises II*. 
Esto implica que si \( F \) es una distribución con densidad \( f \) positiva en un intervalo \( (a, M_F) \), para algún \( a < M_F \), y además se cumple que:

\[
\lim_{x \to M_F^-} \frac{(M_F - x) f(x)}{1 - F(x)} = \alpha,
\]

entonces \( F \in DAM(\Psi_{\alpha}) \).   Esta condición garantiza que la función de distribución se comporta como una potencia cerca del extremo superior, y por tanto pertenece al dominio de atracción de Weibull. Más aún, toda distribución cuya cola izquierda sea equivalente a una que cumpla esta condición también pertenece al \( DAM(\Psi_{\alpha}) \) [@deHaanFerreira2006]

<!--En tal caso, $F$ pertenece a la $DAM$ de $\Psi_{\alpha}$ y mas aún, la $DAM$ de $\Psi_{\alpha}$ son todas las distribuciones que tienen cola equivalente a alguna distribución que cumpla la Condición de
Von Mises II. -->
:::



::: {.theorem #th11 name="DAM de la Gumbel"}
Una distribución $F$ se dice una Función de Von Mises con función auxiliar $h$ si existe $a < M_F$ ($M_F$ puede ser finito o infinito) tal que para algún $c>0$ se tiene

\begin{equation}
1-F(x)= c\; \exp{- \int_{a}^{x} 1/h(t) dt}
(\#eq:th11)
\end{equation}

Se tiene entonces que la $DAM$ de $\Lambda$ son todas las distribuciones que tienen cola equivalente a alguna distribución que sea una Función de Von Mises.
::: 


Básicamente, se trata de colas más livianas que cualquier expresión del tipo $1/x^k$, más aún, con decaimiento "del tipo exponencial", en el sentido preciso siguiente. Si, como en el \@ref(thm:th11), se tiene que 

\begin{equation}
1-F(x)= c\; \exp{- \int_{a}^{x} \frac{1}{h(t)} dt},
(\#eq:th11_eq)
\end{equation}

entonces se tiene $1-F(x)\leq c\; \exp\{-(x-a)/h(x)\}$, donde la función auxiliar
$h$ es no-decreciente y con asíntota horizontal.


Además, $d_n$ y $c_n$ suelen involucrar expresiones logarítmicas. Más concretamente, $dn = F^{-1}(1-1/n)$ y,
$c_n = h(d_n)$, donde $F^{-1}$ es la inversa generalizada (o función cuantil), definida por
$F^{-1}(p)= \inf\{t / F(t)\geq p\}$, para $0<p<1$.




```{corollary, label="coll2"}
Si $F$ pertenece al $DAM$ Gumbel, $M_F$
es infinito, y se considera $X$ con distribucion $F$,
entonces $E(X+k)$ es finito para todo $k$ natural.
```



Los resultados antes vistos nos permiten reconocer
qué distribuciones tienen $DEA$ y si la tienen, cual es. Cierran el tema. Adicionalmente, permiten ver con mucha precisión que el quid de esta teoría es el comportamiento de las colas de las distribuciones, que Fréchet corresponde a las colas más pesadas, luego la Gumbel y finalmente Weibull. Para terminar el capítulo presentaremos la distribución de valores extremos generalizada
(GEV, por sus siglas en inglés), que es una forma de compactar en una unica fórmula las tres
distribuciones extremales, debida a Jenkinson-Von Mises.

<!-- __Observación 23.__ Tiempos y Valores de Retorno. --> 

::: {.exercise #Ejercicio3}
a. Recompruebe en función de lo
anterior que la distribución uniforme tiene $DEA$ Weibull. 

b. Encuentre la fórmula explícita de
alguna distribución que no sea la uniforme y tenga
$DEA$ Weibull. Solo resta encontrar la $DEA$ Gumbel, y eso lo
aporta el próximo resultado.
:::


::: {.exercise #Ejercicio4}
Recompruebe en función de lo anterior que la distribución exponencial y la
distribución normal tienen $DEA$ Gumbel.
:::


::: {.exercise #Ejercicio5}
a. Determinar si la distribución log-normal ($\log X$ es normal) tiene DEA y si la tiene,
determinar cuál es su DEA. 

b. Con la ayuda de R
simular una muestra de 100 datos iid, cada uno de
los cuales es el máximo de 500 log-normales
standard $iid$. Intente ajustar la distribución de la
muestra de 100 datos de acuerdo a lo obtenido en
la parte a).
:::



::: {.exercise #Ejercicio6 name="Variable acotada en DAM Gumbel"}
Tomemos tres constantes estrictamente positivas $\alpha,\; M, \;K$ y definamos $F(x)= 1 – K \exp\{-\alpha /(M-x)\}$ para $x<M$. Mostrar que $F$ es una distribución y que $M_F= M$.

Probar que $F$ es una función de Von Mises con
función auxiliar $h(t)= (M-t)^2/\alpha$ y que por lo tanto
está en el $DAM$ Gumbel. Finalmente, si $X_1,\dots,X_k$
$iid$ con distribución $F$, calcular las sucesiones de
reales, $d_n$ y $c_n>0$, tales que la distribución de
$\frac{max(X_1,\dots,X_n)- d_n}{c_n}\rightarrow \Lambda$ cuando $n\rightarrow \infty$.
:::


Las soluciones de todos los ejercicios se encuentran en el Capítulo \@ref(sec:soluciones).


<!---  comentarios -->
\newpage

## Distribución Generalizada de valores extremos (GEV)

Las familias de distribuciones Gumbel, Fréchet y Weibull se pueden combinar en una única familia de modelos con función de distribucion (GEV) bajo la forma , 

\begin{equation}
G\left( z \right)=\exp\left\{ -\left[ 1+ \xi\left( \frac{z-\mu}{\beta} \right) \right]^{-1/\xi} \right\}
(\#eq:Gz)
\end{equation}

definida en el conjunto $\left\{ z: 1+\xi\left( z-\mu \right)/\beta>0 \right\}$ donde los parámetros satisfacen $-\infty < \mu < \infty$, $\beta>0$ y $-\infty < \xi < \infty$ [@coles2001introduction]. 

Tomando en consideración los disintos valores que puede tomar $\xi$, se expresa la siguiente definición. 

::: {.definition #def5 name="Distribución Generalizada de valores extremos (GEV)"}
Se define la GEV como
\begin{equation}
G(z;\mu,\beta,\xi) = \left\{ \begin{array}{cl}
e^{\left( 1+\frac{\xi(z-\mu)}{\beta} \right)^{-\frac{1}{\xi}}}, & \text{si}\;\xi \neq 0,\\
e^{ -\exp\left( -\frac{(z-\mu)}{\beta} \right) }, & \text{si}\;\xi = 0
\end{array} \right.
\end{equation} 
para todo $z$, con parámetros de posición $\mu$, escala $\beta$ e índice $\xi$.
\[
\text{Cuando }
\begin{cases}
\xi=0 & \text{: corresponde a Gumbel} \\
\xi< 0 & \text{: corresponde a Weibull} \\
\xi> 0 & \text{: corresponde a Fréchet}
\end{cases}
\]
:::



En __R__ existen rutinas para estimar $\xi$ con intervalos de confianza (por máxima verosimilitud, etc.) lo cual da formas de testear si una extremal es Gumbel, Weibull o Fréchet.

__Observación 21.__ En algunas situaciones los datos extremales pueden ajustarse a más de un modelo. Por ejemplo, puede ocurrir que tanto ajusten los datos una Gumbel como una Weibull. Frente a estas situaciones, no hay una receta única de cómo proceder sino que quien está modelando debe tener claro si corresponde volcarse hacia cálculos más pesimistas (que dan mayor probabilidad a eventos extremos muy severos) o más optimistas. 

Usualmente la opción pesimista implica privilegiar la seguridad y la optimista la economía de recursos, pero insistimos en que la reflexión ante cada caso es indispensable. Un poquito más delante veremos, al comparar un modelo Gumbel con un modelo Fréchet, que las diferencias pueden ser sumamente drásticas.

__Observación 22.__ Antes de seguir adelante, demos la respuesta a la parte a) del Ejercicio 5. Es un
ejercicio de Cálculo Diferencial sencillo mostrar que la cola de un $N(0,1)$, es decir $Q(t)=P(X>t)$,
donde $X$ tiene distribución $N(0,1)$, es equivalente, para $t$ tendiendo a infinito, a la función $\phi(t)/t$, donde $\phi$ representa la densidad normal típica (campana de Gauss). Basándose en esto, si se considera ahora una variable log-normal $Y$, tal que $\log(Y)$ es una $N(0,1)$, puede probarse que su cola $R(t)=P(Y>t)$, es equivalente, para t tendiendo a
infinito, a la función $\phi(\log(t))/\log(t)$. Con un poco más de Cálculo, esta última función puede
escribirse para $a>e$ (por ejemplo $a=3$), como $c\;\exp\{-\int_a^{t} \frac{1}{h(s)}ds \}$  para $t>a$, donde $c$ se expresa
en función de $a$ y $h(s)=\frac{s\log(s)}{(\log(s))^2}$, la cual cumple las hipótesis del Teo. \@ref(thm:th11).

Se concluye entonces que la log-normal está en el DAM Gumbel, o, lo que es lo mismo, que la log-normal admite DEA Gumbel.

<!---  transicion a  Tiempos y Valores de Retorno --->

## Estimación por Máxima Verosimilitud (MV)
 
Una vez que decidimos usar una distribución GEV para modelar los máximos por bloques (por ejemplo, las máximas anuales de temperatura o precipitación), el siguiente paso es ajustar el modelo a los datos. Esto significa estimar los tres parámetros de la GEV:

 

- \( \mu \): de localización

- \( \sigma \): de escala

- \( \xi \): parámetro de forma

Estos parámetros determinan completamente la forma y el comportamiento de la distribución.



El método más utilizado es el de Máxima Verosimilitud (MV). La idea central consiste en buscar los valores de los parámetros que hacen que los datos observados sean "lo más probables posible" bajo el modelo (ver Apéndice \@ref(sec:ap), sección \@ref(sec:mv)).



Este procedimiento se realiza mediante optimización numérica. En la práctica, existen funciones en R (como `fgev()` del paquete `ismev`, o `fevd()` del paquete `extRemes`) que implementan este ajuste de manera automática.


Pero este método no siempre funciona bien. El método de máxima verosimilitud tiene muy buenas propiedades si se cumplen ciertas condiciones:

a. Cuando el parámetro de forma \( \xi > -0.5 \), los estimadores son regulares y se comportan bien en muestras grandes.

b. Si \( -1 < \xi < -0.5 \), los estimadores existen, pero no tienen las propiedades asintóticas habituales.

c. Si \( \xi < -1 \), los estimadores por máxima verosimilitud pueden no existir.

En gerenal, en la práctica, vamos a estar en el caso a (@coles2001introduction). 


\newpage

::: {.example #mv name="Estimación por MV"}
Cuando disponemos de una muestra y asumimos que sigue un cierto modelo probabilístico, el paso siguiente consiste en determinar los parámetros de dicho modelo. Como estos parámetros son desconocidos, deben ser estimados a partir de los datos. Existen diversos métodos de estimación, pero el enfoque más utilizado y recomendado es el de máxima verosimilitud, por sus buenas propiedades estadísticas y su aplicabilidad general. 

Con los siguientes datos vamos ajustar y estimar por MV una GEV y una distribución Gumbel para luego analizar la bondad de ajuste en cada caso.
:::



Vamos a usar el paquete `evd` de `R` y de los datos `uccle` tomamos la columna `day` que contiene el máximo anual de precipitaciones diarias.

```{r echo=TRUE}
library(evd)
data(uccle)
```


```{r echo=TRUE}
head(uccle, 5)
```
```{r echo=TRUE}
datos=uccle$day
class(datos)
```



Vamos a emplear la función `fgev` que ajusta los parámetros por MV a una distribución de valores
extremos generalizada, $GEV(\mu,\beta,\xi)$.


```{r echo=TRUE}
fit_gev <- evd::fgev(datos)
fit_gev$estimate
```



Para ajustar una distribución de Gumbel, imponemos $shape=0$. 

```{r echo=TRUE}
fit_gumbel <- evd::fgev(datos,shape=0)
fit_gumbel$estimate
```



Se suelen utilizar gráficos de niveles de retorno para visualizar la relación entre los niveles estimados y sus correspondientes periodos de retorno, los cuales suelen representarse en escala logarítmica. Estos gráficos permiten obtener una estimación visual de los valores extremos esperados para distintos horizontes temporales. Sin embargo, para una mayor precisión, se recomienda realizar los cálculos numéricamente.

Además, existen diversos **gráficos de diagnóstico** que permiten evaluar de forma cualitativa la **bondad del ajuste** del modelo a los datos observados como se presentan en las siguientes figuras, para el caso del ajuste `fit_gev`.



```{r diagnosticofgev, fig.cap=c("Gráfico de cuantiles (Q–Q)", "Gráfico de probabilidades (P–P)", "Densidad ajustada vs empírica", "Niveles de retorno"), fig.height=4, fig.width=12, echo=FALSE, warning=FALSE}
plot(fit_gev, which = 1)
plot(fit_gev, which = 2)
plot(fit_gev, which = 3)
plot(fit_gev, which = 4)
```


- Gráfico P–P: este gráfico compara las probabilidades empíricas (representadas por cruces) con las probabilidades teóricas que predice el modelo GEV. Si el ajuste es bueno, los puntos deben alinearse cerca de la diagonal (la línea de 45°). Cuanto más cerca estén los puntos de esa línea, mejor es la concordancia entre los datos y el modelo. Además, el gráfico suele incluir bandas de tolerancia: si la mayoría de los puntos cae dentro de esas bandas, eso indica que el modelo ajusta adecuadamente los datos.

- Grafico Q-Q plot:  el ajuste será mejor cuanto más se aproximen los puntos a la recta y estén entre los límites.

- Gráfico de la función de densidad empírica y la función de valores extremos generalizada estimada. El análisis en este caso es visual. 

- Gráfico de niveles y periodos de retorno: calcularlos permite concluir respecto al ajuste (Veremos estos conceptos en la próxima sección.).

Otra herramienta útil para evaluar la bondad del ajuste de un modelo es el test de Kolmogorov–Smirnov (KS). Este test está implementado en R mediante la función `ks.test()`. El objetivo del test es comparar si una distribución empírica (obtenida a partir de los datos) se ajusta razonablemente bien a una distribución teórica especificada previamente.

Se plantea un contraste de hipótesis

- \( H_0 \): los datos provienen de una determinada distribución teórica (por ejemplo, normal o GEV).
- \( H_1 \): los datos no provienen de esa distribución.


Al aplicar el test, se obtiene

- Un valor estadístico \( D \), que representa la máxima diferencia absoluta entre la función de distribución acumulada empírica y la teórica.

- Un p-valor, que permite decidir si hay evidencia suficiente para rechazar \( H_0 \).

Si \( D \) es pequeño (y el p-valor es grande), eso indica que no hay una diferencia significativa entre los datos y la distribución teórica, y por tanto el modelo podría ser adecuado.

El critorio de decisión consiste en comparar el estadístico \( D \) con un valor crítico \( D_\alpha \), determinado por el nivel de significación \( \alpha \):

- Si \( D > D_\alpha \), se rechaza \( H_0 \): los datos no se ajustan a la distribución propuesta.
- Si \( D \leq D_\alpha \), no se rechaza \( H_0 \): no hay evidencia para descartar el modelo.

En resumen, el test KS proporciona una forma cuantitativa y no paramétrica de evaluar si una distribución teórica es compatible con los datos observados.

Una forma práctica y habitual de tomar decisiones con el test de Kolmogorov–Smirnov es a través del p-valor que devuelve el test

- Si el p-valor es mayor que \( \alpha \) entonces no se rechaza la hipótesis nula. El modelo podría ajustarse bien.
- Si el p-valor es menor que \( \alpha \) entonces se rechaza la hipótesis nula. Hay evidencia de que el modelo no se ajusta.

El valor de \( \alpha \) más común es 0.05.

\vspace{1cm}

```{r echo=TRUE}
# Aplico el test sobre fit_gev:
ks.test(uccle$day, "pgev", 
        fit_gev$estimate[1], 
        fit_gev$estimate[2], 
        fit_gev$estimate[3])
```

- Advertencia sobre "ties":   El mensaje `"ties should not be present"` indica que hay valores repetidos (empates) en los datos (`uccle$day`).   El test de Kolmogorov–Smirnov asume que los datos provienen de una distribución continua, sin empates exactos.   Aunque el test se puede ejecutar, la presencia de empates puede afectar la precisión del valor-p, especialmente si son muchos. Esto es común cuando los datos se redondean a valores discretos (por ejemplo, a milímetros enteros).


-  $D = 0.0797$ : Este valor representa la máxima diferencia absoluta entre la función de distribución empírica y la función acumulada de la GEV ajustada. Cuanto menor sea \( D \), mejor es el ajuste del modelo a los datos.

- $\text{p-valor} = 0.9793$: Este valor es muy alto, lo que indica que no hay evidencia suficiente para rechazar la hipótesis nula de que los datos provienen de la distribución GEV ajustada.

Entonces, según el test de Kolmogorov–Smirnov, el modelo GEV se ajusta adecuadamente a los datos analizados.


```{r}
#Aplico el test sobre fit_gumbel:
ks.test(uccle$day, "pgev", 
        fit_gumbel$estimate[1], 
        fit_gumbel$estimate[2])
```


Según el test de Kolmogorov–Smirnov, el modelo Gumbel se ajusta adecuadamente a los datos analizados.

\newpage

## Tiempos y Valores de Retorno

Siguiendo a @coles2001introduction, vamos a ver la relación que existe entre GEV y los conceptos de __valor de retorno__ (VR) y __tiempo de retorno__ (T) de la presente sección. La Definición \@ref(def:def5) implica que se puede aproximar a la Eq.\@ref(eq:Mnr) como una integrante de la familia de GEV cuando $n$ es grande. Esto conlleva al siguiente enfoque para modelar las observaciones de datos extremos $X_1,X_2,\dots$ : se forman bloques de observaciones de largo $n$ (para $n$ grande) lo que permite crear bloques de máximos $M_{n,1},\dots, M_{n,m}$. Con esto va a ser posible ajustar una distibución GEV.  

A menudo, los bloques se eligen para que correspondan a un período de tiempo de un año de duración, en cuyo caso $n$ es el número de observaciones en un año y los máximos por bloque son máximos anuales. Considerando lo anterior, al invertir la Ec. \@ref(eq:Gz) obtenermos las expresiones para estimar los cuantiles extremos de la distribución del máximo anual, dadas por  

\begin{equation}
z_p=\left\{ \begin{array}{cl}
\mu-\frac{\beta}{\xi}\left[ 1-\left\{ -\log\left( 1-p \right) \right\}^{-\xi} \right], &  \xi \neq 0 \\
\mu-\beta\log\left\{ -\log\left( 1-p \right) \right\}, &  \xi = 0
\end{array} \right.
 (\#eq:rv)
\end{equation}

donde $P(X>z_p)=1/p\; \Rightarrow G(z_p)=1-p$. Con los parámetros estimados por MV $\hat{\mu}, \hat{\beta}, \hat{\xi}$ sustituimos en \@ref(eq:rv) y obtenermos $z_p$.

Decimos que $z_p$ es el valor de retorno ($VR$) asociado al tiempo de retorno ($T$) expresado como $1/p$ y se espera que el valor $z_p$ sea excedido en media una vez cada $1/p$ años. Es decir, para un año cualquiera, existe una probabilidad $p$ de que el valor máximo supere el nivel $z_p$.

Como los cuantiles permiten trabajar en la misma escala que los datos, es más fácil entender cómo los parámetros del modelo GEV afectan al modelo usando las fórmulas de los cuantiles \@ref(eq:rv). En particular, si $y_p=-\log\left(1-p\right)$, entonces 

\begin{equation}
z_p=\left\{ \begin{array}{cl}
\mu-\frac{\beta}{\xi}\left[ 1-y_p^{-\xi} \right], &  \xi \neq 0 \\
\mu-\beta\log y_p, &  \xi = 0
\end{array} \right.
\end{equation}


Si se grafica $z_p$ contra $\log y_p$:

- si $\xi = 0$ entonces se tiene una recta. Implica que los eventos extremos aumentan lentamente con el tiempo. Los eventos extremos son posibles, pero crecen de manera moderada y predecible.

- si $\xi <0$, la curva es convexa con límite asintótico a medida que  $p \to 0\;$ en $\mu - \beta / \xi$. No importa cuánto tiempo pase, el valor extremo no supera cierto límite. Hay un tope físico o natural. Por ejemplo, si estamos midiendo velocidad del viento, quizá físicamente no puede superar cierto umbral.

- si $\xi >0$, la curva es cóncava y no tiene un límite finito. No hay límite para los extremos. A medida que el tiempo de retorno aumenta, los eventos extremos se vuelven cada vez más grandes y más peligrosos. Esto es típico en fenómenos con colas pesadas, como catástrofes financieras o terremotos extremos.

Debido a la simplicidad en su interpretación, y a que la elección de la escala comprime la cola de la distribución, resaltando así el efecto de la extrapolación, las gráficas de niveles de retorno son especialmente útiles para presentar los datos y para validar. En la siguiente Figura \@ref(fig:nr) se presentan las curvas de valores de retorno para distintos valores de $\xi$.
 


```{r nr, fig.cap="Curvas de VR para distintos valores del parámetro de forma", fig.height=3, fig.width=6, echo=FALSE, message=FALSE, warning=FALSE}
# Parámetros
mu <- 0
beta <- 1
xi_vals <- c(-0.2, 0, 0.2)


# Dominio: log y de -2 a 6
log_y <- seq(-2, 6, length.out = 500)
y <- exp(log_y)

# Forma correcta de z_p en función de y_p
zp_function <- function(xi, mu, beta, y) {
  if (xi == 0) {
    return(mu + beta * log(y))
  } else {
    return(mu + (beta / xi) * (y^xi - 1))
  }
}

# Construir el data frame
df <- data.frame()
for (xi in xi_vals) {
  zp <- zp_function(xi, mu, beta, y)
  df <- rbind(df, data.frame(
    log_y = log_y,
    zp = zp,
    xi = factor(xi, levels = xi_vals,
                labels = c("Shape = -0.2", "Shape = 0", "Shape = 0.2"))
  ))
}

# Graficar
ggplot(df, aes(x = log_y, y = zp, color = xi)) +
  geom_line(size = 1.2) +
  labs(x = "Log y", y = "Quantile") +
  theme_minimal(base_size = 12) +
  scale_color_manual(values = c("blue", "black", "red")) +
  theme(legend.title = element_blank(),
        legend.position = c(0.8, 0.2),
        legend.background = element_blank())
```


__¿Por qué son importantes estos conceptos?__ 

En Ingeniería y Ciencias Ambientales, suele pensarse a los eventos extremos, en términos de tiempos de retorno, es decir, el tiempo que se espera para que ocurra un evento. En este sentido, bajo las hipótesis de datos $iid$, se dice que el tiempo de retorno $T$ tiene una distribución $Geo(p)$, con $p = P(evento)$. Por lo tanto, el tiempo de retorno medio es $E(T)=1/p$ y pueden hacerse intervalos de confianza para $E(T)$, en la medida que exista información de $P(evento)$.


Cabe observar que muchas veces se utiliza la expresión Tiempo de Retorno (TR) para $E(T)$. Más precisamente, $TR(u)$, el tiempo de retorno del valor $u$, es el valor esperado (o la media) del tiempo que se debe esperar para que la variable en estudio supere el valor $u$. Esto implica que  $TR(u) =1/P(X>u)$, si $X$ es la variable en estudio.

Por otro lado, y de manera inversa, el Valor de Retorno a tiempo $t$, $VR(t)$ es el valor de $u$ para el cual $TR(u)=t$, es decir que $TR(VR(t))=t$ (y también $VR(TR(u))=u$), es decir que TR y VR se pueden considerar funciones inversas una de la otra.

En suma:

- El tiempo de retorno (TR) permite cuantificar cuán raro o extremo es un evento. Por ejemplo, si se espera exceder un cierto nivel del río una vez cada 100 años, decimos que el tiempo de retorno $T=100$.
- El TR se basa en la probabilidad de excedencia $p$, es decir, la probabilidad de que un evento supere cierto umbral $z$ en un año $T=1/p$ entonces $p=1/T$.
- Para un nivel $z_p$ su valor de retorno en $T$ años es tal que $P(X>z_p)=1/T$.
- La forma de la curva de valores de retorno $z_p$ vs el tiempo $T$, o su equivalente $y_p=-\log(1-p)$ nos indica cómo crecen los valores extremos con el tiempo TR:

| Tipo de GEV | $\xi$     | Forma               | Implicación sobre el valor de retorno $z_T$ a largo plazo                       |
| ----------- | --------- | ------------------- | ------------------------------------------------------------------------------- |
| **Gumbel**  | 0         | lineal en $\log T$  | Crece lentamente                                                                |
| **Weibull** | $\xi < 0$ | se aplana (convexa) | Tiene un **máximo**: no importa cuánto esperes, no pasa de ahí                  |
| **Fréchet** | $\xi > 0$ | cóncava             | Crece **sin límite**: cuanto mayor el $T$, **más extremo** es el valor esperado |





::: {.example}
Para "bajar un poco a tierra" estos conceptos, vamos a calcularlos y compararlos cuando la variable $X$ es Gumbel y cuando es Fréchet (con los mismos valores de posición $\mu$ y escala $\beta>0$).
:::

- Comencemos por la __Gumbel__: $X$ tiene distribución $\Lambda^{( \mu,\beta)}$ si $X= \mu+\beta Y$, donde $Y$ tiene distribución $\Lambda$. 

Dado entonces un valor $u>0$, otro valor $t>0$ resulta que


::: {.definition #defg name="Ecuaciones G"}
\begin{align}
&P(X>u)=1-\exp\left\{ -\exp\left\{ -\frac{(u-\mu)}{\beta} \right\} \right\} \\
&TR(u)=1/P(X>u)\\ 
&VR(t)= \mu+\beta \log\left\{ \frac{t}{t-1} \right\}
\end{align}
:::

Cabe observar que si se supone que las observaciones son diarias (o enteras en la unidad que corresponda), los tiempos de retorno (TR) se redondean a enteros y los valores de $t$ en la última ecuación se toman enteros.


- Sigamos ahora por la Fréchet, recordemos que $X$ tiene distribución $\Phi_{\alpha}^{\mu,\beta}$ si $x=\mu+\beta Y$, donde $Y$ tiene distribución $\Phi_{\alpha}$.

Dado entonces un valor $u>0$, otro valor $t$ entero, resulta que

::: {.definition #deff name="Ecuaciones F"}
\begin{align}
&P(X>u)= 1-\exp\left\{ -\left\{ \frac{\left( u-\mu \right)}{\beta} \right\}^{-\alpha} \right\} \\
& TR(u)=1/P(X>u)\\
&VR(t)= \mu+\beta \left\{ \log\left\{\frac{t}{t-1} \right\} \right\}^{-1/\alpha}
\end{align}
:::

\vspace{1cm}
Se realizan las siguientes simulaciones empleando en ambos casos $\mu=15$, $\beta=10$ y $\alpha= 2.5$ ($\xi =0.4$, no muy distante del $\xi=0$ de la Gumbel).

\vspace{1cm}




```{r echo=TRUE, warning=FALSE}
# Parámetros comunes
mu <- 15
beta <- 10
alpha <- 2.5

# Secuencia de valores u (nivel) y t (retorno)
u_vals <- seq(15, 250, by = 5)
t_vals <- 90:4140

# Funciones para la Gumbel
P_exceed_gumbel <- function(u, mu, beta) {
  1 - exp(-exp(-(u - mu) / beta))
}

TR_gumbel <- function(u, mu, beta) {
  1 / P_exceed_gumbel(u, mu, beta)
}

VR_gumbel <- function(t, mu, beta) {
  mu + beta * log(t / (t - 1))
}

# Funciones para la Fréchet
P_exceed_frechet <- function(u, mu, beta, alpha) {
  1 - exp(-((u - mu) / beta)^(-alpha))
}

TR_frechet <- function(u, mu, beta, alpha) {
  1 / P_exceed_frechet(u, mu, beta, alpha)
}

VR_frechet <- function(t, mu, beta, alpha) {
  mu + beta * (log(t / (t - 1)))^(-1 / alpha)
}

# Cálculos
tr_gumbel_vals <- TR_gumbel(u_vals, mu, beta)
tr_frechet_vals <- TR_frechet(u_vals, mu, beta, alpha)

vr_gumbel_vals <- VR_gumbel(t_vals, mu, beta)
vr_frechet_vals <- VR_frechet(t_vals, mu, beta, alpha)
```








```{r echo=TRUE, warning=FALSE}
# Cálculo de probabilidades de excedencia
p_gumbel <- P_exceed_gumbel(u_vals, mu, beta)
p_frechet <- P_exceed_frechet(u_vals, mu, beta, alpha)
```



Se pueden visualizar claramente los resultados en la Fig.\@ref(fig:trioplot):

- De la Fig. \@ref(fig:trioplot) (a) se desprende que el  modelo Fréchet da probabilidades mucho mayores a valores muy elevados: es más pesimista, si los
valores mayores representan mayores esfuerzos o problemas .

- En cuanto a los TR de la Fig. \@ref(fig:trioplot) (b): Es claro que, siguiendo la lógica anterior, va a ser más pesimista_el modelo que de tiempos de retorno menores en valores elevados. Y eso es lo que ocurre con el modelo Fréchet. 

- En cuanto a los VR de la Fig. \@ref(fig:trioplot) (b): será  más pesimista quien dé mayores VR. Resulta evidente el mayor pesimismo del modelo Fréchet.

\newpage


```{r trioplot, fig.cap="Excedencias, tiempos y valores de retorno", fig.height=15, fig.width=16, echo=FALSE, message=FALSE, warning=FALSE}
par(mfrow = c(3, 1), mar = c(6.5, 5.5, 4.5, 2), oma = c(4, 0, 4, 0))

# 1. Probabilidades de excedencia
plot(u_vals, p_gumbel, type = "l", col = "blue", lwd = 2,
     ylim = c(0, 1), xlab = "u", ylab = expression(P(X > u)), main = "(a) Probabilidades de Excedencia",
     cex.lab = 2.5, cex.axis = 2, cex.main = 3)
lines(u_vals, p_frechet, col = "red", lwd = 2, lty = 2)
legend("topleft", legend = c("Gumbel", "Fréchet"), col = c("blue", "red"),
       lty = 1:2, lwd = 2, bty = "n", cex = 2.5)

# 2. Tiempo de retorno
plot(u_vals, tr_gumbel_vals, type = "l", col = "blue", lwd = 2,
     ylab = "Tiempo de Retorno", xlab = "Nivel u", main = "(b) TR(u)",
     cex.lab = 2.5, cex.axis = 2, cex.main = 3)
lines(u_vals, tr_frechet_vals, col = "red", lwd = 2, lty = 2)
legend("topleft", legend = c("Gumbel", "Fréchet"), col = c("blue", "red"),
       lty = 1:2, lwd = 2, bty = "n", cex = 2.5)

# 3. Valores de retorno (CORREGIDO)
plot(t_vals, vr_gumbel_vals, type = "l", col = "blue", lwd = 2,
     xlab = "Días", ylab = "Valor", main = "(c) Valores de Retorno",
     ylim = range(c(vr_gumbel_vals, vr_frechet_vals)),
     cex.lab = 2.5, cex.axis = 2, cex.main = 3)
lines(t_vals, vr_frechet_vals, col = "red", lwd = 2)
legend("topleft", legend = c("Gumbel", "Fréchet 2,5"), col = c("blue", "red"),
       lwd = 2, bty = "n", cex = 2.5)
```



:: {.example name="Valores y tiempos de retorno"}
Con los datos el Ej. \@ref(exm:mv) vamos ahora a calcular los VR y los TR.
:::

Para calcular los VR definimos  un vector con los valores de años (TR) para los que queremos calcular estos niveles, por ejemplo para 2, 5, 10, 15, 25, 50, 100, 150 y 200 años:

```{r}
# años para los que vamos a calcular los VR
tpo <- c(2,5,10,15,25,50,100, 150,200) 
```



```{r}
# probabilidades para esos años
prob <- 1/tpo
```




<!--

Ahora, salvando las ligeras diferencias fruto de que $t$ es discreto y hay redondeos, se tiene que:

- $TR(VR(t))\approx t$ para cada modelo,

- $VR(TR(u))\approx u$ para cada modelo.

Ejemplo para Gumbel

Tomamos \( t = 90 \) y calculamos $VR(90) = 15 + 10 \cdot \log\left( \frac{90}{89} \right) \approx 15.11$


Luego aplicamos la función de tiempo de retorno

\[
TR(15.11) = \frac{1}{1 - \exp\left( -\exp\left( -\frac{15.11 - 15}{10} \right) \right)} \approx 90
\]

Por tanto, $TR(VR(90)) \approx 90$.




```{r tablaVR}
# Tabla de valores de retorno (primeros 5 días)
tabla_vr <- data.frame(
  t = t_vals,  # ← nombre correcto aquí
  Gumbel = vr_gumbel_vals,
  Fréchet = vr_frechet_vals
)

# Mostrar con kable
knitr::kable(tabla_vr, digits = 1, caption = "VR para Gumbel y Fréchet")
```

```{r tablaTR}
# Tabla de tiempos de retorno (primeros 25 niveles u)
tabla_tr <- data.frame(
  u = u_vals,
  Gumbel =tr_gumbel_vals,
  Fréchet = tr_frechet_vals
)

# Mostrar con kable
knitr::kable(tabla_tr, digits = 1, caption = "TR para Gumbel y Fréchet")
```

Ahora si observamos los resultados tabulados en las Tablas \@ref(tab:tablaVR) y \@ref(tab:tablaTR):



```{r}
# Elegir t
t <- 90

# Calcular VR(t)
vr_t_gumbel <- vr_gumbel_vals[which(t_vals == t)]
vr_t_frechet <- vr_frechet_vals[which(t_vals == t)]

# Evaluar TR en VR(t)
tr_vr_gumbel <- TR_gumbel(vr_t_gumbel, mu, beta)
tr_vr_frechet <- TR_frechet(vr_t_frechet, mu, beta, alpha)

# Mostrar
cat("Gumbel:\n VR(90) =", round(vr_t_gumbel, 0), "→ TR(VR) =", round(tr_vr_gumbel, 1), "\n")
cat("Fréchet:\n VR(90) =", round(vr_t_frechet, 0), "→ TR(VR) =", round(tr_vr_frechet, 1), "\n")
```
Interpretación del resultado: 

- Para la Gumbel: Si vamos a la Tabla \@ref(tab:tablaVR) y miramos $t=90$, el $VR(t=90)\approx 15$, si vamos a la la Tabla \@ref(tab:tablaTR) el $TR(u=15)\approx 1.6$. 

- Para la Fréchet: Si vamos a la Tabla \@ref(tab:tablaVR) y miramos $t=90$, el $VR(t=90)\approx 75$, si vamos a la la Tabla \@ref(tab:tablaTR) el $TR(u=15)\approx 1.6$. 

-->



<!--

```{r fig1, fig.cap="(1) Gráfico de cuantiles (Q–Q)", echo=FALSE, fig.height=3, fig.width=6}
plot(fit_gev, which = 1)
```

```{r fig2, fig.cap="(2) Gráfico de probabilidades (P–P)", echo=FALSE, fig.height=3, fig.width=6}
plot(fit_gev, which = 2)
```

```{r fig3, fig.cap="(3) Gráfico de densidad", echo=FALSE, fig.height=3, fig.width=6}
plot(fit_gev, which = 3)
```

```{r fig4, fig.cap="(4) Gráfico de residuos estandarizados", echo=FALSE, fig.height=3, fig.width=6}
plot(fit_gev, which = 4)
```


-->







